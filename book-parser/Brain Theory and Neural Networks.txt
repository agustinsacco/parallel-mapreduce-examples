The Handbook of

Brain Theory
and Neural Networks

This Page Intentionally Left Blank

The Handbook of

Brain Theory
and Neural Networks
Second Edition
EDITED BY

Michael A. Arbib
EDITORIAL ADVISORY BOARD
Shun-ichi Amari • John Barnden • Andrew Barto • Ronald Calabrese
Avis Cohen • Joaquı́n Fuster • Stephen Grossberg • John Hertz
Marc Jeannerod • Mitsuo Kawato • Christof Koch • Wolfgang Maass
James McClelland • Kenneth Miller • Terrence Sejnowski
Noel Sharkey • DeLiang Wang

EDITORIAL ASSISTANT
Prudence H. Arbib

A Bradford Book

THE MIT PRESS
Cambridge, Massachusetts
London, England

䉷 2003 Massachusetts Institute of Technology
All rights reserved. No part of this book may be reproduced in any form by any electronic or
mechanical means (including photocopying, recording, or information storage and retrieval)
without permission in writing from the publisher.
This book was set in Times Roman by Impressions Book and Journal Services, Inc., Madison,
Wisconsin, and was printed and bound in the United States of America.
Library of Congress Cataloging-in-Publication Data
The handbook of brain theory and neural networks / Michael A. Arbib,
editor—2nd ed.
p. cm.
“A Bradford book.”
Includes bibliographical references and index.
ISBN 0–262–01197–2
1. Neural networks (Neurobiology)— Handbooks, manuals, etc.
2. Neural networks (Computer science)—Handbooks, manuals, etc.
I. Arbib, Michael A.
QP363.3.H36 2002
612.8⬘2—dc21
2002038664
CIP

Contents
Preface to the Second Edition ix
Preface to the First Edition xi
How to Use This Book xv

Part I: Background: The Elements of
Brain Theory and Neural Networks 1
How to Use Part I 3
I.1. Introducing the Neuron 3
The Diversity of Receptors 4
Basic Properties of Neurons 4
Receptors and Effectors 7
Neural Models 7
More Detailed Properties of Neurons 9
I.2. Levels and Styles of Analysis 10
A Historical Fragment 10
Brains, Machines, and Minds 11
Levels of Analysis 12
Schema Theory 13
I.3. Dynamics and Adaptation in Neural Networks 15
Dynamic Systems 15
Continuous-Time Systems 15
Discrete-Time Systems 16
Stability, Limit Cycles, and Chaos 16
Hopfield Nets 17
Adaptation in Dynamic Systems 18
Adaptive Control 18
Pattern Recognition 18
Associative Memory 19
Learning Rules 19
Hebbian Plasticity and Network
Self-Organization 19
Perceptrons 20
Network Complexity 20
Gradient Descent and Credit Assignment 21
Backpropagation 21
A Cautionary Note 22
Envoi 23

Part II: Road Maps: A Guided Tour of
Brain Theory and Neural Networks 25
How to Use Part II 27
II.1. The Meta-Map 27
II.2. Grounding Models of Neurons and Networks 29
Grounding Models of Neurons 29
Grounding Models of Networks 31

II.3. Brain, Behavior, and Cognition 31
Neuroethology and Evolution 31
Mammalian Brain Regions 34
Cognitive Neuroscience 37
II.4. Psychology, Linguistics, and
Artificial Intelligence 40
Psychology 40
Linguistics and Speech Processing 42
Artificial Intelligence 44
II.5. Biological Neurons and Networks 47
Biological Neurons and Synapses 47
Neural Plasticity 49
Neural Coding 52
Biological Networks 54
II.6. Dynamics and Learning in Artificial Networks 55
Dynamic Systems 55
Learning in Artificial Networks 58
Computability and Complexity 64
II.7. Sensory Systems 65
Vision 65
Other Sensory Systems 70
II.8. Motor Systems 71
Robotics and Control Theory 71
Motor Pattern Generators 73
Mammalian Motor Control 74
II.9. Applications, Implementations, and Analysis 77
Applications 77
Implementation and Analysis 78

Part III: Articles 81
The articles in Part III are arranged alphabetically by title.
To retrieve articles by author, turn to the contributors list,
which begins on page 1241.
Action Monitoring and Forward Control of
Movements 83
Activity-Dependent Regulation of Neuronal
Conductances 85
Adaptive Resonance Theory 87
Adaptive Spike Coding 90
Amplification, Attenuation, and Integration 94
Analog Neural Nets: Computational Power 97
Analog VLSI Implementations of Neural Networks 101
Analogy-Based Reasoning and Metaphor 106
Arm and Hand Movement Control 110
Artifical Intelligence and Neural Networks 113

vi

Contents

Associative Networks 117
Auditory Cortex 122
Auditory Periphery and Cochlear Nucleus 127
Auditory Scene Analysis 132
Axonal Modeling 135
Axonal Path Finding 140
Backpropagation: General Principles 144
Basal Ganglia 147
Bayesian Methods and Neural Networks 151
Bayesian Networks 157
Biologically Inspired Robotics 160
Biophysical Mechanisms in Neuronal Modeling 164
Biophysical Mosaic of the Neuron 170
Brain Signal Analysis 175
Brain-Computer Interfaces 178
Canonical Neural Models 181
Cerebellum and Conditioning 187
Cerebellum and Motor Control 190
Cerebellum: Neural Plasticity 196
Chains of Oscillators in Motor and Sensory Systems 201
Chaos in Biological Systems 205
Chaos in Neural Systems 208
Cognitive Development 212
Cognitive Maps 216
Cognitive Modeling: Psychology and Connectionism 219
Collective Behavior of Coupled Oscillators 223
Collicular Visuomotor Transformations for Gaze
Control 226
Color Perception 230
Command Neurons and Command Systems 233
Competitive Learning 238
Competitive Queuing for Planning and Serial
Performance 241
Compositionality in Neural Systems 244
Computing with Attractors 248
Concept Learning 252
Conditioning 256
Connectionist and Symbolic Representations 260
Consciousness, Neural Models of 263
Constituency and Recursion in Language 267
Contour and Surface Perception 271
Convolutional Networks for Images, Speech, and Time
Series 276
Cooperative Phenomena 279
Cortical Hebbian Modules 285
Cortical Memory 290
Cortical Population Dynamics and Psychophysics 294
Covariance Structural Equation Modeling 300
Crustacean Stomatogastric System 304
Data Clustering and Learning 308
Databases for Neuroscience 312
Decision Support Systems and Expert Systems 316

Dendritic Learning 320
Dendritic Processing 324
Dendritic Spines 332
Development of Retinotectal Maps 335
Developmental Disorders 339
Diffusion Models of Neuron Activity 343
Digital VLSI for Neural Networks 349
Directional Selectivity 353
Dissociations Between Visual Processing Modes 358
Dopamine, Roles of 361
Dynamic Link Architecture 365
Dynamic Remapping 368
Dynamics and Bifurcation in Neural Nets 372
Dynamics of Association and Recall 377
Echolocation: Cochleotopic and Computational Maps
381
EEG and MEG Analysis 387
Electrolocation 391
Embodied Cognition 395
Emotional Circuits 398
Energy Functionals for Neural Networks 402
Ensemble Learning 405
Equilibrium Point Hypothesis 409
Event-Related Potentials 412
Evolution and Learning in Neural Networks 415
Evolution of Artificial Neural Networks 418
Evolution of Genetic Networks 421
Evolution of the Ancestral Vertebrate Brain 426
Eye-Hand Coordination in Reaching Movements 431
Face Recognition: Neurophysiology and Neural
Technology 434
Face Recognition: Psychology and Connectionism 438
Fast Visual Processing 441
Feature Analysis 444
Filtering, Adaptive 449
Forecasting 453
Gabor Wavelets and Statistical Pattern Recognition 457
Gait Transitions 463
Gaussian Processes 466
Generalization and Regularization in Nonlinear Learning
Systems 470
GENESIS Simulation System 475
Geometrical Principles in Motor Control 476
Global Visual Pattern Extraction 482
Graphical Models: Parameter Learning 486
Graphical Models: Probabilistic Inference 490
Graphical Models: Structure Learning 496
Grasping Movements: Visuomotor Transformations 501
Habituation 504
Half-Center Oscillators Underlying Rhythmic
Movements 507

Contents

Hebbian Learning and Neuronal Regulation 511
Hebbian Synaptic Plasticity 515
Helmholtz Machines and Sleep-Wake Learning 522
Hemispheric Interactions and Specialization 525
Hidden Markov Models 528
Hippocampal Rhythm Generation 533
Hippocampus: Spatial Models 539
Hybrid Connectionist/Symbolic Systems 543
Identification and Control 547
Imaging the Grammatical Brain 551
Imaging the Motor Brain 556
Imaging the Visual Brain 562
Imitation 566
Independent Component Analysis 569
Information Theory and Visual Plasticity 575
Integrate-and-Fire Neurons and Networks 577
Invertebrate Models of Learning: Aplysia and
Hermissenda 581
Ion Channels: Keys to Neuronal Specialization 585
Kalman Filtering: Neural Implications 590
Laminar Cortical Architecture in Visual Perception 594
Language Acquisition 600
Language Evolution and Change 604
Language Evolution: The Mirror System Hypothesis 606
Language Processing 612
Layered Computation in Neural Networks 616
Learning and Generalization: Theoretical Bounds 619
Learning and Statistical Inference 624
Learning Network Topology 628
Learning Vector Quantization 631
Lesioned Networks as Models of Neuropsychological
Deficits 635
Limb Geometry, Neural Control 638
Localized Versus Distributed Representations 643
Locomotion, Invertebrate 646
Locomotion, Vertebrate 649
Locust Flight: Components and Mechanisms in the
Motor 654
Markov Random Field Models in Image Processing 657
Memory-Based Reasoning 661
Minimum Description Length Analysis 662
Model Validation 666
Modular and Hierarchical Learning Systems 669
Motion Perception: Elementary Mechanisms 672
Motion Perception: Navigation 676
Motivation 680
Motoneuron Recruitment 683
Motor Control, Biological and Theoretical 686
Motor Cortex: Coding and Decoding of Directional
Operations 690
Motor Pattern Generation 696

vii

Motor Primitives 701
Motor Theories of Perception 705
Multiagent Systems 707
Muscle Models 711
Neocognitron: A Model for Visual Pattern
Recognition 715
Neocortex: Basic Neuron Types 719
Neocortex: Chemical and Electrical Synapses 725
Neural Automata and Analog Computational Complexity
729
Neuroanatomy in a Computational Perspective 733
Neuroethology, Computational 737
Neuroinformatics 741
Neurolinguistics 745
Neurological and Psychiatric Disorders 751
Neuromanifolds and Information Geometry 754
Neuromodulation in Invertebrate Nervous Systems 757
Neuromodulation in Mammalian Nervous Systems 761
Neuromorphic VLSI Circuits and Systems 765
NEURON Simulation Environment 769
Neuropsychological Impairments 773
Neurosimulation: Tools and Resources 776
NMDA Receptors: Synaptic, Cellular, and Network
Models 781
NSL Neural Simulation Language 784
Object Recognition 788
Object Recognition, Neurophysiology 792
Object Structure, Visual Processing 797
Ocular Dominance and Orientation Columns 801
Olfactory Bulb 806
Olfactory Cortex 810
Optimal Sensory Encoding 815
Optimality Theory in Linguistics 819
Optimization, Neural 822
Optimization Principles in Motor Control 827
Orientation Selectivity 831
Oscillatory and Bursting Properties of Neurons 835
PAC Learning and Neural Networks 840
Pain Networks 843
Past Tense Learning 848
Pattern Formation, Biological 851
Pattern Formation, Neural 859
Pattern Recognition 864
Perception of Three-Dimensional Structure 868
Perceptrons, Adalines, and Backpropagation 871
Perspective on Neuron Model Complexity 877
Phase-Plane Analysis of Neural Nets 881
Philosophical Issues in Brain Theory and
Connectionism 886
Photonic Implementations of Neurobiologically Inspired
Networks 889

viii

Contents

Population Codes 893
Post-Hebbian Learning Algorithms 898
Potential Fields and Neural Networks 901
Prefrontal Cortex in Temporal Organization of Action
905
Principal Component Analysis 910
Probabilistic Regularization Methods for Low-Level
Vision 913
Programmable Neurocomputing Systems 916
Prosthetics, Motor Control 919
Prosthetics, Neural 923
Prosthetics, Sensory Systems 926
Pursuit Eye Movements 929
Q-Learning for Robots 934
Radial Basis Function Networks 937
Rate Coding and Signal Processing 941
Reaching Movements: Implications for Computational
Models 945
Reactive Robotic Systems 949
Reading 951
Recurrent Networks: Learning Algorithms 955
Recurrent Networks: Neurophysiological Modeling 960
Reinforcement Learning 963
Reinforcement Learning in Motor Control 968
Respiratory Rhythm Generation 972
Retina 975
Robot Arm Control 979
Robot Learning 983
Robot Navigation 987
Rodent Head Direction System 990
Schema Theory 993
Scratch Reflex 999
Self-Organization and the Brain 1002
Self-Organizing Feature Maps 1005
Semantic Networks 1010
Sensor Fusion 1014
Sensorimotor Interactions and Central Pattern
Generators 1016
Sensorimotor Learning 1020
Sensory Coding and Information Transmission 1023
Sequence Learning 1027
Short-Term Memory 1030
Silicon Neurons 1034
Simulated Annealing and Boltzmann Machines 1039
Single-Cell Models 1044
Sleep Oscillations 1049
Somatosensory System 1053
Somatotopy: Plasticity of Sensory Maps 1057
Sound Localization and Binaural Processing 1061
Sparse Coding in the Primate Cortex 1064
Speech Processing: Psycholinguistics 1068

Speech Production 1072
Speech Recognition Technology 1076
Spiking Neurons, Computation with 1080
Spinal Cord of Lamprey: Generation of Locomotor
Patterns 1084
Statistical Mechanics of Generalization 1087
Statistical Mechanics of Neural Networks 1090
Statistical Mechanics of On-line Learning and
Generalization 1095
Statistical Parametric Mapping of Cortical Activity
Patterns 1098
Stereo Correspondence 1104
Stochastic Approximation and Efficient Learning 1108
Stochastic Resonance 1112
Structured Connectionist Models 1116
Support Vector Machines 1119
Synaptic Interactions 1126
Synaptic Noise and Chaos in Vertebrate Neurons 1130
Synaptic Transmission 1133
Synchronization, Binding and Expectancy 1136
Synfire Chains 1143
Synthetic Functional Brain Mapping 1146
Systematicity of Generalizations in Connectionist
Networks 1151
Temporal Dynamics of Biological Synapses 1156
Temporal Integration in Recurrent Microcircuits 1159
Temporal Pattern Processing 1163
Temporal Sequences: Learning and Global Analysis 1167
Tensor Voting and Visual Segmentation 1171
Thalamus 1176
Universal Approximators 1180
Unsupervised Learning with Global Objective
Functions 1183
Vapnik-Chervonenkis Dimension of Neural Networks
1188
Vestibulo-Ocular Reflex 1192
Visual Attention 1196
Visual Cortex: Anatomical Structure and Models of
Function 1202
Visual Course Control in Flies 1205
Visual Scene Perception, Neurophysiology 1210
Visual Scene Segmentation 1215
Visuomotor Coordination in Frog and Toad 1219
Visuomotor Coordination in Salamander 1225
Winner-Take-All Networks 1228
Ying-Yang Learning 1231

Editorial Advisory Board 1239
Contributors 1241
Subject Index 1255

Preface to the Second Edition
Like the first edition, which it replaces, this volume is inspired by two great questions:
“How does the brain work?” and “How can we build intelligent machines?” As in the first
edition, the heart of the book is a set of close to 300 articles in Part III which cover the
whole spectrum of Brain Theory and Neural Networks. To help readers orient themselves
with respect to this cornucopia, I have written Part I to provide the elementary background
on the modeling of both brains and biological and artificial neural networks, and Part II
to provide a series of road maps to help readers interested in a particular topic steer through
the Part III articles on that topic. More on the motivation and structure of the book can be
found in the Preface to the First Edition, which is reproduced after this. I also recommend
reading the section “How to Use This Book”—one reader of the first edition who did not
do so failed to realize that the articles in Part III were in alphabetical order, or that the
Contributors list lets one locate each article written by a given author.
The reader new to the study of Brain Theory and Neural Networks will find it wise to
read Part I for orientation before jumping into Part III, whereas more experienced readers
will find most of Part I familiar. Many readers will simply turn to articles in Part III of
particular interest at a given time. However, to help readers who seek a more systematic
view of a particular subfield of Brain Theory and Neural Networks, Part II provides 22
Road Maps, each providing an essay linking most of the articles on a given topic. (I say
“most” because the threshold is subjective for deciding when a particular article has more
than a minor mention of the topic in a Road Map.) The Road Maps are organized into 8
groups in Part II as follows:
Grounding Models of Neurons and Networks
Grounding Models of Neurons
Grounding Models of Networks
Brain, Behavior, and Cognition
Neuroethology and Evolution
Mammalian Brain Regions
Cognitive Neuroscience
Psychology, Linguistics, and Artificial Intelligence
Psychology
Linguistics and Speech Processing
Artificial Intelligence
Biological Neurons and Networks
Biological Neurons and Synapses
Neural Plasticity
Neural Coding
Biological Networks
Dynamics and Learning in Artificial Networks
Dynamic Systems
Learning in Artificial Networks
Computability and Complexity
Sensory Systems
Vision
Other Sensory Systems
Motor Systems
Robotics and Control Theory
Motor Pattern Generators
Mammalian Motor Control

x

Preface to the Second Edition

Applications, Implementations, and Analysis
Applications
Implementation and Analysis
The authors of the articles in Part III come from a broad spectrum of disciplines—such
as biomedical engineering, cognitive science, computer science, electrical engineering,
linguistics, mathematics, physics, neurology, neuroscience, and psychology—and have
worked hard to make their articles accessible to readers across the spectrum. The utility
of each article is enhanced by cross-references to other articles within the body of the
article, and lists at the end of the article referring the reader to road maps, background
material, and related reading.
To get some idea of how radically the new edition differs from the old, note that the
new edition has 285 articles in Part III, as against the 266 articles of the first edition. Of
the articles that appeared in the first edition, only 9 are reprinted unchanged. Some 135
have been updated (or even completely rewritten) by their original authors, and more than
30 have been written anew by new authors. In addition, there are over 100 articles on new
topics. The primary shift of emphasis from the first edition has been to drastically reduce
the number of articles on applications of artificial neural networks (from astronomy to
steelmaking) and to greatly increase the coverage of models of fundamental neurobiology
and neural network approaches to language, and to add the new papers which are now
listed in the Road Maps on Cognitive Neuroscience, Neural Coding, and Other Sensory
Systems (i.e., other than Vision, for which coverage has also been increased). Certainly,
a number of the articles in the first edition remain worthy of reading in themselves, but
the aim has been to make the new edition a self-contained introduction to brain theory and
neural networks in all its current breadth and richness.
The new edition not only appears in print but also has its own web site.
Acknowledgments
My foremost acknowledgment is again to Prue Arbib, who served as Editorial Assistant
during the long and arduous process of eliciting and assembling the many, many contributions to Part III. I thank the members of the Editorial Advisory Board, who helped
update the list of articles from the first edition and focus the search for authors, and I thank
these authors not only for their contributions to Part III but also for suggesting further
topics and authors for the Handbook, in an ever-widening circle as work advanced on this
new edition. I also owe a great debt to the hundreds of reviewers who so constructively
contributed to the final polishing of the articles that now appear in Part III. Finally, I thank
the staff of P. M. Gordon Associates and of The MIT Press for once again meeting the
high standards of copy editing and book production that contributed so much to the success
of the first edition.
Michael A. Arbib
Los Angeles and La Jolla
October 2002

Preface to the First Edition
This volume is inspired by two great questions: “How does the brain work?” and “How
can we build intelligent machines?” It provides no simple, single answer to either question
because no single answer, simple or otherwise, exists. However, in hundreds of articles it
charts the immense progress made in recent years in answering many related, but far more
specific, questions.
The term neural networks has been used for a century or more to describe the networks
of biological neurons that constitute the nervous systems of animals, whether invertebrates
or vertebrates. Since the 1940s, and especially since the 1980s, the term has been used for
a technology of parallel computation in which the computing elements are “artificial neurons” loosely modeled on simple properties of biological neurons, usually with some adaptive capability to change the strengths of connections between the neurons.
Brain theory is centered on “computational neuroscience,” the use of computational
techniques to model biological neural networks, but also includes attempts to understand
the brain and its function through a variety of theoretical constructs and computer analogies. In fact, as the following pages reveal, much of brain theory is not about neural
networks per se, but focuses on structural and functional “networks” whose units are in
scales both coarser and finer than that of the neuron. Computer scientists, engineers, and
physicists have analyzed and applied artificial neural networks inspired by the adaptive,
parallel computing style of the brain, but this Handbook will also sample non-neural approaches to the design and analysis of “intelligent” machines. In between the biologists
and the technologists are the connectionists. They use artificial neural networks in psychology and linguistics and make related contributions to artificial intelligence, using neuron-like unites which interact “in the style of the brain” at a more abstract level than that
of individual biological neurons.
Many texts have described limited aspects of one subfield or another of brain theory
and neural networks, but no truly comprehensive overview is available. The aim of this
Handbook is to fill that gap, presenting the entire range of the following topics: detailed
models of single neurons; analysis of a wide variety of neurobiological systems; “connectionist” studies; mathematical analyses of abstract neural networks; and technological applications of adaptive, artificial neural networks and related methodologies. The excitement, and the frustration, of these topics is that they span such a broad range of disciplines,
including mathematics, statistical physics and chemistry, neurology and neurobiology, and
computer science and electrical engineering, as well as cognitive psychology, artificial
intelligence, and philosophy. Much effort, therefore, has gone into making the book accessible to readers with varied backgrounds (an undergraduate education in one of the
above areas, for example, or the frequent reading of related articles at the level of the
Scientific American) while still providing a clear view of much of the recent specialized
research.
The heart of the book comes in Part III, in which the breadth of brain theory and neural
networks is sampled in 266 articles, presented in alphabetical order by title. Each article
meets the following requirements:
1. It is authoritative within its own subfield, yet accessible to students and experts in a
wide range of other fields.
2. It is comprehensive, yet short enough that its concepts can be acquired in a single
sitting.
3. It includes a list of references, limited to 15, to give the reader a well-defined and
selective list of places to go to initiate further study.
4. It is as self-contained as possible, while providing cross-references to allow readers to
explore particular issues of related interest.

xii

Preface to the First Edition

Despite the fourth requirement, some articles are more self-contained than others. Some
articles can be read with almost no prior knowledge; some can be read with a rather general
knowledge of a few key concepts; others require fairly detailed understanding of material
covered in other articles. For example, many articles on applications will make sense only
if one understands the “backpropagation” technique for training artificial neural networks;
and a number of studies of neuronal function will make sense only if one has at least some
idea of the Hodgkin-Huxley equation. Whenever appropriate, therefore, the articles include
advice on background articles.
Parts I and II of the book provide a more general approach to helping readers orient
themselves. Part I: Background presents a perspective on the “landscape” of brain theory
and neural networks, including an exposition of the key concepts for viewing neural networks as dynamic, adaptive systems. Part II: Road Maps then provides an entrée into the
many articles of Part III, with “road maps” for 23 different themes. The “Meta-Map,“
which introduces Part II, groups these themes under eight general headings which, in and
of themselves, give some sense of the sweep of the Handbook:
Connectionism: Psychology, Linguistics, and Artificial Intelligence
Dynamics, Self-Organization, and Cooperativity
Learning in Artificial Neural Networks
Applications and Implementations
Biological Neurons and Networks
Sensory Systems
Plasticity in Development and Learning
Motor Control
A more detailed view of the structure of the book is provided in the introductory section
“How to Use this Book.” The aim is to ensure that readers will not only turn to the book
to get good brief reviews of topics in their own specialty, but also will find many invitations
to browse widely—finding parallels amongst different subfields, or simply enjoying the
discovery of interesting topics far from familiar territory.
Acknowledgments
My foremost acknowledgment is to Prue Arbib, who served as Editorial Assistant during
the long and arduous process of eliciting and assembling the many, many contributions to
Part III; we both thank Paulina Tagle for her help with our work. The initial plan for the
book was drawn up in 1991, and it benefited from the advice of a number of friends,
especially George Adelman, who shared his experience as Editor of the Encyclopedia of
Neuroscience. Refinement of the plan and the choice of publishers occupied the first few
months of 1992, and I thank Fiona Stevens of The MIT Press for her support of the project
from that time onward.
As can be imagined, the plan for a book like this has developed through a time-consuming process of constraint satisfaction. The first steps were to draw up a list of about 20
topic areas (similar to, but not identical with, the 23 areas surveyed in Part II), to populate
these areas with a preliminary list of over 100 articles and possible authors, and to recruit
the first members of the Editorial Advisory Board to help expand the list of articles and
focus on the search for authors. A very satisfying number of authors invited in the first
round accepted my invitation, and many of these added their voices to the Editorial Advisory Board in suggesting further topics and authors for the Handbook.
I was delighted, stimulated, and informed as I read the first drafts of the articles; but I
have also been grateful for the fine spirit of cooperation with which the authors have
responded to editorial comments and reviews. The resulting articles not only are authoritative and accessible in themselves, but also have been revised to match the overall style
of the Handbook and to meet the needs of a broad readership. With this I express my
sincere thanks to the editorial advisors, the authors, and the hundreds of reviewers who so

Preface to the First Edition

xiii

constructively contributed to the final polishing of the articles that now appear in Part III;
to Doug Gordon and the copy editors and typesetters who transformed the diverse styles
of the manuscripts into the style of the Handbook; and to the graduate students who helped
so much with the proofreading.
Finally, I want to record a debt that did not reach my conscious awareness until well
into the editing of this book. It is to Hiram Haydn, who for many years was editor of The
American Scholar, which is published for general circulation by Phi Beta Kappa. In 1971
or so, Phi Beta Kappa conducted a competition to find authors to receive grants for books
to be written, if memory serves aright, for the Bicentennial of the United States. I submitted
an entry. Although I was not successful, Mr. Haydn, who had been a member of the jury,
wrote to express his appreciation of that entry, and to invite me to write an article for the
Scholar. What stays in my mind from the ensuing correspondence was the sympathetic
way in which he helped me articulate the connections that were at best implicit in my
draft, and find the right voice in which to “speak” with the readers of a publication so
different from the usual scientific journal. I now realize that it is his example I have tried
to follow as I have worked with these hundreds of authors in the quest to see the subject
of brain theory and neural networks whole, and to share it with readers of diverse interests
and backgrounds.
Michael A. Arbib
Los Angeles and La Jolla
January 1995

How to Use This Book
More than 90% of this book is taken up by Part III, which, in 285 separately authored
articles, covers a vast range of topics in brain theory and neural networks, from language
to motor control, and from the neurochemistry to the statistical mechanics of memory.
Each article has been made as self-contained as possible, but the very breadth of topics
means that few readers will be expert in a majority of them. To help the reader new to
certain areas of the Handbook, I have prepared Part I: Background and Part II: Road Maps.
The next few pages describe these aids to comprehension, as well as offering more information on the structure of articles in Part III.
Part I: Background: The Elements of Brain Theory and Neural Networks
Part I provides background material for readers new to computational neuroscience or
theoretical approaches to neural networks considered as dynamic, adaptive systems. Section I.1, “Introducing the Neuron,” conveys the basic properties of neurons and introduces
several basic neural models. Section I.2, “Levels and Styles of Analysis,” explains the
interdisciplinary nexus in which the present study of brain theory and neural networks is
located, with historical roots in cybernetics and with current work going back and forth
between brain theory, artificial intelligence, and cognitive psychology. We also review the
different levels of analysis involved, with schemas providing the functional units intermediate between an overall task and neural networks. Finally, Section I.3, “Dynamics and
Adaptation in Neural Networks,” provides a tutorial on the concepts essential for understanding neural networks as dynamic, adaptive systems. We close by stressing that the full
understanding of the brain and the improved design of intelligent machines will require
not only improvements in the learning methods presented in Section I.3, but also fuller
understanding of architectures based on networks of networks, with initial structures well
constrained for the task at hand.
Part II: Road Maps: A Guided Tour of Brain Theory and Neural Networks
The reader who wants to survey a major theme of brain theory and neural networks, rather
than seeking articles in Part III one at a time, will find in Part II a set of 22 road maps
that, among them, place every article in Part III in a thematic perspective. Section II.1
presents a Meta-Map, which briefly surveys all these themes, grouping them under eight
general headings:
Grounding Models of Neurons and Networks
Grounding Models of Neurons
Grounding Models of Networks
Brain, Behavior, and Cognition
Neuroethology and Evolution
Mammalian Brain Regions
Cognitive Neuroscience
Psychology, Linguistics, and Artificial Intelligence
Psychology
Linguistics and Speech Processing
Artificial Intelligence
Biological Neurons and Networks
Biological Neurons and Synapses
Neural Plasticity
Neural Coding
Biological Networks

xvi

How to Use This Book

Dynamics and Learning in Artificial Networks
Dynamic Systems
Learning in Artificial Networks
Computability and Complexity
Sensory Systems
Vision
Other Sensory Systems
Motor Systems
Robotics and Control Theory
Motor Pattern Generators
Mammalian Motor Control
Applications, Implementations, and Analysis
Applications
Implementation and Analysis

This ordering of the themes has no special significance. It is simply one way to approach
the richness of the Handbook, making it easy for you to identify one or two key road maps
of special interest. By the same token, the order of articles in each of the 22 road maps
that follow the Meta-Map is one among many such orderings. Each road map starts with
an alphabetical listing of the articles most relevant to the current theme. The road map
itself will provide suggestions for interesting traversals of articles, but this need not imply
that an article provides necessary background for the articles it precedes.
Part III: Articles
Part III comprises 285 articles. These articles are arranged in alphabetical order, both to
make it easier to find a specific topic (although a Subject Index is provided as well, and
the alphabetical list of Contributors on page 1241 lists all the articles to which each author
has contributed) and because a given article may be relevant to more than one of the
themes of Part II, a fact that would be hidden were the article to be relegated to a specific
section devoted to a single theme. Most of these articles assume some prior familiarity
with neural networks, whether biological or artificial, and so the reader new to neural
networks is encouraged to master the material in Part I before tackling Part III.
Most articles in Part III have the following structure: The introduction provides a nontechnical overview of the material covered in the whole article, while the final section
provides a discussion of key points, open questions, and linkages with other areas of brain
theory and neural networks. The intervening sections may be more or less technical, depending on the nature of the topic, but the first and last sections should give most readers
a basic appreciation of the topic, irrespective of such technicalities. The bibliography for
each article contains about 15 references. People who find their favorite papers omitted
from the list should blame my editorial decision, not the author’s judgment. The style I
chose for the Handbook was not to provide exhaustive coverage of research papers for the
expert. Rather, references are there primarily to help readers who look for an introduction
to the literature on the given topic, including background material, relevant review articles,
and original research citations. In addition to formal references to the literature, each article
contains numerous cross-references to other articles in the Handbook. These may occur
either in the body of the article in the form THE TITLE OF THE ARTICLE IN SMALL CAPS,
or at the end of the article, designated as “Related Reading.” In addition to suggestions
for related reading, the reader will find, just prior to the list of references in each article,
a mention of the road map(s) in which the article is discussed, as well as background
material, when the article is more advanced.
In summary, turn directly to Part III when you need information on a specific topic.
Read sections of Part I to gain a general perspective on the basic concepts of brain theory
and neural networks. For an overview of some theme, read the Meta-Map in Part II to

How to Use This Book

xvii

choose road maps in Part II; read a road map to choose articles in Part III. A road map
can also be used as an explicit guide for systematic study of the area under review. Then
continue your exploration through further use of road maps, by following cross-references
in Part III, by looking up terms of interest in the index, or simply by letting serendipity
take its course as you browse through Part III at random.

Part I: Background
The Elements of Brain Theory
and Neural Networks
Michael A. Arbib

I.1. Introducing the Neuron

3

How to Use Part I
Part I provides background material, summarizing a set of concepts
established for the formal study of neurons and neural networks by
1986. As such, it is designed to hold few, if any, surprises for
readers with a fair background in computational neuroscience or
theoretical approaches to neural networks considered as dynamic,
adaptive systems. Rather, Part I is designed for the many readers—
be they neuroscience experimentalists, psychologists, philosophers,
or technologists—who are sufficiently new to brain theory and
neural networks that they can benefit from a compact overview of
basic concepts prior to reading the road maps of Part II and the
articles in Part III. Of course, much of what is covered in Part I is
also covered at some length in the articles in Part III, and crossreferences will steer the reader to these articles for alternative expositions and reviews of current research. In this exposition, as
throughout the Handbook, we will move back and forth between
computational neuroscience, where the emphasis is on modeling
biological neurons, and neural computing, where the emphasis
shifts back and forth between biological models and artificial neural
networks based loosely on abstractions from biology, but driven
more by technological utility than by biological considerations.
Section I.1, “Introducing the Neuron,” conveys the basic properties of neurons, receptors, and effectors, and then introduces several simple neural models, including the discrete-time McCullochPitts model and the continuous-time leaky integrator model.
References to Part III alert the reader to more detailed properties
of neurons which are essential for the neuroscientist and provide
interesting hints about future design features for the technologist.
Section I.2, “Levels and Styles of Analysis,” is designed to give
the reader a feel for the interdisciplinary nexus in which the present
study of brain theory and neural networks is located. The selection
begins with a historical fragment which traces our federation of
disciplines back to their roots in cybernetics, the study of control
and communication in animals and machines. We look at the way
in which the research addresses brains, machines, and minds, going

back and forth between brain theory, artificial intelligence, and cognitive psychology. We then review the different levels of analysis
involved, whether we study brains or intelligent machines, and the
use of schemas to provide intermediate functional units that bridge
the gap between an overall task and the neural networks which
implement it.
Section I.3, “Dynamics and Adaptation in Neural Networks,”
provides a tutorial on the concepts essential for understanding neural networks as dynamic, adaptive systems. It introduces the basic
dynamic systems concepts of stability, limit cycles, and chaos, and
relates Hopfield nets to attractors and optimization. It then introduces a number of basic concepts concerning adaptation in neural
nets, with discussions of pattern recognition, associative memory,
Hebbian plasticity and network self-organization, perceptrons, network complexity, gradient descent and credit assignment, and
backpropagation. This section, and with it Part I, closes with a
cautionary note. The basic learning rules and adaptive architectures
of neural networks have already illuminated a number of biological
issues and led to useful technological applications. However, these
networks must have their initial structure well constrained (whether
by evolution or technological design) to yield approximate solutions to the system’s tasks—solutions that can then be efficiently
and efficaciously shaped by experience. Moreover, the full understanding of the brain and the improved design of intelligent machines will require not only improvements in these learning methods and their initialization, but also a fuller understanding of
architectures based on networks of networks. Cross-references to
articles in Part III will set the reader on the path to this fuller
understanding. Because Part I focuses on the basic concepts established for the formal study of neurons and neural networks by 1986,
it differs hardly at all from Part I of the first edition of the Handbook. By contrast, Part II, which provides the road maps that guide
readers through the radically updated Part III, has been completely
rewritten for the present edition to reflect the latest research results.

I.1. Introducing the Neuron
We introduce the neuron. The dangerous word in the preceding
sentence is the. In biology, there are radically different types of
neurons in the human brain, and endless variations in neuron types
of other species. In brain theory, the complexities of real neurons
are abstracted in many ways to aid in understanding different aspects of neural network development, learning, or function. In neural computing (technology based on networks of “neuron-like”
units), the artificial neurons are designed as variations on the abstractions of brain theory and are implemented in software, or VLSI
or other media. There is no such thing as a “typical” neuron, yet
this section will nonetheless present examples and models which
provide a starting point, an essential set of key concepts, for the
appreciation of the many variations on the theme of neurons and
neural networks presented in Part III.
An analogy to the problem we face here might be to define vehicle for a handbook of transportation. A vehicle could be a car, a
train, a plane, a rowboat, or a forklift truck. It might or might not
carry people. The people could be crew or passengers, and so on.
The problem would be to give a few key examples of form (such
as car versus plane) and function (to carry people or goods, by
land, air, or sea, etc.). Moreover, we would find interesting examples of co-evolution: for example, modern highway systems would

not have been created without the pressure of increasing car traffic;
most features of cars are adapted to the existence of sealed roads,
and some features (e.g., cruise control) are specifically adapted to
good freeway conditions. Following a similar procedure, Part III
offers diverse examples of neural form and function in both biology
and technology.
Here, we start with the observation that a brain is made up of a
network of cells called neurons, coupled to receptors and effectors.
Neurons are intimately connected with glial cells, which provide
support functions for neural networks. New empirical data show
the importance of glia in regeneration of neural networks after damage and in maintaining the neurochemical milieu during normal
operation. However, such data have had very little impact on neural
modeling and so will not be considered further here. The input to
the network of neurons is provided by receptors, which continually
monitor changes in the external and internal environment. Cells
called motor neurons (or motoneurons), governed by the activity
of the neural network, control the movement of muscles and the
secretion of glands. In between, an intricate network of neurons (a
few hundred neurons in some simple creatures, hundreds of billions
in a human brain) continually combines the signals from the receptors with signals encoding past experience to barrage the motor

4

Part I: Background

neurons with signals that will yield adaptive interactions with the
environment. In animals with backbones (vertebrates, including
mammals in general and humans in particular), this network is
called the central nervous system (CNS), and the brain constitutes
the most headward part of this system, linked to the receptors and
effectors of the body via the spinal cord. Invertebrate nervous systems (neural networks) provide astounding variations on the vertebrate theme, thanks to eons of divergent evolution. Thus, while
the human brain may be the source of rich analogies for technologists in search of “artificial intelligence,” both invertebrates and
vertebrates provide endless ideas for technologists designing neural
networks for sensory processing, robot control, and a host of other
applications. (A few of the relevant examples may be found in the
Part II road maps, Vision, Robotics and Control Theory, Motor
Pattern Generators, and Neuroethology and Evolution.)
The brain provides far more than a simple stimulus-response
chain from receptors to effectors (although there are such reflex
paths). Rather, the vast network of neurons is interconnected in
loops and tangled skeins so that signals entering the net from the
receptors interact there with the billions of signals already traversing the system, not only to yield the signals that control the effectors but also to modify the very properties of the network itself, so
that future behavior will reflect prior experience.

The Diversity of Receptors
Rod and cone receptors in the eyes respond to light, hair cells in
the ears respond to pressure, and other cells in the tongue and the
mouth respond to subtle traces of chemicals. In addition to touch
receptors, there are receptors in the skin that are responsive to
movement or to temperature, or that signal painful stimuli. These
external senses may be divided into two classes: (1) the proximity
senses, such as touch and taste, which sense objects in contact with
the body surface, and (2) the distance senses, such as vision and
hearing, which let us sense objects distant from the body. Olfaction
is somewhere in between, using chemical signals “right under our
noses” to sense nonproximate objects. Moreover, even the proximate senses can yield information about nonproximate objects, as
when we feel the wind or the heat of a fire. More generally, much
of our appreciation of the world around us rests on the unconscious
fusion of data from diverse sensory systems.
The appropriate activity of the effectors must depend on comparing where the system should be—the current target of an ongoing movement—with where it is now. Thus, in addition to the

external receptors, there are receptors that monitor the activity of
muscles, tendons, and joints to provide a continual source of feedback about the tensions and lengths of muscles and the angles of
the joints, as well as their velocities. The vestibular system in the
head monitors gravity and accelerations. Here, the receptors are
hair cells monitoring fluid motion. There are also receptors to monitor the chemical level of the bloodstream and the state of the heart
and the intestines. Cells in the liver monitor glucose, while others
in the kidney check water balance. Receptors in the hypothalamus,
itself a part of the brain, also check the balance of water and sugar.
The hypothalamus then integrates these diverse messages to direct
behavior or other organs to restore the balance. If we stimulate the
hypothalamus, an animal may drink copious quantities of water or
eat enormous quantities of food, even though it is already well
supplied; the brain has received a signal that water or food is lacking, and so it instructs the animal accordingly, irrespective of whatever contradictory signals may be coming from a distended
stomach.

Basic Properties of Neurons
To understand the processes that intervene between receptors and
effectors, we must have a closer look at “the” neuron. As already
emphasized, there is no such thing as a typical neuron. However,
we will summarize properties shared by many neurons. The “basic
neuron” shown in Figure 1 is abstracted from a motor neuron of
mammalian spinal cord. From the soma (cell body) protrudes a
number of ramifying branches called dendrites; the soma and dendrites constitute the input surface of the neuron. There also extrudes
from the cell body, at a point called the axon hillock (abutting the
initial segment), a long fiber called the axon, whose branches form
the axonal arborization. The tips of the branches of the axon, called
nerve terminals or boutons, impinge on other neurons or on effectors. The locus of interaction between a bouton and the cell on
which it impinges is called a synapse, and we say that the cell with
the bouton synapses upon the cell with which the connection is
made. In fact, axonal branches of some neurons can have many
varicosities, corresponding to synapses, along their length, not just
at the end of the branch.
We can imagine the flow of information as shown by the arrows
in Figure 1. Although “conduction” can go in either direction on
the axon, most synapses tend to “communicate” activity to the dendrites or soma of the cell they synapse upon, whence activity passes
to the axon hillock and then down the axon to the terminal arbo-

Figure 1. A “basic neuron” abstracted from a
motor neuron of mammalian spinal cord. The
dendrites and soma (cell body) constitute the major part of the input surface of the neuron. The
axon is the “output line.” The tips of the branches
of the axon form synapses upon other neurons or
upon effectors (although synapses may occur
along the branches of an axon as well as at the
ends). (From Arbib, M. A., 1989, The Metaphorical Brain 2: Neural Networks and Beyond,
New York: Wiley-Interscience, p. 52. Reproduced with permissions. Copyright 䉷 1989 by
John Wiley & Sons, Inc.)

I.1. Introducing the Neuron
rization. The axon can be very long indeed. For instance, the cell
body of a neuron that controls the big toe lies in the spinal cord
and thus has an axon that runs the complete length of the leg. We
may contrast the immense length of the axon of such a neuron with
the very small size of many of the neurons in our heads. For example, amacrine cells in the retina have branchings that cannot
appropriately be labeled dendrites or axons, for they are short and
may well communicate activity in either direction to serve as local
modulators of the surrounding network. In fact, the propagation of
signals in the “counter-direction” on dendrites away from the soma
has in recent years been seen to play an important role in neuronal
function, but this feature is not included in the account of the “basic
neuron” given here (see DENDRITIC PROCESSING—titles in SMALL
CAPS refer to articles in Part III).
To understand more about neuronal “communication,” we emphasize that the cell is enclosed by a membrane, across which there
is a difference in electrical charge. If we change this potential difference between the inside and outside, the change can propagate
in much the same passive way that heat is conducted down a rod
of metal: a normal change in potential difference across the cell
membrane can propagate in a passive way so that the change occurs
later, and becomes smaller, the farther away we move from the site
of the original change. This passive propagation is governed by the
cable equation
V
2V
⳱ 2
t
x
If the starting voltage at a point on the axon is V0, and no further
conditions are imposed, the potential will decay exponentially, having value V(x) ⳱ V0eⳮx at distance x from the starting point, where
the length unit, the length constant, is the distance in which the
potential changes by a factor of 1/e. This length unit will differ
from axon to axon. For “short” cells (such as the rods, cones, and
bipolar cells of the retina), passive propagation suffices to signal a
potential change from one end to the other; but if the axon is long,
this mechanism is completely inadequate, since changes at one end
will decay almost completely before reaching the other end. Fortunately, most nerve cells have the further property that if the
change in potential difference is large enough (we say it exceeds a
threshold), then in a cylindrical configuration such as the axon, a
pulse can be generated that will actively propagate at full amplitude
instead of fading passively.
If propagation of various potential differences on the dendrites
and soma of a neuron yields a potential difference across the membrane at the axon hillock which exceeds a certain threshold, then
a regenerative process is started: the electrical change at one place
is enough to trigger this process at the next place, yielding a spike
or action potential, an undiminishing pulse of potential difference
propagating down the axon. After an impulse has propagated along
the length of the axon, there is a short refractory period during
which a new impulse cannot be propagated along the axon.
The propagation of action potentials is now very well understood. Briefly, the change in membrane potential is mediated by
the flow of ions, especially sodium and potassium, across the membrane. Hodgkin and Huxley (1952) showed that the conductance
of the membrane to sodium and potassium ions—the ease with
which they flow across the membrane—depends on the transmembrane voltage. They developed elegant equations describing the
voltage and time dependence of the sodium and potassium conductances. These equations (see the article AXONAL MODELING in
Part III) have given us great insight into cellular function. Much
mathematical research has gone into studying Hodgkin-Huxleylike equations, showing, for example, that neurons can support
rhythmic pulse generation even without input (see OSCILLATORY
AND BURSTING PROPERTIES OF NEURONS), and explicating trig-

5

gered long-distance propagation. Hodgkin and Huxley used curve
fitting from experimental data to determine the terms for conductance change in their model. Subsequently, much research has
probed the structure of complex molecules that form channels
which selectively allow the passage of specific ions through the
membrane (see ION CHANNELS: KEYS TO NEURONAL SPECIALIZATION). This research has demonstrated how channel properties
not only account for the terms in the Hodgkin-Huxley equation,
but also underlie more complex dynamics which may allow even
small patches of neural membrane to act like complex computing
elements. At present, most artificial neurons used in applications
are very simple indeed, and much future technology will exploit
these “subneural subtleties.”
An impulse traveling along the axon from the axon hillock triggers new impulses in each of its branches (or collaterals), which
in turn trigger impulses in their even finer branches. Vertebrate
axons come in two varieties, myelinated and unmyelinated. The
myelinated fibers are wrapped in a sheath of myelin (Schwann cells
in the periphery, oligodendrocytes in the CNS—these are glial
cells, and their role in axonal conduction is the primary role of glia
considered in neural modeling to date). The small gaps between
successive segments of the myelin sheath are called nodes of Ranvier. Instead of the somewhat slow active propagation down an
unmyelinated fiber, the nerve impulse in a myelinated fiber jumps
from node to node, thus speeding passage and reducing energy
requirements (see AXONAL MODELING).
Surprisingly, at most synapses, the direct cause of the change in
potential of the postsynaptic membrane is not electrical but chemical. When an impulse arrives at the presynaptic terminal, it causes
the release of transmitter molecules (which have been stored in the
bouton in little packets called vesicles) through the presynaptic
membrane. The transmitter then diffuses across the very small synaptic cleft to the other side, where it binds to receptors on the
postsynaptic membrane to change the conductance of the postsynaptic cell. The effect of the “classical” transmitters (later we shall
talk of other kinds, the neuromodulators) is of two basic kinds:
either excitatory, tending to move the potential difference across
the postsynaptic membrane in the direction of the threshold (depolarizing the membrane), or inhibitory, tending to move the polarity away from the threshold (hyperpolarizing the membrane).
There are some exceptional cell appositions that are so large or
have such tight coupling (the so-called gap junctions) that the impulse affects the postsynaptic membrane without chemical mediation (see NEOCORTEX: CHEMICAL AND ELECTRICAL SYNAPSES).
Most neural modeling to date focuses on the excitatory and inhibitory interactions that occur on a fast time scale (a millisecond,
more or less), and most biological (as distinct from technological)
models assume that all synapses from a neuron have the same
“sign.” However, neurons may also secrete transmitters that modulate the function of a circuit on some quite extended time scale.
Modeling that takes account of this neuromodulation (see SYNAPTIC INTERACTIONS and NEUROMODULATION IN INVERTEBRATE
NERVOUS SYSTEMS) will become increasingly important in the future, since it allows cells to change their function, enabling a neural
network to switch dramatically its overall mode of activity.
The excitatory or inhibitory effect of the transmitter released
when an impulse arrives at a bouton generally causes a subthreshold change in the postsynaptic membrane. Nonetheless, the cooperative effect of many such subthreshold changes may yield a potential change at the axon hillock that exceeds threshold, and if this
occurs at a time when the axon has passed the refractory period of
its previous firing, then a new impulse will be fired down the axon.
Synapses can differ in shape, size, form, and effectiveness. The
geometrical relationships between the different synapses impinging
on the cell determine what patterns of synaptic activation will yield
the appropriate temporal relationships to excite the cell (see

6

Part I: Background

DENDRITIC PROCESSING). A highly simplified example (Figure 2)
shows how the properties of nervous tissue just presented would
indeed allow a simple neuron, by its very dendritic geometry, to
compute some useful function (cf. Rall, 1964, p. 90). Consider a
neuron with four dendrites, each receiving a single synapse from a
visual receptor, so arranged that synapses A, B, C, and D (from
left to right) are at increasing distances from the axon hillock. (This
is not meant to be a model of a neuron in the retina of an actual
organism; rather, it is designed to make vivid the potential richness
of single neuron computations.) We assume that each receptor re-

acts to the passage of a spot of light above its surface by yielding
a generator potential which yields, in the postsynaptic membrane,
the same time course of depolarization. This time course is propagated passively, and the farther it is propagated, the later and the
lower is its peak. If four inputs reached A, B, C, and D simultaneously, their effect may be less than the threshold required to
trigger a spike there. However, if an input reaches D before one
reaches C, and so on, in such a way that the peaks of the four
resultant time courses at the axon hillock coincide, the total effect
could well exceed threshold. This, then, is a cell that, although very

Figure 2. An example, conceived by
Wilfrid Rall, of the subtleties that can be
revealed by neural modeling when dendritic properties (in this case, lengthdependent conduction time) are taken
into account. As shown in Part C, the effect of simultaneously activating all inputs may be subthreshold, yet the cell
may respond when inputs traverse the cell
from right to left (D). (From Arbib,
M. A., 1989, The Metaphorical Brain 2:
Neural Networks and Beyond, New York:
Wiley-Interscience, p. 60. Reproduced
with permission. Copyright 䉷 1989 by
John Wiley & Sons, Inc.)

I.1. Introducing the Neuron
simple, can detect direction of motion across its input. It responds
only if the spot of light is moving from right to left, and if the
velocity of that motion falls within certain limits. Our cell will not
respond to a stationary object, or one moving from left to right,
because the asymmetry of placement of the dendrites on the cell
body yields a preference for one direction of motion over others
(for a more realistic account of biological mechanisms, see DIRECTIONAL SELECTIVITY). This simple example illustrates that the form
(i.e., the geometry) of the cell can have a great impact on the function of the cell, and we thus speak of form-function relations. When
we note that neurons in the human brain may have 10,000 or more
synapses upon them, we can understand that the range of functions
of single neurons is indeed immense.

Receptors and Effectors
On the “input side,” receptors share with neurons the property of
generating potentials, which are transmitted to various synapses
upon neurons. However, the input surface of a receptor does not
receive synapses from other neurons, but can transduce environmental energy into changes in membrane potential, which may then
propagate either actively or passively. (Visual receptors do not generate spikes; touch receptors in the body and limbs use spike trains
to send their message to the spinal cord.) For instance, the rods and
cones of the eye contain various pigments that react chemically to
light in different frequency bands, and these chemical reactions, in
turn, lead to local potential changes, called generator potentials, in
the membrane. If the light falling on an array of rods and cones is
appropriately patterned, then their potential changes will induce
interneuron changes to, in turn, fire certain ganglion cells (retinal
output neurons whose axons course toward the brain). Properties
of the light pattern will thus be signaled farther into the nervous
system as trains of impulses (see RETINA).
At the receptors, increasing the intensity of stimulation will
increase the generator potential. If we go to the first level of neurons that generate pulses, the axons “reset” each time they fire a
pulse and then have to get back to a state where the threshold and
the input potential meet. The higher the generator potential, the
shorter the time until they meet again, and thus the higher the
frequency of the pulse. Thus, at the “input” it is a useful first
approximation to say that intensity or quantity of stimulation is
coded in terms of pulse frequency (more stimulus  more spikes),
whereas the quality or type of stimulus is coded by different lines
carrying signals from different types of receptors. As we leave the
periphery and move toward more “computational” cells, we no
longer have such simple relationships, but rather interactions of
inhibitory cells and excitatory cells, with each inhibitory input
moving a cell away from, and each excitatory input moving it
toward, threshold.
To discuss the “output side,” we must first note that a muscle is
made up of many thousands of muscle fibers. The motor neurons
that control the muscle fibers lie in the spinal cord or the brainstem,
whence their axons may have to travel vast distances (by neuronal
standards) before synapsing upon the muscle fibers. The smallest
functional entity on the output side is thus the motor unit, which
consists of a motor neuron cell body, its axon, and the group of
muscle fibers the axon influences.
A muscle fiber is like a neuron to the extent that it receives its
input via a synapse from a motor neuron. However, the response
of the muscle fiber to the spread of depolarization is to contract.
Thus, the motor neurons which synapse upon the muscle fibers can
determine, by the pattern of their impulses, the extent to which the
whole muscle comprised of those fibers contracts, and can thus
control movement. (Similar remarks apply to those cells that secrete various chemicals into the bloodstream or gut, or those that
secrete sweat or tears.)

7

Synaptic activation at the motor end-plate (i.e., the synapse of a
motor neuron upon a muscle fiber) yields a brief “twitch” of the
muscle fiber. A low repetition rate of action potentials arriving at
a motor end-plate causes a train of twitches, in each of which the
mechanical response lasts longer than the action potential stimulus.
As the frequency of excitation increases, a second action potential
will arrive while the mechanical effect of the prior stimulus still
persists. This causes a mechanical summation or fusion of contractions. Up to a point, the degree of summation increases as the
stimulus interval becomes shorter, although the summation effect
decreases as the interval between the stimuli approaches the refractory period of the muscle, and maximum tension occurs. This
limiting response is called a tetanus. To increase the tension exerted
by a muscle, it is then necessary to recruit more and more fibers to
contract. For more delicate motions, such as those involving the
fingers of primates, each motor neuron may control only a few
muscle fibers. In other locations, such as the shoulder, one motor
neuron alone may control thousands of muscle fibers. As descending signals in the spinal cord command a muscle to contract more
and more, they do this by causing motor neurons with larger and
larger thresholds to start firing. The result is that fairly small fibers
are brought in first, and then larger and larger fibers are recruited.
The result, known as Henneman’s Size Principle, is that at any
stage, the increment of activation obtained by recruiting the next
group of motor units involves about the same percentage of extra
force being applied, aiding smoothness of movement (see MOTONEURON RECRUITMENT).
Since there is no command that a neuron may send to a muscle
fiber that will cause it to lengthen—all the neuron can do is stop
sending it commands to contract—the muscles of an animal are
usually arranged in pairs. The contraction of one member of the
pair will then act around a pivot to cause the expansion of the other
member of the pair. Thus, one set of muscles extends the elbow
joint, while another set flexes the elbow joint. To extend the elbow
joint, we do not signal the flexors to lengthen, we just stop signaling
them to contract, and then they will be automatically lengthened
as the extensor muscles contract. For convenience, we often label
one set of muscles as the “prime mover” or agonist, and the opposing set as the antagonist. However, in such joints as the shoulder, which are not limited to one degree of freedom, many muscles,
rather than an agonist-antagonist pair, participate. Most real movements involve many joints. For example, the wrist must be fixed,
holding the hand in a position bent backward with respect to the
forearm, for the hand to grip with its maximum power. Synergists
are muscles that act together with the main muscles involved. A
large group of muscles work together when one raises something
with one’s finger. If more force is required, wrist muscles may also
be called in; if still more force is required, arm muscles may be
used. In any case, muscles all over the body are involved in maintaining posture.

Neural Models
Before presenting more realistic models of the neuron (see PERSPECTIVE ON NEURON MODEL COMPLEXITY; SINGLE-CELL MODELS), we focus on the work of McCulloch and Pitts (1943), which
combined neurophysiology and mathematical logic, using the allor-none property of neuron firing to model the neuron as a binary
discrete-time element. They showed how excitation, inhibition, and
threshold might be used to construct a wide variety of “neurons.”
It was the first model to tie the study of neural nets squarely to the
idea of computation in its modern sense. The basic idea is to divide
time into units comparable to a refractory period so that, in each
time period, at most one spike can be generated at the axon hillock
of a given neuron. The McCulloch-Pitts neuron (Figure 3A) thus
operates on a discrete-time scale, t ⳱ 0, 1, 2, 3, . . . , where the

8

Part I: Background

Figure 3. a, A McCulloch-Pitts neuron operating on a discrete-time scale.
Each input has an attached weight wi, and the neuron has a threshold h.
The neuron “fires” at time t Ⳮ 1 just in case the weighted values of its
inputs at time t is at least h. b, Settings of weights and threshold for neurons
that function as an AND gate (i.e., the output fires if x1 and x2 both fire).
c, An OR gate (the output fires if x1 or x2, or both fire). d, A NOT gate (the
output fires if x1 does NOT fire).

time unit is (in biology) on the order of a millisecond. We write
y(t) ⳱ 1 if a spike does appear at time t, and y(t) ⳱ 0 if not. Each
connection, or synapse, from the output of one neuron to the input
of another has an attached weight. Let wi be the weight on the ith
connection onto a given neuron. We call the synapse excitatory if
wi  0, and inhibitory if wi  0. We also associate a threshold h
with each neuron, and assume exactly one unit of delay in the effect
of all presynaptic inputs on the cell’s output, so that a neuron “fires”
(i.e., has value 1 on its output line) at time t Ⳮ 1 if the weighted
value of its inputs at time t is at least h. Formally, if at time t the
value of the ith input is xi(t) and the output one time step later is
y(t Ⳮ 1), then
y(t Ⳮ 1) ⳱ 1

if and only if

兺i wixi(t) ⱖ h

Parts b through d of Figure 3 show how weights and threshold
can be set to yield neurons that realize the logical functions AND,
OR, and NOT. As a result, McCulloch-Pitts neurons are sufficient
to build networks that can function as the control circuitry for a
computer carrying out computations of arbitrary complexity; this
discovery played a crucial role in the development of automata
theory and in the study of learning machines. Although the
McCulloch-Pitts neuron no longer plays an active part in computational neuroscience, it is still widely used in neural computing,
especially when it is generalized so that the input and output values
can lie anywhere in the range [0, 1] and the function f (iwixi(t)),
which yields y(t Ⳮ 1), is a continuously varying function rather
than a step function. However, it is one thing to define model neurons with sufficient logical power to subserve any discrete computation; it is quite another to understand how the neurons in actual
brains perform their tasks. More generally, the problem is to select
just which units to model, and to decide how such units are to be
represented. Thus, when we turn from neural computing to computational neuroscience, we must turn to more realistic models of
neurons. On the other hand, we may say that neural computing
cannot reach its full power without applying new mechanisms
based on current and future study of biological neural networks
(see the road map Biological Neurons and Synapses).
Modern brain theory no longer uses the binary model of the
neuron, but instead uses continuous-time models that either rep-

resent the variation in average firing rate of the neuron or actually
capture the time course of membrane potentials. It is only through
such correlates of measurable brain activity that brain models can
really feed back to biological experiments. Such models also require the brain theorist to know a great deal of detailed anatomy
and physiology as well as behavioral data. Hodgkin and Huxley
(1952) have shown us how much can be learned from analysis of
membrane properties about the propagation of electrical activity
along the axon: Rall (1964; cf. Figure 2) was a leader in showing
that the study of membrane properties in a variety of connected
“compartments” of membrane in dendrite, soma, and axon can help
us understand small neural circuits, as in the OLFACTORY BULB
(q.v.) or for DENDRITIC PROCESSING (q.v.). Nonetheless, in many
cases, the complexity of compartmental analysis makes it more
insightful to use a more lumped representation of the individual
neuron if we are to assemble the model neurons to analyze large
networks. A computer simulation of the response of a whole brain
region which analyzed each component at the finest level of detail
available would be too large to run on even a network of computers.
In addition to the importance of detailed models of single neurons
in themselves, such studies can also be used to fine-tune more economical models of neurons, which can then serve as the units in
models of large networks, whether to model systems in the brain
or to design artificial neural networks which exploit subtle neural
capabilities.
We may determine units in the brain physiologically, e.g., by
electrical recording, and anatomically, e.g., by staining. In many
regions of the brain, we have an excellent correlation between
physiological and anatomical units; that is, we know which anatomical entity yields which physiological response. Unfortunately,
this is not always the case. We may have data on the electrophysiological correlates of animal behavior, and anatomical data as well,
yet not know which specific cell, defined anatomically, yields an
observed electrophysiological response. Another problem that we
confront in modeling is that we have both too much and too little
anatomical detail: too much in that there are many synapses that
we cannot put into our model without overloading our capabilities
for either mathematical analysis or computer simulation, and too
little in that we often do not know which details of synaptology
may determine the most important modes of behavior of a particular region of the brain. Judicious choices from available data, and
judicious hypotheses concerning missing data, must thus be made
in setting up a model, leading to the design of experiments whose
results may either confirm these hypotheses or lead to their modification. An important point of good modeling methodology is thus
to set up simulations in such a way that we can use different connectivity on different simulations, both to test alternative hypotheses and to respond to new data as they become available.
The simplest “realistic” model consonant with the above material
is the leaky integrator model. Although some biological neurons
communicate by the passive propagation (cable equation) of membrane potential down their (necessarily short) axons, most communicate by the active propagation of “spikes.” The generation and
propagation of such spikes has been described in detail by the
Hodgkin-Huxley equations. However, the leaky integrator model
omits such details. It is a continuous-time model based on using
the firing rate (e.g., the number of spikes traversing the axon in the
most recent 20 ms) as a continuously varying output measure of
the cell’s activity, in which the internal state of the neuron is described by a single variable, the membrane potential at the spike
initiation zone. The firing rate is approximated by a simple, sigmoid
function of the membrane potential. That is, we introduce a function r of the membrane potential m such that r(m) increases from
0 to some maximum value as m increases from ⳮ to Ⳮ (e.g.,
the sigmoidal function k/[l Ⳮ exp(ⳮm/h)], increasing from 0 to its
maximum k). Then the firing rate M(t) of the cell is given by the

I.1. Introducing the Neuron
equation:
M(t) ⳱ r(m(t))
The time evolution of the cell’s membrane potential is given by
a differential equation. Consider first the simple equation
s

dm(t)
⳱ ⳮm(t) Ⳮ h
dt

(1)

We say that m(t) is in an equilibrium if it does not change under
the dynamics described by the differential equation. However,
dm(t)/dt ⳱ 0 if and only if m(t) ⳱ h, so that h is the unique
equilibrium of Equation 1. To get more information, we now integrate Equation 1 to get
m(t) ⳱ eⳮt/sm(0) Ⳮ (1 ⳮ eⳮt/s)h
which tends to the resting level h with time constant s with increasing t so long as s is positive. We now add synaptic inputs to obtain
s

dm(t)
⳱ ⳮm(t) Ⳮ
dt

兺i wi Xi(t) Ⳮ h

(2)

where Xi(t) is the firing rate at the ith input. Thus, an excitatory
input (wi  0) will be such that increasing it will increase dm(t)/
dt, while an inhibitory input (wi  0) will have the opposite effect.
A neuron described by Equation 2 is called a leaky integrator neuron. This is because the equation
s

dm(t)
⳱
dt

兺i wi Xi(t)

(3)

would simply integrate the inputs with scaling constant s,
m(T) ⳱ m(0) Ⳮ

1
s

T

冮 兺 w X (t)dt
0

i

i

(4)

i

but the ⳮm(t) term in Equation 3 opposes this integration by a
“leakage” of the potential m(t) as it tries to return to its input-free
equilibrium h.
It should be noted that, even at this simple level of modeling,
there are alternative models. In the foregoing model, we have used
subtractive inhibition. But there are inhibitory synapses which
seem better described by shunting inhibition which, applied at a
given point on a dendrite, serves to divide, rather than subtract
from, the potential change passively propagating from more distal
synapses. Again, the “lumped frequency” model cannot model the
relative timing effects crucial to our motion detector example (see
Figure 2). These might be approximated by introducing appropriate
delay terms
s

dm(t)
⳱ ⳮm(t) Ⳮ
dt

兺i wi Xi(t ⳮ ti) Ⳮ h

Another class of neuron models—spiking neurons, including
integrate-and-fire neurons—are intermediate in complexity between leaky integrator models in which the output is the average
firing rate (see RATE CODING AND SIGNAL PROCESSING) and detailed biophysical models in which the fine details of action potential generation are modeled using the Hodgkin-Huxley equation. In
these intermediate models, the output is a spike whose timing is
continuously variable as a result of cellular interactions, but the
spike is represented simply by its time of occurrence, with no internal structure. For example, one may track the continuous variable (4), then generate a spike each time this quantity reaches
threshold, while simultaneously resetting the integral to some baseline value (see INTEGRATE-AND-FIRE NEURONS AND NETWORKS).

9

Such models include the ability to transmit information very rapidly through small temporal differences between the spikes sent out
by different neurons (see SPIKING NEURONS, COMPUTATION WITH).
All this reinforces the observation that there is no modeling approach that is automatically appropriate. Rather, we seek to find
the simplest model adequate to address the complexity of a given
range of problems. The articles in Part III of the Handbook will
provide many examples of the diversity of neural models appropriate to different tasks.

More Detailed Properties of Neurons
In Section I.3, the only details we will add to the neuron models
just presented will be various, relatively simple, rules of synaptic
plasticity. This level of detail (though with many variations) will
suffice for a fair range of models of biological neural networks,
and for a range of current work on artificial neural networks
(ANNs). The road map Biological Neurons and Synapses in Part
II surveys a set of articles that demonstrate that biological neurons
are vastly more complex than the present models suggest. Other
road maps show the special structures revealed in “specialpurpose” neural circuitry in different species of animals. Table 1
lists some of the relevant articles on such circuits, together with
the specific animal types on which the studies were based. The
point is that much is to be learned from features specific to many
different types of nervous systems, as well as from studies in humans, monkeys, cats, and rats that focus on commonalities with
the human nervous system.
An appreciation of this complexity is necessary for the computational neuroscientist wishing to address the increasingly detailed
database of experimental neuroscience, but it should also prove
important for the technologist looking ahead to the incorporation
of new capabilities into the next generation of ANNs. Nonetheless,
much can be accomplished with simple models, as we shall see in
Section I.3.

Table 1. A Sampling of Articles Showing the Lessons to be Learned
from the Study of Nervous Systems Very Different from Those of
Humans
Crustacean Stomatogastric System
Development of Retinotectal Maps
Echolocation: Cochleotopic and
Computational Maps
Electrolocation
Half-Center Oscillators Underlying Rhythmic
Movements
Invertebrate Models of Learning
Locomotion, Invertebrate
Locust Flight: Components and Mechanisms
in the Motor
Motor Primitives
Neuromodulation in Invertebrate Nervous
Systems
Oscillatory and Bursting Properties of
Neurons
Scratch Reflex
Sound Localization and Binaural Processing
Spinal Cord of Lamprey: Generation of
Locomotor Patterns
Visual Course Control in Flies
Visuomotor Coordination in Frog and Toad
Visuomotor Coordination in Salamander

Crabs and lobsters
Frogs
Bats
Electric fish
Various
Aplysia and Hermissenda
Various insects
Locusts
Frogs
Various
Various
Turtles
Owls
Lampreys
Flies
Frogs and toads
Salamanders

10

Part I: Background

I.2. Levels and Styles of Analysis
Many articles in this book show the benefits of interplay between
biology and technology. Nonetheless, it is essential to distinguish
between studying the brain and building an effective technology
for intelligent systems and computation, and to distinguish among
the various levels of investigation that exist (from the molecular to
the system level) in these related, but by no means identical, disciplines. The present section provides a fuller sense of the disciplines that come together in brain theory and neural networks, and
of the different levels of analysis involved in the study of complex
biological and technological systems.

A Historical Fragment
Perhaps the simplest history of brain theory and neural networks
would restrict itself to just three items: studies by McCulloch and
Pitts (1943), Hebb (1949), and Rosenblatt (1958). These publications introduced the first model of neural networks as “computing
machines,” the basic model of network self-organization, and the
model of “learning with a teacher,” respectively. (Section I.3 provides a semitechnical introduction to this work and a key set of
currently central ideas that build upon it.) The present historical
fragment is designed to take us up to 1948, the year preceding the
publication of Hebb’s book, to reveal our present federation of
disciplines as the current incarnation of what emerged in the 1940s
and is aptly summed up in the title of the book, Cybernetics: Or
Control and Communication in the Animal and the Machine (Wiener, 1948). But whereas Wiener’s view of cybernetics was dominated by concepts of control and communication, our subject is
dominated by notions of parallel and distributed computation, with
special attention to learning in neural networks. On the other hand,
notions of information and statistical mechanics championed by
Wiener have reemerged as a strong strand in the study of neural
networks today (see, e.g., the articles FEATURE ANALYSIS and STATISTICAL MECHANICS OF NEURAL NETWORKS in Part III). The articles in Part III will make abundantly clear how far we have come
since 1948, and also how many problems remain. My intent in the
present “fragment” is to enrich the reader’s understanding of current contributions by using a selective historical tour to place them
in context.
Noting that the Greek word cybernetics (jtbeqmeser) means the
helmsman of a ship (cf. the Latin word gubernator, which gives
us the word “governor” in English), Wiener (1948) used the term
for a subject in which feedback played a central role. Feedback is
the process whereby, e.g., the helmsman notes the “error,” the extent to which he is off course, and “feeds it back” to decide which
way to move the rudder. We can see the importance of this concept
in endowing automata (“self-moving” machines) with flexible behavior. Two hundred years earlier, in L’Homme machine, La Mettrie had suggested that such automata as the mechanical duck and
flute player of Vaucanson indicated the possibility of one day building a mechanical man that could talk. While these clockwork automata were capable of surprisingly complex behavior, they lacked
a crucial aspect of animal behavior, let alone human intelligence:
they were unable to adapt to changing circumstances. In the following century, machines were built that could automatically
counter disturbances to restore desired performance. Perhaps the
best-known example of this is Watt’s governor for the steam engine, which would let off excess steam if the velocity of the engine
became too great. This development led to Maxwell’s (1868) paper,
“On Governors,” which laid the basis for both the theory of negative feedback and the study of system stability (both of which are
discussed in Section I.3). Negative feedback was feedback in which
the error (in Watt’s case, the amount by which actual velocity ex-

ceeded desired velocity) was used to counteract the error; stability
occurred if this feedback was apportioned to reduce the error toward zero. Bernard (1878) brought these notions back to biology
with his study of what Cannon (1939) would later dub homeostasis,
observing that physiological processes often form circular chains
of cause and effect that could counteract disturbances in such variables as body temperature, blood pressure, and glucose level in the
blood. In fact, following publication of Wiener’s 1948 book, the
Josiah Macy, Jr., Foundation conferences, in which many of the
pioneers of cybernetics were involved, became referred to as the
Cybernetics Group, with the proceedings entitled Cybernetics: Circular Causal and Feedback Mechanisms in Biological and Social
Systems, (see Heims, 1991, for a history of the conferences and
their participants).
The nineteenth century also saw major developments in the
understanding of the brain. At an overall anatomical level, a major
achievement was the understanding of localization in the cerebral
cortex (see Young, 1970, for a history). Magendie and Bell had
discovered that the dorsal roots of the spinal cord were sensory,
carrying information from receptors in the body, while the ventral
roots (on the belly side) were motor, carrying commands to the
muscles. Fritsch and Hitzig, and then Ferrier, extended this principle to the brain proper, showing that the rear of the brain contains the primary receiving areas for vision, hearing, and touch,
while the motor cortex is located in front of the central fissure.
All this understanding of localization in the cerebral cortex led to
the nineteenth century neurological doctrine, perhaps best exemplified in Lichtheim’s (1885) development of the insights of Broca
and Wernicke into brain mechanisms of language, which viewed
different mental “faculties” as being localized in different regions
of the brain. Thus, neurological deficits were to be explained as
much in terms of lesions of the connections linking two such
regions as in terms of lesions to the regions themselves. We may
also note a major precursor of the connectionism of this volume,
where the connections are those between neuron-like units rather
than anatomical regions: the associationist psychology of Alexander Bain (1868), who represented associations of ideas by the
strengths of connections between “neurons” representing those
ideas.
Around 1900, two major steps were taken in revealing the finer
details of the brain. In Spain, Santiago Ramón y Cajal (e.g., 1906)
gave us exquisite anatomical studies of many regions of the brain,
revealing the particular structure of each as a network of neurons.
In England, the physiological studies of Charles Sherrington (1906)
on reflex behavior provided the basic physiological understanding
of synapses, the junction points between the neurons. Somewhat
later, in Russia, Ivan Pavlov (1927), extending associationist psychology and building on the Russian studies of reflexes by Sechenov in the 1860s, established the basic facts on the modifiability
of reflexes by conditioning (see Fearing, 1930, for a historical
review).
A very different setting of the scene for cybernetics came from
work in mathematical logic in the 1930s. Kurt Gödel published his
famous Incompleteness Theorem in 1931 (see Arbib, 1987, for a
proof as well as a debunking of the claim that Gödel’s theorem sets
limits on machine intelligence). The “formalist” program initiated
by David Hilbert, which sought to place all mathematical truth
within a single formal system, had reached its fullest expression in
the Principia Mathematica of Whitehead and Russell. But Gödel
showed that, if one used the approach offered in Principia Mathematica to set up consistent axioms for arithmetic and prove theorems by logical deduction from them, the theory must be incomplete, no matter which axioms (“knowledge base”) one started

I.2. Levels and Styles of Analysis
with—there would be true statements of arithmetic that could not
be deduced from the axioms.
Following Gödel’s 1931 study, many mathematical logicians
sought to formalize the notion of an effective procedure, of what
could and could not be done by explicitly following an algorithm
or set of rules. Kleene (1936) developed the theory of partial recursive functions; Turing (1936) developed his machines; Church
(1941) developed the lambda calculus, the forerunner of McCarthy’s list processing language, LISP, a one-time favorite of artificial
intelligence (AI) workers; while Emil Post (1943) introduced systems for rewriting strings of symbols, of which Chomsky’s early
formalizations of grammars in 1959 were a special case. Fortunately, these methods proved to be equivalent. Whatever could be
computed by one of these methods could be computed by any other
method if it were equipped with a suitable “program.” It thus came
to be believed (Church’s thesis) that if a function could be computed by any machine at all, it could be computed by each one of
these methods.
Turing (1936) helped chart the limits of the computable with his
notion of what is now called a Turing machine, a device that followed a fixed, finite set of instructions to read, write, and move
upon a finite but indefinitely extendible tape, each square of which
bore a symbol from some finite alphabet. As one of the ingredients
of Church’s thesis, Turing offered a “psychology of the computable,” making plausible the claim that any effectively definable computation, that is, anything that a human could do in the way of
symbolic manipulation by following a finite and completely explicit set of rules, could be carried out by such a machine equipped
with a suitable program. Turing also provided the most famous
example of a noncomputable problem, “the unsolvability of the
Halting Problem.” Let p be the numerical code for a Turing machine program, and let x be the code for the initial contents of a
Turing machine’s tape. Then the halting function h( p, x) ⳱ 1 if
Turing machine p will eventually halt if started with data x; otherwise it is 0. Turing showed that there was no “computer program”
that could compute h.
And so we come to 1943, the key year for bringing together the
notions of control mechanism and intelligent automata.
In “A Logical Calculus of the Ideas Immanent in Nervous Activity,” McCulloch and Pitts (1943) united the studies of neurophysiology and mathematical logic. Their formal model of the neuron as a threshold logic unit (see Section I.1) built on the neuron
doctrine of Ramón y Cajal and the excitatory and inhibitory synapses of Sherrington, using notation from the mathematical logic
of Whitehead, Russell, and Carnap. McCulloch and Pitts provided
the “physiology of the computable” by showing that the control
box of any Turing machine, the essential formalization of symbolic
computation, could be implemented by a network (with loops) of
their formal neurons. The ideas of McCulloch and Pitts influenced
John von Neumann and his colleagues when they defined the basic
architecture of stored program computing. Thus, as electronic computers were built toward the end of World War II, it was understood
that whatever they could do could be done by a network of neurons.
Craik’s (1943) book, The Nature of Explanation, viewed the
nervous system “as a calculating machine capable of modeling or
paralleling external events,” suggesting that the process of forming
an “internal model” that paralleled the world is the basic feature of
thought and explanation. In the same year, Rosenblueth, Wiener,
and Bigelow published “Behavior, Purpose and Teleology.” Engineers had noted that if feedback used in controlling the rudder of
a ship were too brusque, the rudder would overshoot, compensatory
feedback would yield a larger overshoot in the opposite direction,
and so on and so on as the system wildly oscillated. Wiener and
Bigelow asked Rosenblueth whether there was any corresponding
pathological condition in humans and were given the example of
intention tremor associated with an injured cerebellum. This evi-

11

dence for feedback within the human nervous system (see MOTOR
BIOLOGICAL AND THEORETICAL) led the three scientists
to advocate that neurophysiology move beyond the Sherringtonian
view of the CNS as a reflex device adjusting itself in response to
sensory inputs. Rather, setting reference values for feedback systems could provide the basis for analysis of the brain as a purposive
system explicable only in terms of circular processes, that is, from
nervous system to muscles to the external world and back again
via receptors.
Such studies laid the basis for the emergence of cybernetics,
which in turn gave birth to a number of distinct new disciplines,
such as AI, biological control theory, cognitive psychology, and
neural modeling, which each went their separate ways in the 1970s.
The next subsection introduces a number of these disciplines and
the relations between them; this analysis will continue in many
articles in Part III of the Handbook.
CONTROL,

Brains, Machines, and Minds
Brains. Brain theory comprises many different theories as to how
the structures of the brain can subserve such diverse functions as
perception, memory, control of movement, and higher mental function. As such, it includes both attempts to extend notions of computing, as well as applications of modern electronic computers to
explore the performance of complex models. An example of the
former is the study of cooperative computation between different
structures in the brain which seeks to offer a new paradigm for
computing that transcends classical notions associated with serial
execution of symbolic programs. For the latter, computational neuroscience makes systematic use of mathematical analysis and computer simulation to provide ever better models of the structure and
function of living brains, building on earlier work in both neural
modeling and biological control theory.
Machines. Artificial intelligence studies how computers may be
programmed to yield “intelligent” behavior without necessarily attempting to provide a correlation between structures in the program
and structures in the brain. Robotics is related to AI but emphasizes
the flexible control of machines (robots) which have receptors (e.g.,
television cameras) and effectors (e.g., wheels, legs, arms, grippers)
that allow them to interact with the world.
Brain theory has spawned a companion field of neural computing, which involves the design of machines with circuitry inspired
by, but which need not faithfully emulate, the neural networks of
brains. Many technologists usurp the term “neural networks” for
this latter field, but we will use it as an umbrella term which may,
depending on context, describe biological nervous systems, models
thereof, and the artificial networks which (sometimes at great remove) they inspire. When the emphasis is on “higher mental functions,” neural computing may be seen as a new branch of AI (see
the road map Artificial Intelligence in Part II), but it also contributes to robotics (especially to those robot designs inspired by analysis of animal behavior), and to a wide range of technologies, including those based on image analysis, signal processing, and
control (see the road map Applications).
For the latter work, many people emphasize adaptive neural networks which, without specific programming, can adjust their connections through self-organization or to meet specifications given
by some teacher. There are also significant contributions to the
systematic design, rather than emergence through learning, of neural networks, especially for applications in low-level vision (such
as stereopsis, optic flow, and shape-from-shading). However, complex problems cannot, in general, be solved by the tuning or the
design of a single unstructured network. For example, robot control
may integrate a variety of low-level vision networks with a set
of competing and cooperating networks for motor control and its

12

Part I: Background

planning. Brain theory and neural computing thus have to address
the analysis and design, respectively, of networks of networks (see,
e.g., HYBRID CONNECTIONIST/SYMBOLIC SYSTEMS and MODULAR
AND HIERARCHICAL LEARNING SYSTEMS).
Minds. Here, I want to distinguish the brain from the mind (the
realm of the “mental”). In great part, brain theory seeks to analyze
how the brain guides the behaving organism in its interactions with
the dynamic world around it, but much of the control of such interactions is not mental, and much of what is mental is subsymbolic
and/or unconscious (see PHILOSOPHICAL ISSUES IN BRAIN THEORY
AND CONNECTIONISM and CONSCIOUSNESS, NEURAL MODELS OF).
Without offering a precise definition of “mental,” let me just say
that many people can agree on examples of mental activity (perceiving a visual scene, reading, thinking, etc.) even if they take the
diametrically opposite philosophical positions of dualism (mind
and brain are separate) or monism (mind is a function of brain).
They would then agree that some mental activity (e.g., contemplation) need not result in overt “interactions with the dynamic real
world,” and that much of the brain’s activity (e.g., controlling normal breathing) is not mental. Face recognition seems to be a mental
activity that we do not carry out through symbol manipulation.
Indeed, even psychologists who reject Freud’s particular psychosexual theories accept his notion that much of our mental behavior
is shaped by unconscious forces (for an assessment of Freud and
an account of consciousness, see Arbib and Hesse, 1986).
Cognitive psychology attempts to explain the mind in terms of
“information processing” (a notion which is continuing to change).
It thus occupies a middle ground between brain theory and AI in
which the model must explain psychological data (e.g., what tasks
are hard for humans, people’s ability at memorization, the development of the child, patterns of human errors, etc.) but in which
the units of the model need not correspond to actual brain structures. In the 1960s and 1970s, the majority of cognitive psychologists formulated their theories in terms of information theory and/
or symbol manipulation, while theories of biological organization
were ignored. However, workers in both AI and cognitive psychology now pay increasing attention to the cooperative computation paradigm. The term connectionism has come to be used for
studies that model human thought and behavior in terms of parallel
distributed networks of neuron-like units, with learning mediated
by changes in strength of the connections between these elements
(see COGNITIVE MODELING: PSYCHOLOGY AND CONNECTIONISM).
The study of brain theory and neural networks thus has a twofold
aim: (1) to enhance our understanding of human thought and the
neural basis of human and animal behavior (brain theory), and (2)
to learn new strategies for building “intelligent” machines or adaptive robots (neural computing). In either case, we seek organizational principles that will help us understand how neurons (whether
biological or artificial) can work together to yield complex patterns
of behavior. Brain theory requires empirical data to shape and constrain modeling, but in return provides concepts and hypotheses to
shape and constrain experimentation. In neural computing, the criterion for success is the design of a machine that can perform a
task cheaply, reliably, and effectively, even if, in the process of
making the best use of available (e.g., silicon) technology, the final
design departs radically from the biological neural network that
inspired it. It will be important in reading this Handbook, then, to
be clear as to whether a particular study is an exercise in brain
theory/computational neuroscience or in AI/neural computing.
What will not be in doubt is that the influence of these subjects
works both ways: not only can brain mechanisms inspire new technology, but new technologies provide metaphors to drive new theories of brain function. To this it must be added that most workers
in ANNs know little of brain function, and relatively few neuroscientists have a deep understanding of brain theory or know much

of neural computing beyond the basic ideas of Hebbian plasticity
and, perhaps, backpropagation (see Section I.3). However, the level
of interchange has increased since the first edition of this Handbook
appeared, and this new edition is designed to further increase the
flow of information between these scientific communities.

Levels of Analysis
Whether the emphasis is on humans, animals, or machines, it becomes clear that we can seek insight at many different levels of
analysis; from large information processing blocks down to the
finest details of molecular structure. Much of psychology and linguistics looks at human behavior “from the outside,” whether
studying overall competence or attending to details of performance.
Neuropsychology relates behavior to the interaction of various
brain regions. Neurophysiology studies the activity of neurons,
both to understand the intrinsic properties of the neurons and to
help understand their role in the subsystems dissected out by the
neuropsychologist, such as networks for pattern recognition or for
visuomotor coordination. Molecular and cell biology and biophysics correlate the structure and connectivity of the membranes and
subcellular systems which constitute cells with the way these cells
transform incoming patterns or subserve memory by changing
function with repeated interactions.
These differing levels make it possible to focus individual research studies, but they are ill-defined, and a scientist who works
on any one level needs to make occasional forays, both downward
to find mechanisms for the functions studied, and upward to understand what role the studied function can play in the overall
scheme of things. Top-down modeling starts from some overall
behavior and explains it in terms of the interaction of high-level
functional units, while bottom-up modeling starts from the interaction of individual neurons (or even smaller units) to explain network properties. It requires a judicious blend of the two to connect
the clear overview of crucial questions to the hard data of neuroscience or, in the case of neural engineering, to the details of implementation. Most successful modeling will be purely bottom-up
or top-down only in its initial stages, if at all—constraints on an
initial top-down model will be given, for example, by the data on
regional localization offered by the neurologist, or the circuit-cellsynapse studies of much current neuroscience.
We must now distinguish the brain’s computation from connectionist computation “in the style of the brain.” If a connectionist
model succeeds in describing some psychological input/output behavior, it may become an important hypothesis that its internal
structure is “real” (see RECURRENT NETWORKS: NEUROPHYSIOLOGICAL MODELING). In general, however, much additional work
will be required to find and assimilate neurophysiological data to
provide brain models in which the neurons are not mere formal
units but actually represent biological neurons in the brain.
Much study of the brain is guided by evolutionary and comparative studies of animal behavior and brain function (cf. EVOLUTION
OF THE ANCESTRAL VERTEBRATE BRAIN and related articles in the
road map Neuroethology and Evolution). The information about
the function of the human brain that is gained in the neurological
clinic or during neurosurgery can thus be supplemented by humane
experimentation on animals. (However, as evidenced by Table
1 of Section I.1, we can learn a great deal by studying the differences, as well as the similarities, between the brains of different
species.) We learn by stimulating, recording from, or excising portions of an animal’s brain and seeing how the animal’s behavior
changes. We may then compare such results with observations using such techniques as positron emission tomography (PET) or
functional magnetic resonance imaging (fMRI) of the relative activity of different parts of the human brain during different tasks
(see IMAGING THE GRAMMATICAL BRAIN, IMAGING THE MOTOR

I.2. Levels and Styles of Analysis
BRAIN, and IMAGING THE VISUAL BRAIN). The grand aim of cognitive neuroscience (as neuropsychology has now become; see the
Cognitive Neuroscience road map) is to use clinical data and brain
imaging to form a high-level view of the involvement of various
brain regions in human cognition, using single-cell activity recorded from animals engaged in analogous behaviors to suggest
the neural networks underlying this involvement (see SYNTHETIC
FUNCTIONAL BRAIN MAPPING). The catch, of course, is that the
“analogous behaviors” of animals are not very analogous at all
when it comes to such symbolic activities as language and reasoning. In Part III, we will see that “higher mental functions” tend to
be modeled more in connectionist terms constrained (if at all) by
psychological or psycholinguistic data (cf. the Part II road maps
Psychology and Linguistics and Speech Processing), while the
greatest successes in seeking the neural underpinnings of human
behavior have come in areas such as vision, memory, and motor
control, where we can make neural network models of animal analogues of human capabilities (cf. the road maps Vision, Other
Sensory Systems, Neural Plasticity, Biological Networks, Motor
Pattern Generators, and Mammalian Motor Control).
We also learn from the attempt to reproduce various aspects of
human behavior in a robot, even though human action, memory,
learning, and perception are far richer than those of any machine
yet built or likely to be built in the near future (see BIOLOGICALLY
INSPIRED ROBOTICS). Thus, when we suggest that the brain can be
thought of in some ways as a (highly distributed) computer, we are
not trying to reduce humans to the level of extant machines, but
rather to understand ways in which machines give us insight into
human attributes. This type of study has been referred to as cybernetics, extending the concept of Norbert Wiener, who, as we have
seen, defined the subject as “the study of control and communication in man and machine.”
To the extent that they address “higher mental function,” the
studies presented in this Handbook suggest that there is no single
“thing” called intelligence, but rather a plexus of properties that,
taken one at a time, may be little cause for admiration, but any
sizable collection of which will yield behavior that we would label
as intelligent. Turing (1950) argued that we would certainly regard
a machine as intelligent if it could pass the following test: An
experimenter sits in a room with two teletypes by which she conducts a “conversation” with two systems. One is a human, the other
is a machine, but the experimenter is not told which is which. If,
after asking many questions, she is likely to have much doubt about
which is human and which is machine, we should, says Turing,
concede intelligence to the machine. However, unless one dogmatically insists that being intelligent entails behaving in a human
way, it is “harder” for a machine to pass this Turing test than to be
intelligent. For instance, whereas a computer can answer problems
in arithmetic quickly and correctly, a much more complex program
would be required to ensure that it answered as slowly and erratically as a human. Turing’s aim was not to find a necessary set of
conditions to ensure intelligence, but rather to devise a test which,
if passed by a machine, would convince most skeptics that the
machine had intelligence.

Schema Theory
The analysis of complex systems, whether they subserve natural or
artificial intelligence, requires a coarser grain of analysis to complement that of neural networks. To make sense of the brain, we
often divide it into functional systems—such as the motor system,
the visual system, and so on—as well as into structural subsystems—from the spinal cord and the hippocampus to the various
subdivisions of the prefrontal cortex. Similarly, in distributed AI
(see MULTIAGENT SYSTEMS), the solution of a task may be distributed over a complex set of interacting agents, each with their

13

dedicated processors for handling the information available to them
locally. Thus, both neuroscience and artificial intelligence require
a language for expressing the distribution of function across units
intermediate between overall function and the final units of analysis
(e.g., neurons or simple instructions).
Since the “units of thought” or the subfunctions of a complex
behavior may be quite high-level compared to the fine-grain computation of the myriad neurons in the human brain, SCHEMA THEORY (q.v.; see also Arbib, 1981; Arbib, Érdi, and Szentágothai,
1998, chap. 3) complements connectionism by providing a bridging
language between functional description and neural networks. It is
based on a theory of the concurrent activity of interacting functional
units called schemas. Perceptual schemas are those used for perceptual analysis, while motor schemas are those which provide the
control systems that can be coordinated to effect a wide variety of
movement. Other schemas compete and cooperate to meld action,
internal state, and perception in an ongoing action-perception cycle.
Figure 4A represents brain theory, while Figure 4B offers a similar but distinct picture for distributed AI. We may model the brain
either functionally, analyzing some behavior in terms of interacting
schemas, or structurally, through the interaction of anatomically
defined units, such as brain regions (cf. the examples in the road
map Mammalian Brain Regions) or substructures of these regions, such as layers or columns. In brain theory, we ultimately
seek an explanation in terms of neural networks, since the neuron
may be considered the basic unit of function as well as of structure,
and much further work in computational neuroscience seeks to explain the complex functionality of real neurons in terms of “subneural” units, such as membrane compartments, channels, spines,
and synapses. What makes the story more subtle is that, in general,
a functional analysis proceeding “top-down” from some overall
behavior need not map directly into a “bottom-up” analysis proceeding upward from the neural circuitry (brain theory) or basic set
of processors (distributed AI), and that several iterations from the
“middle out” may be required to bring the structural and functional
accounts into consonance. Brain theory may then seek to replace
an initially plausible schema analysis with one whose schemas may
be constituted by an assemblage of schemas which can each be
embodied in one structure (without denying that a given brain region may support the activity of multiple schemas). The schemas
that serve as the functional units in our initial hypotheses about the
decomposition of some overall function may well differ from the
more refined hypotheses which provide an account of structural
correlates as well. On the other hand, distributed AI may adopt any
schema analysis that is technologically effective, and the schemas
may be implemented in whatever medium is appropriate, whether
as conventional computer programs, ANNs, or special-purpose devices. These different approaches then rest on effective design of
VLSI “chips” or other computing materials (cf. the road map Implementation and Analysis).
For brain theory, the top-level schemas must be “large” enough
to allow an analysis of behavior at or near the psychological level,
yet also be subject to successive decomposition down to a level
that may, in certain cases, be implemented in specific neural networks. We again distinguish a schema as a functional unit from a
neural network as a structural unit. A given schema may be distributed across several neural networks; a given neural network
may be involved in the implementation of several different schemas. The same will be true for relating connectionist units to single
biological neurons. If there is to be a fuller rapprochement between
connectionism and neuropsychology, it will be important to use a
vocabulary (or context) that allows one to make the necessary distinctions between connectionist and biological neurons.
A top-down analysis (decomposing a function) may suggest that
a certain schema is embedded in a certain part of the brain; we can
then marshal the available data from anatomy and neurophysiology

14

Part I: Background

Figure 4. Views of level of analysis of brain and behavior (A) and a distributed technological system (B), highlighting the role of schemas as an intermediate
level of functional analysis in each case.

to assess whether the circuitry can, indeed, subserve an instance of
that schema. It often happens that the empirical data are inadequate.
We then make hypotheses for experimental confirmation. Alternatively, bottom-up analysis of a brain region (assembling its constituents) may suggest that it subserves a different schema from
that originally hypothesized, and we must then conduct a new topdown analysis in the light of these newfound constraints.
To illuminate the notion of experimental insight modifying an
initial top-down analysis, we consider an example from Rana computatrix, a set of models of visuomotor coordination in the frog
and toad (cf. VISUOMOTOR COORDINATION IN FROG AND TOAD).
Frogs and toads snap at small moving objects and jump away from
large ones (to oversimplify somewhat). Thus, a simple schemamodel of the frog brain might simply postulate four schemas: two
perceptual schemas (processes for recognizing objects or situations) and two motor schemas (processes for controlling some
structured behavior). One perceptual schema would recognize
small moving objects and activate a motor schema for approaching
the prey; the other would recognize large moving objects and activate a motor schema for avoiding the predator. Lesion experiments can put such a model to the test if it is enhanced by hypotheses on the localization of each schema in the brain. It was thought
that the tectum (a key visual region in the animal’s midbrain) was
the locus for recognizing small moving objects, while the pretectum (a region just in front of the tectum) was the locus for recognizing large moving objects. Based on these localization hypotheses, the model described would predict that an animal with a
lesioned pretectum would be unresponsive to large objects, but
would respond normally to small objects. However, the facts are
quite different. A pretectum-lesioned toad will approach moving
objects, both large and small, and does not exhibit avoidance behavior. This has led to a new schema model in which a perceptual
schema to recognize large moving objects is still localized in the
pretectum, but the tectum now contains a perceptual schema for all
moving objects. We then add that activity of the pretectal schema
not only triggers the avoidance motor schema but also inhibits approach. This new schema model still yields the normal behavior to
large and small moving objects, but also fits the lesion data, since
removal of the pretectum removes inhibition, meaning that the ani-

mal will now approach any moving object (Ewert and von Seelen,
1974).
We have thus seen how schemas may be used to provide falsifiable models of the brain, using lesion experiments to test
schema models of behavior, and leading to new functional models
that better match the structure of the brain. Note again that, in
different species, the map from function to brain structure may be
different, while in distributed AI the constraints are not those of
analysis but rather those of design—namely, for a given function
and a given set of processors, a schema decomposition must be
found that will map most efficiently onto a network of processors
of a certain kind.
While the brain may be considered a network of interacting
“boxes” (anatomically distinguishable structures), there is no reason to expect each such box to mediate a single function that is
well-defined from a behavioral standpoint. We have just seen that
the frog tectum is implicated in both approach and (when modulated by pretectum) avoidance behavior. The language of schemas
lets us express hypotheses about the various functions that the brain
performs without assuming localization of any one function in any
one region, but also allows us to express the way in which many
regions participate in a given function, or a given region participates in many functions.
The style of cooperative computation (see COOPERATIVE PHENOMENA) exhibited in both schema theory and connectionism is
far removed from serial computation and the symbol-based ideas
that have dominated conventional AI. As we shall see in example
after example in Part III, the brain has many specialized areas, each
with a partial representation of the world. It is only through the
interaction of these regions that the unity of behavior of the animal
emerges, and the human is no different in this regard. The representation of the world is the pattern of relationships between all
its partial representations. Much work in AI contributes to schema
theory, even when it does not use this term. For example, Brooks
(1986) builds robot controllers using layers made up of asynchronous modules that can be considered to be a version of schemas
(see REACTIVE ROBOTIC SYSTEMS). This work shares with schema
theory, with its mediation of action through a network of schemas,
the point that no single, central, logical representation of the world

I.3. Dynamics and Adaptation in Neural Networks
needs link perception and action. It is also useful to view cooperative computation as a social phenomenon. A schema is a selfcontained computing agent (object) with the ability to communicate with other agents, and whose function is specified by some
behavior. Whereas schema theory was motivated in great part by
the study of interacting brain regions (other influences are reviewed

15

in SCHEMA THEORY), much early work in distributed AI was motivated by a social analogy in which the schemas were thought of
as “agents” analogous to people interacting in a social setting to
compete or cooperate in solving some overall problem, a theme
elaborated on by Minsky (1985) and whose current status is reviewed in MULTIAGENT SYSTEMS.

I.3. Dynamics and Adaptation in Neural Networks
Section I.1 introduced a number of key concepts from the biological study of neurons, stressing the diversity of neurons both within
the human CNS and across species. It presented several simple
models of neurons, noting that computational neuroscience has
gone on to produce more subtle and complicated neuronal models,
while neural computing tends to use simple neurons augmented by
“learning rules” for changing connection strengths on the basis of
“experience.” The purpose of this section is to introduce two key
approaches that dominate the modern study of neural networks: (1)
the study of neural networks as dynamic systems (developed more
fully in the road map Dynamic Systems), and (2) the study of
neural networks as adaptive systems (see Learning in Artificial
Networks). To make this section essentially self-contained, we
start by recalling the definitions of the McCulloch-Pitts and leaky
integrator neurons from Section I.1, but we do this in the context
of a general, semiformal, introduction to dynamic systems.

Dynamic Systems
We motivate the notion of dynamic systems by considering how
to abstract the interaction of an organism (or a machine) with its
environment. The organism will be influenced by aspects of the
current environment—the inputs to the organism—while the activity of the environment will be responsive in turn to aspects of the
current activity of the organism, the outputs of the organism. The
inputs and outputs that actually enter into a theory of the organism
(or machine) are a small sampling of the flux of its interactions
with the rest of the universe. There is essentially no limit to how
many variables one could include in the analysis; a crucial task in
any theory building is to pick the “right” variables.
Depending on the context, we will use the word system to denote
either the physical reality (which we cannot know in its entirety)
or the abstraction with which we approximate it. Inputs and outputs
do not constitute a complete description of a system. We cannot
predict how someone will answer a question unless we know her
state of knowledge; nor can we tell how a computer will process
its data unless we know the instructions controlling its computation. In short, we must include a description of the internal state
of the system which determines what it will extract from its current
stimulation in determining its current actions and modifying its
internal state. Our abstraction of any real system contains five
elements:
1. The set of inputs: those variables of the environment which we
believe will affect the system behavior of interest to us.
2. The set of outputs: those variables of the system which we
choose to observe, or which we believe will significantly affect
the environment.
3. The set of states: those internal variables of the system (which
may or may not also be output variables) which determine the
relationship between input and output. Essentially, the state of
a system is the system’s “internal residue of the past”: when we

know the state of a system, no further information about the past
behavior of the system will enable us to refine predictions of
the way in which future inputs and outputs of the system will
be related.
4. The state-transition function: that function which determines
how the state will change when the system obtains various
inputs.
5. The output function: that function which determines what output
the system will yield with a given input when in a given state.

Any system in which the state-transition function and output
function uniquely determine the new state and output from a specification of the initial state and subsequent inputs is called a deterministic system. If, no matter how carefully we specify subsequent
inputs to a system, we cannot specify exactly what will be the
subsequent states and outputs, we say the system is probabilistic
or stochastic. A stochastic treatment may be worthwhile, either
because we are analyzing systems, which are “inescapably” stochastic (e.g., at the quantum level), or because we are analyzing
macroscopic systems, which lend themselves to a stochastic description by ignoring “fine details” of microscopic variables. For
example, it is usually more reasonable to describe a coin in terms
of a 0.5 probability of coming up heads than to measure the initial
placement of the coin on the finger and the thrust of the thumb in
sufficient detail to determine whether the coin will come up heads
or tails.

Continuous-Time Systems
In Newtonian mechanics, the state of the system comprises the
positions of its components, which are directly observable, and
their velocities, which can be estimated from the observed trajectory over a period of time. Time is continuous (i.e., characterized
by the set ⺢ of real numbers), and the way in which the state
changes is described by a differential equation: classical mechanics
provides the basic example of continuous-time systems in which
the present state and input determine the rate at which the state
changes. This requires that the input, output, and state spaces be
continuous spaces in which such continuous changes can occur.
Consider the simple example of a point mass undergoing rectilinear
motion. At any time, its position y(t) is the observable output of
the system, and the force u(t) acting upon it is the input applied to
the system. Newton’s third law says that the force applied to the
system equals the mass times the acceleration ÿ(t) ⳱ mu(t), where
the acceleration ÿ(t) is the second derivative of y(t). According to
Newton’s laws, the state of the system is given by the position and
velocity of the particle. We call the position-velocity pair, at any
time, the instantaneous state q(t) of the system. In fact, the earlier
equation gives us enough information to deduce the rate of change
dq(t)/dt of this state. Using standard matrix formalism, we thus

16

Part I: Background

have

冤 冥 冤 冥 冤

冥 冤 冥冤 冥 冤 冥

d y(t)
ẏ(t)
ẏ(t)
0 1 y(t)
0
⳱
⳱
⳱
Ⳮ
u(t)
ÿ(t)
mu(t)
0 0 ẏ(t)
m
dt ẏ(t)
while
y(t) ⳱

冤01冥q(t)

This is an example of a linear system in which the rate of change
of state depends linearly on the present state and input, and the
present output depends linearly on the present state. That is, there
are matrices F, G, and H such that
dq(t)
⳱ Fq(t) Ⳮ Gu(t); y(t) ⳱ Hq(t)
dt
More generally, a physical system can be expressed by a pair of
equations:
dq(t)
⳱ f (q(t), u(t))
dt
The first expresses the rate of change dq(t)/dt of the state as a
function of both the state q(t) and the input or control vector u(t)
applied at any time t; the second reads the output from the current
state.
We now present the definition of a leaky integrator neuron as a
continuous-time system. The internal state of the neuron is its membrane potential, m(t), and its output is the firing rate, M(t). The state
transition function of the cell is expressed as
dm(t)
⳱ ⳮm(t) Ⳮ
dt

兺i wi Xi(t) Ⳮ h

(1)

while the output function of the cell is given by the equation
M(t) ⳱ r(m(t))

s

dmi(t)
⳱ ⳮmi(t) Ⳮ
dt

兺j wij Xij(t) Ⳮ hi

(3)

as soon as we specify whether Xij(t) is the output Mk(t) of the kth
neuron or the value xl(t) currently being applied on the lth input
line of the overall network.

Discrete-Time Systems

y(t) ⳱ g(q(t))

s

carrying identical output signals, and either connecting each line
to a unique input of another neuron or feeding it outside the net to
provide one of the K network output lines. Then every input to a
given neuron must be connected either to an output of another
neuron or to one of the (possibly split) L input lines of the network.
Thus the input set X ⳱ ⺢L, the state set Q ⳱ ⺢N, and the output
set Y ⳱ ⺢K. If the ith output line comes from the jth neuron, then
the output function is determined by the fact that the ith component
of the output at time t is the firing rate Mj(t) ⳱ rj(Mj(t)) of the jth
neuron at time t. The state transition function for the neural network
follows from the state transition functions of each of the N neurons

(2)

Thus, if there are m inputs Xi(t), i ⳱ 1, . . . , m, then the input
space of the neuron is ⺢m, with current value (X1(t), . . . , Xm(t)),
while the state and output spaces of the neuron both equal ⺢, with
current values m(t) and M(t), respectively.
Let us now briefly (and semiformally) see how a neural network
comprised of leaky integrator neurons can also be seen as a
continuous-time system in this sense. As typified in Figure 5, we
characterize a neural network by selecting N neurons (each with
specified input weights and resting potential) and by taking the
axon of each neuron, which may be split into several branches

In contrast to continuous-time systems, which must have continuous state spaces on which the differential equations for the state
transition function can be defined, discrete-time systems may have
either continuous or discrete state spaces. (A discrete state space
is just a set with no specific metric or topological structure.) For
example, a McCulloch-Pitts neuron is considered to operate on a
discrete-time scale, t ⳱ 0, 1, 2, 3, . . . , and has connection weights
wi and threshold h. If at time t the value of the ith input is xi(t),
then the output one time step later, y(t Ⳮ 1), equals 1 if and only
if iwixi(t) ⱖ h. If there are m inputs (x1(t), . . . , xm(t)), then, since
inputs and outputs are binary, such a neuron has input set ⳱ {0,
1}m, state set ⳱ output set {0, 1} (we treat the current state and
output as being identical). On the other hand, the important learning
scheme known as backpropagation (defined later) is based on neurons which operate on discrete time, but with both input and output
taking continuous values in some range, say [0, 1].
In computer science, an automaton is a discrete-time system with
discrete input, output, and state spaces. Formally, we describe an
automaton by the sets X, Y, and Q of inputs, outputs, and states,
respectively, together with the next-state function d: Q ⳯ X r Q
and the output function b: Q r Y. If the automaton is in state q and
receives input x at time t, then its next state will be d(q, x) and its
next output will be b(q). It should be clear that a McCulloch-Pitts
neural network (i.e., a network like that shown in Figure 5, but a
discrete-time network with each neuron a McCulloch-Pitts neuron)
functions like a finite automaton, as each neuron changes state synchronously on each tick of the time scale t ⳱ 0, 1, 2, 3, . . . .
Conversely, it can be shown (see Arbib, 1987; the result was essentially, though inscrutably, due to McCulloch and Pitts, 1943)
that any finite automaton can be simulated by a suitable
McCulloch-Pitts neural network.

Stability, Limit Cycles, and Chaos

Figure 5. A neural network viewed as a system. The input at time t is the
pattern of firing on the input lines, the output is the pattern of firing on the
output lines; and the internal state is the vector of firing rates of all the
neurons of the network.

With the previous discussion, we now have more than enough material to understand the crucial dynamic systems concept of stability
and the related concepts of limit cycles and chaos (see COMPUTING
WITH ATTRACTORS and CHAOS IN NEURAL SYSTEMS). We want to
know what happens to an “unperturbed” system, i.e., one for which
the input is held constant (possibly with some specific “null input,”
usually denoted by 0, the “zero” input in X). An equilibrium is a
state q in which the system can stay at rest, i.e., such that d(q, 0)
⳱ q (discrete time) or dq/dt ⳱ f (q, 0) ⳱ 0 (continuous time). The
study of stability is concerned with the issue of whether or not this
rest point will be maintained in the face of slight disturbances. To
see the variety of equilibria, we use the image of a sticky ball

I.3. Dynamics and Adaptation in Neural Networks
rolling on the “hillside” of Figure 6. We say that point A on the
“hillside” in this diagram is an unstable equilibrium because a
slight displacement from A will tend to increase over time. Point
B is in a region of neutral equilibrium because slight displacements
will tend not to change further, while C is a point of stable equilibrium, since small displacements will tend to decrease over time.
Note the word “small”: in a nonlinear system like that of Figure 6,
a large displacement can move the ball from the basin of attraction
of C (the set of states whose dynamics tends toward C) to another
one. Clearly, the ball will not tend to return to C after a massive
displacement that moves the ball to the far side of A’s hilltop.
Many nonlinear systems have another interesting property: they
may exhibit limit cycles. These are closed trajectories in the state
space, and thus may be thought of as “dynamic equilibria.” If the
state of a system follows a limit cycle, we may also say it oscillates
or exhibits periodic behavior. A limit cycle is stable if a small
displacement will be reduced as the trajectory of the system comes
closer and closer to the original limit cycle. By contrast, a limit
cycle is unstable if such excursions do not die out. Research in
nonlinear systems has also revealed what are called strange attractors. These are attractors which, unlike simple limit cycles,
describe such complex paths through the state space that, although
the system is deterministic, a path that approaches the strange attractor gives every appearance of being random. The point here is
that very small differences in initial state may be amplified with
the passage of time, so that differences that at first are not even
noticeable will yield, in due course, states that are very different
indeed. Such a trajectory has become the accepted mathematical
model of chaos, and it is used to describe a number of physical
phenomena, such as the onset of turbulence in a weather system,
as well as a number of phenomena in biological systems (see
CHAOS IN BIOLOGICAL SYSTEMS; CHAOS IN NEURAL SYSTEMS).

Hopfield Nets
Many authors have treated neural networks as dynamical systems,
employing notions of equilibrium, stability, and so on, to classify
their performance (see, e.g., Grossberg, 1967; Amari and Arbib,
1977; see also COMPUTING WITH ATTRACTORS). However, it was
a paper by John Hopfield (1982) that was the catalyst in attracting
the attention of many physicists to this field of study. In a
McCulloch-Pitts network, every neuron processes its inputs to determine a new output at each time step. By contrast, a Hopfield net
is a net of such units with (1) symmetric weights (wij ⳱ wji) and
no self-connections (wii ⳱ 0), and (2) asynchronous updating. For
instance, let si denote the state (0 or 1) of the ith unit. At each time

Figure 6. An energy landscape: For a ball rolling on the “hillside,” point
A is an unstable equilibrium, point B lies in a region of neutral equilibrium,
and point C is a point of stable equilibrium. Point C is called an attractor:
the basin of attraction of C comprises all states whose dynamics tend toward
C.

17

step, pick just one unit at random. If unit i is chosen, si takes the
value 1 if and only if Rwijsj ⱖ hi. Otherwise si is set to 0. Note that
this is an autonomous (input-free) network: there are no inputs
(although instead of considering hi as a threshold we may consider
ⳮhi as a constant input, also known as a bias).
Hopfield defined a measure called the energy for such a net (see
ENERGY FUNCTIONALS FOR NEURAL NETWORKS)
E ⳱ ⳮ1⁄2

兺ij sisjwij Ⳮ 兺i sihi

(1)

This is not the physical energy of the neural net but a mathematical
quantity that, in some ways, does for neural dynamics what the
potential energy does for Newtonian mechanics. In general, a mechanical system moves to a state of lower potential energy just as,
in Figure 6, the ball tends to move downhill. Hopfield showed that
his symmetrical networks with asynchronous updating had a similar property.
For example, if we pick a unit and the foregoing firing rule does
not change its si, it will not change E. However, if si initially equals
0, and Rwijsj ⱖ hi, then si goes from 0 to 1 with all other sj constant,
and the “energy gap,” or change in E, is given by
DE ⳱ ⳮ1⁄2

兺j (wijsj Ⳮ wjisj) Ⳮ hi

⳱ ⳮ兺 wijsjsj Ⳮ hi, by symmetry
j

ⱕ 0 since

兺 wijsj  hi

Similarly, if si initially equals 1, and Rwijsj  hi, then si goes from
1 to 0 with all other sj constant, and the energy gap is given by
DE ⳱

兺 wijsj ⳮ hi  0

In other words, with every asynchronous updating, we have DE ⱕ
0. Hence the dynamics of the net tends to move E toward a minimum. We stress that there may be different such states—they are
local minima, just as, in Figure 6, both D and E are local minima
(each of them is lower than any “nearby” state) but not global
minima (since C is lower than either of them). Global minimization
is not guaranteed.
The expression just presented for DE depends on the symmetry
condition, wij ⳱ wji, for without this condition, the expression
would instead be DE ⳱ ⳮ(1⁄2)j(wijsj Ⳮ wjisj) Ⳮ hi and in this
case, Hopfield’s updating rule need not yield a passage to energy
minimum, but might instead yield a limit cycle, which could be
useful in, e.g., controlling rhythmic behavior (see, e.g., RESPIRATORY RHYTHM GENERATION). In a control problem, a link wij might
express the likelihood that the action represented by i would precede that represented by j, in which case wij ⳱ wji is normally
inappropriate.
The condition of asynchronous update is crucial, too. If we consider the simple “flip-flop” with w12 ⳱ w21 ⳱ 1 and h1 ⳱ h2 ⳱
0.5, then the McCulloch-Pitts network will oscillate between the
states (0, 1) and (1, 0) or will sit in the states (0, 0) or (1, 1); in
other words, there is no guarantee that it will converge to an equilibrium. However, with E ⳱ ⳮ(1⁄2)Rijsisjwij Ⳮ Risihi, we have E(0,
0) ⳱ 0, E(0, 1) ⳱ E(1, 0) ⳱ 0.5, and E(1, 1) ⳱ 0, and the Hopfield
network will converge to the global minimum at either (0, 0) or
(1, 1).
Hopfield also aroused much interest because he showed how a
number of optimization problems could be “solved” using neural
networks. (The quotes around “solved” acknowledge the fact that
the state to which a neural network converges may represent a local,
rather than a global, optimum of the corresponding optimization

18

Part I: Background

problem.) Such networks were similar to the “constraint satisfaction” networks that had already been studied in the computer vision community. (In most vision algorithms—see, e.g., STEREO
CORRESPONDENCE—constraints can be formulated in terms of
symmetric weights, so that wij ⳱ wji is appropriate.) The aim,
given a “constraint satisfaction” problem, is to so choose weights
for a neural network so that the energy E for that network is a
measure of the overall constraint violation. A famous example is
the Traveling Salesman Problem (TSP): There are n cities, with
a road of length lij joining city i to city j. The salesman wishes
to find a way to visit the cities that is optimal in two ways: each
city is visited only once, and the total route is as short as possible.
We express this as a constraint satisfaction network in the following way: Let the activity of neuron Nij express the decision to go
straight from city i to city j. The cost of this move is simply lij,
and so the total “transportation cost” is Rij1ijNij. It is somewhat
more challenging to express the cost of violating the “visit a city
only once” criterion, but we can reexpress it by saying that, for
city j, there is one and only one city i from which j is directly
approached. Thus, Rj(RiNij ⳮ 1)2 ⳱ 0 just in case this constraint
is satisfied; a non-zero value measures the extent to which this
constraint is violated. This can then be mapped into the setting of
weights and thresholds for a Hopfield network. Hopfield and Tank
(1986) constructed chips for this network which do indeed settle
very quickly to a local minimum of E. Unfortunately, there is no
guarantee that this minimum is globally optimal. The article OPTIMIZATION, NEURAL presents this and a number of other neurally
based approaches to optimization. The article SIMULATED ANNEALING AND BOLTZMANN MACHINES shows how noise may be
added to “shake” a system out of a local minimum and let it settle
into a global minimum. (Consider, for example, shaking that is
strong enough to shake the ball from D to A, and thus into the
basin of attraction of C, in Figure 6, but not strong enough to
shake the ball back from C toward D.)

Adaptation in Dynamic Systems
In the previous discussion of neural networks as dynamic systems,
the dynamics (i.e., the state transition function) has been fixed.
However, just as humans and animals learn from experience, so do
many important applications of ANNs depend on the ability of
these networks to adapt to the task at hand by, e.g., changing the
values of the synaptic weights to improve performance. We now
introduce the general notion of an adaptive system as background
to some of the most influential “learning rules” used in adaptive
neural networks. The key motivation for using learning networks
is that it may be too hard to program explicitly the behavior that
one sees in a black box, but one may be able to drive a network
by the actual input/output behavior of that box, or by some description of its trajectories, to cause it to adapt itself into a network
which approximates that given behavior. However, as we will

stress at the end of this section, a learning algorithm may not solve
a problem within a reasonable period of time unless the initial
structure of the network is suitable.

Adaptive Control
A key problem of technology is to control a complex system so
that it behaves in some desired way, whether getting a space probe
on course to Mars or a steel mill to produce high-quality steel. A
common situation that complicates this control problem is that the
controlled system may not be known accurately; it may even
change its character somewhat with time. For example, as fuel is
depleted, the mass and moments of inertia of the probe may change
in unpredicted ways. The adaptation problem involves determining, on the basis of interaction with a given system, an appropriate
“model” of the system which the controller can use in solving the
control problem.
Suppose we have available an identification procedure which
can find an adequate parametric representation of the controlled
system (see IDENTIFICATION AND CONTROL). Then, rather than
build a controller specifically designed to control this one system,
we may instead build a general-purpose controller which can accommodate to any reasonable set of parameters. The controller then
uses the parameters which the identification procedure provides as
the best estimate of the controlled system’s parameters at that time.
If the identification procedure can make accurate estimates of the
system’s parameters as quickly as they actually change, the controller will be able to act efficiently despite fluctuations in controlled system dynamics. The controller, when coupled to an identification procedure, is an adaptive controller; that is, it adapts its
control strategy to changes in the dynamics of the controlled system. However, the use of an explicit identification procedure is only
one way of building an adaptive controller. Adaptive neural nets
may be used to build adaptive procedures which may directly modify the parameters in some control rule, or identify the system inverse so that desired outputs can be automatically transformed into
the inputs that will achieve them. (See SENSORIMOTOR LEARNING
for the distinction between forward and inverse models.)

Pattern Recognition
In the setup shown in Figure 7, the preprocessor extracts from the
environment a set of “confidence levels” for various input features
(see FEATURE ANALYSIS), with the result represented by a vector
of d real numbers. In this formalization, any pattern x is represented
by a point (x1, x2, . . . , xd) in a d-dimensional Euclidean space ⺢d
called the pattern space. The pattern recognizer then takes the pattern and produces a response that may have one of K distinct values
where there are K categories into which the patterns must be sorted;
points in ⺢d are thus grouped into at least K different sets (see
CONCEPT LEARNING and PATTERN RECOGNITION). However, a

Figure 7. One strategy in pattern recognition is to precede the
adaptive neural network by a fixed layer of “preprocessors” or
“feature extractors” which replace the image by a finite vector
for further processing. In other approaches, the functions defined
by the early layers of the network may themselves be subject to
training.

I.3. Dynamics and Adaptation in Neural Networks
category might be represented in more than one region of ⺢d. To
take an example from visual pattern recognition (although the theory of pattern recognition networks applies to any classification of
⺢d), a and A are members of the category of the first letter of the
English alphabet, but they would be found in different connected
regions of a pattern space. In such cases, it may be necessary to
establish a hierarchical system involving a separate apparatus to
recognize each subset, and a further system that recognizes that the
subsets all belong to the same set (a related idea was originally
developed by Selfridge, 1959; for adaptive versions, see MODULAR
AND HIERARCHICAL LEARNING SYSTEMS). Here we avoid this
problem by concentrating on the case in which the decision space
is divided into exactly two connected regions.
We call a function f: ⺢d r ⺢ a discriminant function if the
equation f (x) ⳱ 0 gives the decision surface separating two regions
of a pattern space. A basic problem of pattern recognition is the
specification of such a function. It is virtually impossible for humans to “read out” the function they use (not to mention how they
use it) to classify patterns. Thus, a common strategy in pattern
recognition is to provide a classification machine with an adjustable
function and to “train” it with a set of patterns of known classification that are typical of those with which the machine must ultimately work. The function may be linear, quadratic, polynomial,
or even more subtle yet, depending on the complexity and shape
of the pattern space and the necessary discriminations. The experimenter chooses a class of functions with parameters which, it is
hoped, will, with proper adjustment, yield a function that will successfully classify any given pattern. For example, the experimenter
may decide to use a linear function of the form
f (x) ⳱ w1 x1 Ⳮ w2 x2 Ⳮ . . . Ⳮ wd xd Ⳮ wdⳭ1
(i.e., a McCulloch-Pitts neuron!) in a two-category pattern classifier. The equation f (x) ⳱ 0 gives a hyperplane as the decision
surface, and training involves adjusting the coefficients (w1, w2,
. . . , wd, wdⳭ1) so that the decision surface produces an acceptable
separation of the two classes. We say that two categories are linearly separable if an acceptable setting of such linear weights exists. Thus, pattern recognition poses (at least) the following challenges to neural networks:
(a) Find a “good” set of preprocessors. Competitive learning
based on Hebbian plasticity (see COMPETITIVE LEARNING, as well
as the following text) provides one way of finding such features by
extracting statistically significant patterns from a set of input patterns. For example, if such a network were exposed to many, but
only, letters of the Roman alphabet, then it would find that certain
line segments and loops occurred repeatedly, even if there were no
teacher to tell it how to classify the patterns.
(b) Given a set of preprocessors and a set of patterns which have
already been classified, adjust the connections of a neural network
so that it acts as an effective pattern recognizer. That is, its response
to a preprocessed pattern should usually agree well with the classification provided by a teacher.
(c) Of course, if the neural network has multiple layers with
adaptable synaptic weights, then the early layers can be thought of
as preprocessors for the later layers, and we have a case of supervised, rather than Hebbian, formation of these “feature detectors”—
emphasizing features which are not only statistically significant
elements of the input patterns but which also serve to distinguish
usefully to which class a pattern belongs.

Associative Memory
In pattern recognition, we associate a pattern with a “label” or
“category.” Alternatively, an associative memory takes some “key”
as input and returns some “associated recollection” as output (see
ASSOCIATIVE NETWORKS). For example, given the sound of a word,

19

we may wish to recall its spelling. Given a misspelled word, we
may wish to recall the correctly spelled word of which it is most
plausibly a “degraded image.” There are two major approaches to
the use of neural networks as associative memories:
In nonrecurrent neural networks, there are no loops (i.e., we
cannot start at any neuron and “follow the arrows” to get back to
that neuron). We use such a network by fixing the pattern of inputs
as the key, and holding them steady. Since the absence of loops
ensures that the input pattern uniquely determines the output pattern (after the new inputs have time to propagate their effects
through the network), this uniquely determined output pattern is
the recollection associated with the key.
In recurrent networks, the presence of loops implies that the
input alone may not determine the output of the net, since this will
also depend on the initial state of the network. Thus, recurrent
networks are often used as associative memories in the following
way. The inputs are only used transiently to establish the initial
state of the neural network. After that, the network operates autonomously (i.e., uninfluenced by any inputs). If and when it reaches
an equilibrium state, that state is read out as the recollection associated with the key.
In either case, the problem is to set the weights of the neural
network so that it associates keys as accurately as possible with the
appropriate recollections.

Learning Rules
Most learning rules in current models of “lumped neurons” (i.e.,
those that exclude detailed analysis of the fine structure of the neuron or the neurochemistry of neural plasticity) take the form of
schemes for adjusting the synaptic weights, the “ws.” The two classic learning schemes for McCulloch-Pitts-type formal neurons are
due to Hebb (see HEBBIAN SYNAPTIC PLASTICITY) and Rosenblatt
(the perceptron, see PERCEPTRONS, ADALINES, AND BACKPROPAGATION), and we now introduce these in turn.

Hebbian Plasticity and Network Self-Organization
In Hebb’s (1949) learning scheme (see HEBBIAN SYNAPTIC PLASTICITY), the connection between two neurons is strengthened if both
neurons fire at the same time. The simplest example of such a rule
is to increase wij by the following amount:
Dwij ⳱ kyi xj
where synapse wij connects a presynaptic neuron with firing rate xj
to a postsynaptic neuron with firing rate yi. The trouble with the
original Hebb model is that every synapse will eventually get
stronger and stronger until they all saturate, thus destroying any
selectivity of association. Von der Malsburg’s (1973) solution was
to normalize the synapses impinging on a given neuron. To accomplish this, one must first compute the Hebbian “update” Dwij ⳱
kxi yj and then divide this by the total putative synaptic weights to
get the final result which replaces wi by
wij Ⳮ Dwij

兺k (wkj Ⳮ Dwkj)
where the summation k extends over all inputs to the neuron. This
new rule not only increases the strengths of those synapses with
inputs strongly correlated with the cell’s activity, but also decreases
the synaptic strengths of other connections in which such correlations did not arise.
Von der Malsburg was motivated by the pattern recognition
problem and was concerned with how individual cells in his network might come to be tuned so as to respond to one particular

20

Part I: Background

input “feature” rather than another (see OCULAR DOMINANCE AND
ORIENTATION COLUMNS for background as well as a review of
more recent approaches). This exposed another problem with
Hebb’s rule: a lot of nearby cells may, just by chance, all have
initial random connectivity which makes them easily persuadable
by the same stimulus; alternatively, the same pattern might occur
many times before a new pattern is experienced by the network. In
either case, many cells would become tuned to the same feature,
with not enough cells left to learn important and distinctive features. To solve this, von der Malsburg introduced lateral inhibition
into his model. In this connectivity pattern, activity in any one cell
is distributed laterally to reduce (partially inhibit) the activity of
nearby cells. This ensures that if one cell—call it A—were especially active, its connections to nearby cells would make them less
active, and so make them less likely to learn, by Hebbian synaptic
adjustment, those features that most excite A.
In summary, then, when the Hebbian rule is augmented by a
normalization rule, it tends to “sharpen” a neuron’s predisposition
“without a teacher,” getting its firing to become better and better
correlated with a cluster of stimulus patterns. This performance is
improved when there is some competition between neurons so that
if one neuron becomes adept at responding to a pattern, it inhibits
other neurons from doing so (COMPETITIVE LEARNING). Thus, the
final set of input weights to the neuron depends both on the initial
setting of the weights and on the pattern of clustering of the set of
stimuli to which it is exposed (see DATA CLUSTERING AND LEARNING). Other “post-Hebbian” rules, motivated both by technological
efficiency and by recent biological findings, are discussed in several
articles in Part III, including HEBBIAN LEARNING AND NEURONAL
REGULATION and POST-HEBBIAN LEARNING ALGORITHMS.
In the adaptive architecture just described, the inputs are initially
randomly connected to the cells of the processing layer. As a result,
none of these cells is particularly good at pattern recognition. However, by sheer statistical fluctuation of the synaptic connections,
one will be slightly better at responding to a particular pattern than
others are; it will thus slightly strengthen those synapses which
allow it to fire for that pattern and, through lateral inhibition, this
will make it harder for cells initially less well tuned for that pattern
to become tuned to it. Thus, without any teacher, this network
automatically organizes itself so that each cell becomes tuned for
an important cluster of information in the sensory inflow. This is
a basic example of the kind of phenomenon treated in SELFORGANIZATION AND THE BRAIN.

Perceptrons
Perceptrons are neural nets that change with “experience,” using
an error-correction rule designed to change the weights of each
response unit when it makes erroneous responses to stimuli that
are presented to the network. We refer to the judge of what is
correct as the “teacher,” although this may be another neural network, or some environmental input, rather than a signal supplied
by a human teacher in the usual schoolroom sense. Consider the
case in which a set R of input lines feeds a Pitts-McCulloch neural
network whose neurons are called associator units and which in
turn provide the input to a single McCulloch-Pitts neuron (called
the output unit of the perceptron) with adjustable weights (w1, . . . ,
wd) and threshold h. (In the case of visual pattern recognition, we
think of R as a rectangular “retina” onto which patterns may be
projected.) A simple perceptron is one in which the associator units
are not interconnected, which means that it has no short-term memory. (If such connections are present, the perceptron is called crosscoupled. A cross-coupled perceptron may have multiple layers and
loops back from an “earlier” to a “later” layer.) If the associator
units feed the pattern x ⳱ (x1, . . . , xd) to the output unit, then the
response of that unit will be to provide the pattern discrimination

with discriminant function f (x) ⳱ w1 x1 Ⳮ . . . Ⳮ wd xd ⳮ h. In
other words, the simple perceptron can only compute a linearly
separable function of the pattern as provided by the associator
units. The question asked by Rosenblatt (1958) and answered by
many others since (cf. Nilsson, 1965) was, “Given a simple perceptron (i.e., only the synaptic weights of the output unit are adjustable), can we ‘train’ it to recognize a given linearly separable
set of patterns by adjusting the ‘weights’ on various interconnections on the basis of feedback on whether or not the network classifies a pattern correctly?” The answer was “Yes: if the patterns are
linearly separable, then there is a learning scheme which will eventually yield a satisfactory setting of the weights.” The best-known
perceptron learning rule strengthens an active synapse if the efferent neuron fails to fire when it should have fired, and weakens an
active synapse if the neuron fires when it should not have done so:
Dwij ⳱ k(Yi ⳮ yi)xj
As before, synapse wij connects a presynaptic neuron with firing
rate xj to a postsynaptic neuron with firing rate yi, but now Yi is the
“correct” output supplied by the “teacher.” (This is similar to the
Widrow-Hoff [1960] least mean squares model of adaptive control;
see PERCEPTRONS, ADALINES, AND BACKPROPAGATION.) Notice
that the rule does change the response to xj “in the right direction.”
If the output is correct, Yi ⳱ yi and there is no change, Dwj ⳱ 0.
If the output is too small, then Yi ⳮ yi  0, and the change in wj
will add Dwj xj ⳱ k(Yi ⳮ yi)xjxj  0 to the output unit’s response
to (x1, . . . , xd). Similarly, if the output is too large, then Yi ⳮ yi
 0, Dwj will add k(Yi ⳮ yi)xj xj  0 to the output unit’s response.
Thus, there is a sense in which the new setting w⬘ ⳱ w Ⳮ Dw
classifies the input pattern x “more nearly correctly” than w does.
Unfortunately, in classifying x “more correctly” we run the risk of
classifying another pattern “less correctly.” However, the perceptron convergence theorem (see Arbib, 1987, pp. 66–69, for a proof)
shows that Rosenblatt’s procedure does not yield an endless seesaw, but will eventually converge to a correct set of weights if one
exists, albeit perhaps after many iterations through the set of trial
patterns.

Network Complexity
The perceptron convergence theorem states that, if a linear separation exists, the perceptron error-correction scheme will find it.
Minsky and Papert (1969) revivified the study of perceptrons (although some AI workers thought they had killed it!) by responding
to such results with questions like, “Your scheme works when a
weighting scheme exists, but when does there exist such a setting
of the weights?” More generally, “Given a pattern-recognition
problem, how much of the retina must each associator unit ‘see’ if
the network is to do its job?” Minsky and Papert studied when it
was possible for a McCulloch-Pitts neuron (no matter how trained)
to combine information in a single preprocessing layer to perform
a given pattern recognition task, such as recognizing whether a
pattern X of 1s on the retina (the other retinal units having output
0) is connected, that is, whether a path can be drawn from any 1
of X to another without going through any 0s. Another question
was to determine whether X is of odd parity, i.e., whether X contains an odd number of 1s. The question is, “How many inputs are
required for the preprocessing units of a simple perceptron to successfully implement f ?” We can get away with using a single element, computing an arbitrary Boolean function, and connecting it
to all the units of the retina. So the question that really interests us
is whether we can get away with a response unit connected to
proprocessors, each of which receives inputs from a limited set of
retinal units to make a global decision by synthesizing an array of
local views.
We convey the flavor of Minsky and Papert’s approach by the
example of XOR, the simple Boolean operation of addition modulo

I.3. Dynamics and Adaptation in Neural Networks
2, also known as the exclusive-or. If we imagine the square with
vertices (0, 0), (0, 1), (1, 1), and (1, 0) in the Cartesian plane, with
(x1, x2) being labeled by x1 丣 x2, we have 0s at one diagonally
opposite pair of vertices and 1s at the other diagonally opposite
pair of vertices. It is clear that there is no way of interposing a
straight line such that the 1s lie on one side and the 0s lie on the
other side. However, we shall prove it mathematically to gain insight into the techniques used by Minsky and Papert.
Consider the claim that we wish to prove wrong: that there actually exists a neuron with threshold h and weights ␣ and b such
that x1 丣 x2 ⳱ 1 if and only if ␣x1 Ⳮ bx2 ⱖ h. The crucial point
is to note that the function of addition modulo 2 is symmetric;
therefore, we must also have x1 丣 x2 ⳱ 1 if and only if bx1 Ⳮ ␣x2
ⱖ h, and, so, adding together the two terms, we have x1 丣 x2 ⳱ 1
if and only if (1⁄2)(␣ Ⳮ b)(x1 Ⳮ x2) ⱖ h. Writing (1⁄2)(␣ Ⳮ b) as
c, we see that we have reduced three putative parameters ␣, b, and
h to just two, namely c and h.
We now set t ⳱ x1 Ⳮ x2 and look at the polynomial ct ⳮ h. It
is a degree 1 polynomial, but note: at t ⳱ 0, ct ⳮ h must be less
than zero (0 丣 0 ⳱ 0); at t ⳱ 1, it is greater than or equal to zero
(0 丣 1 ⳱ 1 丣 0 ⳱ 1); and at t ⳱ 2, it is again less than zero (1
丣 1 ⳱ 0). This is a contradiction—a polynomial of degree 1 cannot
change sign from positive to negative more than once. We conclude
that there is no such polynomial, and thus that there is no threshold
element which will add modulo 2.
We now understand a general method used again and again by
Minsky and Papert: start with a pattern-classification problem. Observe that certain symmetries leave it invariant. For instance, for
the parity problem (is the number of active elements even or odd?),
which includes the case of addition modulo 2 when the retina has
only two units, any permutation of the points of the retina would
leave the classification unchanged. Use this to reduce the number
of parameters describing the circuit. Then lump items together to
get a polynomial and examine actual patterns to put a lower bound
on the degree of the polynomial, fixing things so that this degree
bounds the number of inputs to the response unit of a simple
perceptron.
Minsky and Papert provide many interesting theorems (for the
proof of an illustrative sample, see Arbib, 1987, pp. 82–84). As
just one example, we may note that they prove that the parity function requires preprocessors big enough to scan the whole retina if
the preprocessors can only be followed by a single McCullochPitts neuron. By contrast, to tell whether the number of active retinal inputs reaches a certain threshold only requires two inputs per
neuron in the first layer. (For other complexity results, see the articles listed in the road map Computability and Complexity.)

Gradient Descent and Credit Assignment
The implication of the results on “network complexity” is clear: if
we limit the complexity of the units in a neural network, then in
general we will need many layers, rather than a single layer, if the
network is to have any chance of being trained to realize many
“interesting” functions. This conclusion motivates the study of
training rules for multilayer perceptrons, of which the most widely
used is backpropagation. Before describing this method, we first
discuss two general notions of which it is an important exemplar:
gradient descent and credit assignment.
In discussing Hopfield networks, we introduced the metaphor of
an “energy landscape” (see Figure 6). The asynchronous updates
move the state of the network (the vector of neural activity levels)
“downhill,” tending toward a local energy minimum. Our task now
is to realize that the metaphor works again on a far more abstract
level when we consider learning. In learning, the dynamic variable
is not the network state, but rather the vector of synaptic weights
(or whatever other set of network parameters is adjusted by the

21

learning rules). We now conduct gradient descent in weight space.
At each step, the weights are adjusted in such a way as to improve
the performance of the network. (As in the case of the simple perceptron, the improvement is a “local” one based on the current
situation. It is, in this case, a matter for computer simulation to
prove that the cumulative effect of these small changes is a network
which solves the overall problem.)
But how do we recognize which “direction” in weight space is
“downhill”? Suppose success is achieved by a complex mechanism
after operating over a considerable period of time (for example,
when a chess-playing program wins a game). To what particular
decisions made by what particular components should the success
be attributed? And, if failure results, what decisions deserve blame?
This is closely related to the problem known as the “mesa” or
“plateau” problem (Minsky, 1961). The performance evaluation
function available to a learning system may consist of large level
regions in which gradient descent degenerates to exhaustive search,
so that only a few of the situations obtainable by the learning system and its environment are known to be desirable, and these situations may occur rarely.
One aspect of this problem, then, is the temporal credit assignment problem. The utility of making a certain action may depend
on the sequence of actions of which it is a part, and an indication
of improved performance may not occur until the entire sequence
has been completed. This problem was attacked successfully in
Samuel’s (1959) learning program for playing checkers. The idea
is to interpret predictions of future reward as rewarding events
themselves. In other words, neutral stimulus events can themselves
become reinforcing if they regularly occur before events that are
intrinsically reinforcing. Such temporal difference learning (see
REINFORCEMENT LEARNING) is like a process of erosion: the original uninformative mesa, where only a few sink holes allow gradient
descent to a local minimum, is slowly replaced by broader valleys
in which gradient descent may successfully proceed from many
different places on the landscape.
Another aspect of credit assignment concerns structural factors.
In the simple perceptron, only the weights to the output units are
to be adjusted. This architecture can only support maps which are
linearly separable as based on the patterns presented by the preprocessors, and we have seen that many interesting problems require preprocessing units of undue complexity to achieve linear
separability. We thus need multiple layers of preprocessors, and,
since one may not know a priori the appropriate set of preprocessors for a given problem, these units should be trainable too. This
raises the question, “How does a neuron deeply embedded within
a network ‘know’ what aspect of the outcome of an overall action
was ‘its fault’?” This is the structural credit assignment problem.
In the next section, we shall study the most widely used solution
to this problem, called backpropagation, which propagates back to
a hidden unit some measure of its responsibility.
Backpropagation is an “adaptive architecture”: it is not just a
local rule for synaptic adjustment; it also takes into account the
position of a neuron in the network to indicate how the neuron’s
weights are to change. (In this sense, we may see the use of lateral
inhibition to improve Hebbian learning as the first example of an
adaptive architecture in these pages.) This adaptive architecture is
an example of “neurally inspired” modeling, not modeling of actual
brain structures; and there is no evidence that backpropagation represents actual brain mechanisms.

Backpropagation
The task of backpropagation is to train a multilayer (feedforward)
perceptron (or MLP), a loop-free network which has its units arranged in layers, with a unit providing input only to units in the
next layer of the sequence. The first layer comprises fixed input

22

Part I: Background

units; there may then be several layers of trainable “hidden units”
carrying an internal representation, and finally, there is the layer of
output units, also trainable. (A simple perceptron then corresponds
to the case in which we view the input units as fixed associator
units, i.e., they deliver a preprocessed, rather than a “raw,” pattern
which connect directly to the output units without any hidden units
in between.) For what follows, it is crucial that each unit not be
binary: it has both input and output taking continuous values in
some range, say [0, 1]. The response is a sigmoidal function of the
weighted sum. Thus, if a unit has inputs xk with corresponding
weights wik, the output xi is given by xi ⳱ fi(Rwikxk), where fi is a
sigmoidal function, say
1
1 Ⳮ eⳮ(x

fi(x) ⳱

Ⳮ hi)

with hi being a bias or threshold for the unit.
The environment only evaluates the output units. We are given
a training set of input patterns p and corresponding desired target
patterns t p for the output units. With o p the actual output pattern
elicited by input p, the aim is to adjust the weights in the network
to minimize the error
E⳱

兺

兺

(t pk ⳮ o pk )2

patterns p output neurons k

Rumelhart, Hinton, and Williams (1986) were among those who
devised a formula for propagating back the gradient of this evaluation from a unit to its inputs. This process can continue by backpropagation through the entire net. The scheme seems to avoid
many false minima. At each trial, we fix the input pattern p and
consider the corresponding “restricted error”

兺k (tk ⳮ ok)2

E⳱

where k ranges over designated “output units.” The net has many
units interconnected by weights wij. The learning rule is to change
wij so as to reduce E by gradient descent:
E
Dwij ⳱ ⳮ
⳱ 2
wij

o

兺k (tk ⳮ ok) wkij

Consider a net divided into m Ⳮ 1 layers, with nets in layer g
Ⳮ 1 receiving all their inputs from layer g; with layer 0 comprising
the input units; and layer m comprising the output units. If i is an
output unit (remember, wij connects from j to i) then the only nonzero term in the last equation has k ⳱ i. Now ok ⳱ Rwilol where
wil ⬆ 0 only for o1 which are outputs from the previous layer. We
thus have

冢兺 w o 冣

fi
Dwij ⳱ 2(ti ⳮ oi)

i1 m1

⳱ 2(ti ⳮ oi) f ⬘o
i j

wij

where f ⬘i is the derivative of the activation function evaluated at
the activation level ini ⳱ Rwilol to unit i. Thus Dwij for an output
unit i is proportional to dioj, where di ⳱ (ti ⳮ oi) f ⬘.
i
Next, suppose that i is a hidden unit whose output drives only
output units:

冢兺 w o 冣

fk
Dwij ⳱ 2

兺k

(tk ⳮ ok)

kl l

wij

However, the only ol that depends on wij is oi, and so

冢兺 w o 冣

fk

wij

冢兺 w o 冣 o

fk

kl l

⳱

kl l

i

oi

wij

⳱ [ f k⬘wki] • [ f i⬘oj]

so that Dwij ⳱ 2k(tk ⳮ ok)[ f k⬘wki] • [ f ⬘o
i j].

Recalling that dk ⳱ (tk ⳮ ok) f k⬘ for an output unit k, we may
rewrite this as

冢兺 d w 冣 f ⬘o

Dwij ⳱ 2

k

ki

i j

k

Thus, Dwij is proportional to dioj, with di ⳱ (Rkdkwki) f ⬘,
i where
k runs over all units which receive unit i’s output. More generally,
we can prove the following, by induction on how many layers back
we must go to reach a unit:
Proposition. Consider a layered loop-free net with error E ⳱
k(tk ⳮ ok)2, where k ranges over designated “output units,” and
let the weights wij be changed according to the gradient descent
rule
Dwij ⳱ ⳮE/wij ⳱ 2

o

兺k (tk ⳮ ok) wkij

Then the weights may be changed inductively, working back
from the output units, by the rule
Dwij is proportional to dioj
where:
Basis Step: di ⳱ (ti ⳮ oi) f ⬘i for an output unit.
Induction Step: If i is a hidden unit, and if dk is known for all
units that receive unit i’s output, then di ⳱ (kdkwki) f ⬘,
i where k
runs over all units which receive unit i’s output.
Thus the “error signal” di propagates back layer by layer from
the output units. In Rkdkwki, unit i receives error propagated back
from a unit k to the extent to which i affects k. For output units,
this is essentially the delta rule given by Widrow and Hoff (1960)
(see PERCEPTRONS, ADALINES, AND BACKPROPAGATION).
The theorem just presented tells us how to compute Dwij for
gradient descent. It does not guarantee that the above step-size is
appropriate to reach the minimum, nor does it guarantee that the
minimum, if reached, is global. The backpropagation rule defined
by this proposition is, thus, a heuristic rule, not one guaranteed to
find a global minimum, but is still perhaps the most diversely used
adaptive architecture. Many other approaches to learning, including
some which are “neural-like” in at best a statistical sense, rather
than being embedded in adaptive neural networks, may be found
in the road map Learning in Artificial Networks (not just neural
networks).

A Cautionary Note
The previous subsections have introduced a number of techniques
that can be used to make neural networks more adaptive. In a typical training scenario, we are given a network N which, in response
to the presentation of any x from some set of input patterns, will
eventually settle down to produce a corresponding y from the set
Y of the network’s output patterns. A training set is then a sequence
of pairs (xk, yk) from X ⳯ Y, 1 ⱕ k ⱕ n. The foregoing results say
that, in many cases (and the bounds are not yet well defined), if we
train the net with repeated presentations of the various (xk, yk), it
will converge to a set of connections which cause N to compute a
function f : X r Y with the property that, over the set of k’s from
1 to n, the f (xk) “correlate fairly well” with the yk. Of course, there
are many other functions g: X r Y such that the g(xk) “correlate
fairly well” with the yk, and they may differ wildly on those “tests”
x in X that do not equal an xk in the training set. The view that one
may simply present a trainable net with a few examples of solved
problems, and it will then adjust its connections to be able to solve
all problems of a given class, glosses over three main issues:
(a) Complexity: Is the network complex enough to encode a solution method?

I.3. Dynamics and Adaptation in Neural Networks
(b) Practicality: Can the net achieve such a solution within a feasible period of time? and
(c) Efficacy: How do we guarantee that the generalization achieved
by the machine matches our conception of a useful solution?
Part III provides many “snapshots” of the research underway to
develop answers to these problems (for the “state of play” see, for
example, LEARNING AND GENERALIZATION: THEORETICAL
BOUNDS; PAC LEARNING AND NEURAL NETWORKS; and VAPNIKCHERVONENKIS DIMENSION OF NEURAL NETWORKS). Nonetheless,
it is clear that these training techniques will work best when training is based on an adaptive architecture and an initial set of weights
appropriate to the given problem. Future work on the neurally inspired design of intelligent systems will involve many domainspecific techniques for system design, such as those exemplified in
the road maps Vision and Robotics and Control Theory, as well
as general advances in adaptive architectures.

Envoi
With this, our tour of some of those basic landmarks of Brain
Theory and Neural Networks established by 1986 is complete. I
now invite each reader to follow the suggestions of the section
“How to Use this Book” of the Handbook to begin exploring the
riches of Part III, possibly with the guidance of a number of the
road maps in Part II.
Acknowledgments. All of Part I is a lightly edited version of
Part I as it appeared in the first edition of the Handbook. Section
I.1 is based in large part on material contained in Section 2.3 of
Arbib (1989), while Section I.3 is based on Sections 3.4 and 8.2.

References
Amari, S., and Arbib, M. A., 1977, Competition and cooperation in neural
nets, in Systems Neuroscience (J. Metzler, Ed.), New York: Academic
Press, pp. 119–165.
Arbib, M. A., 1981, Perceptual structures and distributed motor control, in
Handbook of Physiology—The Nervous System, vol. II, Motor Control
(V. B. Brooks, Ed.), Bethesda, MD: American Physiological Society,
pp. 1449–1480.
Arbib, M. A., 1987, Brains, Machines, and Mathematics, 2nd ed., New
York: Springer-Verlag.
Arbib, M. A., 1989, The Metaphorical Brain 2: Neural Networks and Beyond, New York: Wiley-Interscience.
Arbib, M. A., Érdi, P., and Szentágothai, J., 1998, Neural Organization:
Structure, Function, and Dynamics, Cambridge, MA: MIT Press.
Arbib, M. A., and Hesse, M. B., 1986, The Construction of Reality, New
York: Cambridge University Press.
Bain, A., 1868, The Senses and the Intellect, 3rd ed.
Bernard, C., 1878, Leçons sur les phénomènes de la Vie.
Brooks, R. A., 1986, A robust layered control system for a mobile robot,
IEEE Robot. Automat., RA-2:14–23.
Cannon, W. B., 1939, The Wisdom of the Body, New York: Norton.
Chomsky, N., 1959, On certain formal properties of grammars, Inform.
Control, 2:137–167.
Church, A., 1941, The Calculi of Lambda-Conversion, Annals of Mathematics Studies 6, Princeton, NJ: Princeton University Press.
Craik, K. J. W., 1943, The Nature of Explanation, New York: Cambridge
University Press.
Ewert, J.-P., and von Seelen, W., 1974, Neurobiologie and System-Theorie
eines visuellen Muster-Erkennungsmechanismus bei Kroten, Kybernetik,
14:167–183.
Fearing, F., 1930, Reflex Action, Baltimore: Williams and Wilkins.

23

Gödel, K., 1931, Uber formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme: I, Monats. Math. Phys., 38:173–198.
Grossberg, S., 1967, Nonlinear difference-differential equations in prediction and learning theory, Proc. Natl. Acad. Sci. USA, 58:1329–1334.
Hebb, D. O., 1949, The Organization of Behavior, New York: Wiley.
Heims, S. J., 1991, The Cybernetics Group, Cambridge, MA: MIT Press.
Hodgkin, A. L., and Huxley, A. F., 1952, A quantitative description of
membrane current and its application to conduction and excitation in
nerve, J. Physiol. Lond., 117:500–544.
Hopfield, J., 1982, Neural networks and physical systems with emergent
collective computational properties, Proc. Natl. Acad. Sci. USA,
79:2554–2558.
Hopfield, J. J., and Tank, D. W., 1986, Neural computation of decisions in
optimization problems, Biol. Cybern., 52:141–152.
Kleene, S. C., 1936, General recursive functions of natural numbers, Math.
Ann., 112:727–742.
La Mettrie, J., 1953, Man a Machine (trans. by G. Bussey from the French
original of 1748), La Salle, IL: Open Court.
Lichtheim, L., 1885, On aphasia, Brain, 7:433–484.
Maxwell, J. C., 1868, On governors, Proc. R. Soc. Lond., 16:270–283.
McCulloch, W. S., and Pitts, W. H., 1943, A logical calculus of the ideas
immanent in nervous activity, Bull. Math. Biophys., 5:115–133.
Minsky, M. L., 1961, Steps toward artificial intelligence, Proc. IRE, 49:8–
30.
Minsky, M. L., 1985, The Society of Mind, New York: Simon and Schuster.
Minsky, M. L., and Papert, S., 1969, Perceptrons: An Essay in Computational Geometry, Cambridge, MA: MIT Press.
Nilsson, N., 1965, Learning Machines, New York: McGraw–Hill.
Pavlov, I. P., 1927, Conditioned Reflexes: An Investigation of the Physiological Activity of the Cerebral Cortex (translated from the Russian by
G. V. Anrep), New York: Oxford University Press.
Post, E. L., 1943, Formal reductions of the general combinatorial decision
problem, Am. J. Math., 65:197–268.
Rall, W., 1964, Theoretical significance of dendritic trees for neuronal input–output relations, in Neural Theory and Modeling (R. Reiss, Ed.),
Stanford, CA: Stanford University Press, pp. 73–97.
Ramón y Cajal, S., 1906, The structure and connexion of neurons, reprinted
in Nobel Lectures: Physiology or Medicine, 1901–1921, New York: Elsevier, 1967, pp. 220–253.
Rosenblatt, F., 1958, The perceptron: A probabilistic model for information
storage and organization in the brain, Psychol. Rev., 65:386–408.
Rosenblueth, A., Wiener, N., and Bigelow, J., 1943, Behavior, purpose and
teleology, Philos. Sci., 10:18–24.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J., 1986, Learning internal representations by error propagation, in Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. 1 (D. Rumelhart and J. McClelland, Eds.), Cambridge, MA: MIT Press/Bradford
Books, pp. 318–362.
Samuel, A. L., 1959, Some studies in machine learning using the game of
checkers, IBM J. Res. Dev., 3:210–229.
Selfridge, O. G., 1959, Pandemonium: A paradigm for learning, in Mechanisation of Thought Processes, London: Her Majesty’s Stationery
Office, pp. 511–531.
Sherrington, C., 1906, The Integrative Action of the Nervous System, New
York: Oxford University Press.
Turing, A. M., 1936, On computable numbers with an application to the
Entscheidungsproblem, Proc. Lond. Math. Soc. (Series 2), 42:230–265.
Turing, A. M., 1950, Computing machinery and intelligence, Mind,
59:433–460.
von der Malsburg, C., 1973, Self-organization of orientation-sensitive cells
in the striate cortex, Kybernetik, 14:85–100.
Widrow, B., and Hoff, M. E., Jr., 1960, Adaptive switching circuits, in
1960 IRE WESCON Convention Record, vol. 4, pp. 96–104.
Wiener, N., 1948, Cybernetics: Or Control and Communication in the Animal and the Machine, New York: Technology Press and Wiley (2nd ed.,
Cambridge, MA: MIT Press, 1961).
Young, R. M., 1970, Mind, Brain and Adaptation in the Nineteenth Century: Cerebral Localization and Its Biological Context from Gall to Ferrier, New York: Oxford University Press.

Part II: Road Maps
A Guided Tour of Brain Theory
and Neural Networks
Michael A. Arbib

II.1. The Meta-Map

27

How to Use Part II
Part II provides a guided tour of our subject in the form of 22 road
maps, each of which provides an overview of a single theme in
brain theory and neural networks and offers a précis of Part III
articles related to that theme. The road maps are grouped under
eight general headings:
Grounding Models of Neurons and Networks
Brain, Behavior, and Cognition
Psychology, Linguistics, and Artificial Intelligence
Biological Neurons and Networks
Dynamics and Learning in Artificial Networks
Sensory Systems
Motor Systems
Applications, Implementations, and Analysis
Part II starts with the meta-map (Section II.1), which is designed
to give some sense of the diversity yet interconnectedness of the
themes taken up in this Handbook by quickly surveying the 22
different road maps. We then offer eight sections, one for each of

the above headings, that comprise the 22 road maps. In the road
maps, we depart from the convention used elsewhere in this text
whereby titles in capitals and small capitals are used for crossreferences to all other articles. In the road maps, we reserve capitals
and SMALL CAPITALS for articles on the tour, and we use titles in
quotation marks to refer to related articles that are not primary to
the current road map. We will use boldface type to refer to road
maps and other major sections in Part II.
Every article in Part III occurs in at least one road map, and a
few articles appear in two or even three road maps. Clearly, certain
articles unequivocally have a place in a given road map, but as I
considered articles that were less central to a given theme, my
decisions on which articles to include became somewhat arbitrary.
Thus, I invite you to read each road map to get a good overview
of the main themes of each road map, and then continue your exploration by browsing Part III and using the articles listed under
Related Reading and the index of the Handbook to add your own
personal extensions to each map.

II.1. The Meta-Map
There is no one best path for the study of brain theory and neural
networks, and you should use the meta-map simply to get a broad
overview that will help you choose a path that is pleasing, or useful,
to you.

Grounding Models of Neurons and Networks
Grounding Models of Neurons
Grounding Models of Networks
The articles surveyed in these two road maps can be viewed as
continuing the work of Part I, providing the reader with a basic
understanding of the models of both biological and artificial neurons and neural networks that are developed in the 285 articles in
Part III. The road maps will help each reader decide which of these
articles provide necessary background for their own reading of the
Handbook.

Brain, Behavior, and Cognition
Neuroethology and Evolution
Mammalian Brain Regions
Cognitive Neuroscience
The road map Neuroethology and Evolution places the following
road map, Mammalian Brain Regions, in a dual perspective. First,
by reviewing work on modeling neural mechanisms of the behavior
of a variety of nonmammalian animals, it helps us understand the
wealth of subtle neural computations available in other species,
enriching our study of nervous systems that are closer to that of
humans. When we focus on ethology (animal behavior), we often
study the integration of perception and action, thus providing a
useful complement to the many articles that focus on a subsystem
in relative isolation. Second, by offering a number of articles on
both biological and artificial evolution, we take the first steps in
understanding the ways in which different neural architectures may
emerge across many generations. Turning to the mammalian brain,
we first look at Mammalian Brain Regions. We will also study

the role of these brain regions in other road maps as we analyze
such functions as vision, memory, and motor control. We shall see
that every such function involves the “cooperative computation” of
a multiplicity of brain regions. However, Mammalian Brain Regions reviews those articles that focus on a single brain region and
give some sense of how we model its contribution to key neural
functions. The road map Cognitive Neuroscience then pays special
attention to a range of human cognitive functions, including perception, action, memory, and language, with emphasis on the range
of data now available from imaging of the active human brain and
the challenges these data provide for modeling.

Psychology, Linguistics, and Artificial Intelligence
Psychology
Linguistics and Speech Processing
Artificial Intelligence
Our next three road maps–Psychology, Linguistics and Speech
Processing, and Artificial Intelligence—are focused more on the
effort to understand human psychology than on the need to understand the details of neurobiology. For example, the articles on Psychology may overlap those on Cognitive Neuroscience, but overall the emphasis shifts to “connectionist” models in which the
“neurons” rarely correspond to the actual biological neurons of the
human brain (the underlying structure). Rather, the driving idea is
that the functioning of the human mind (the functional expression
of the brain’s activity) is best explored through a parallel, adaptive
processing methodology in which large populations of elements
are simultaneously active, pass messages back and forth between
each other, and can change the strength of their connections as they
do so. This is in contrast to the serial computing methodology,
which is based on the computing paradigm that was dominant from
the 1940s through the 1970s and that now is complemented in
mainstream computer science by work in grid-based computing,
embedded systems, and teams of intelligent agents.
In short, connectionist approaches to psychology and linguistics
use “neurons” that are more like the artificial neurons used to build

28

Part II: Road Maps

new applications for parallel processing than they are like the real
neurons of the living brain.
In dividing this introduction to connectionism into three themes,
I have first distinguished those aspects of connectionist psychology
that relate to perception, memory, emotion, and other aspects of
cognition in general from those specifically involved in connectionist linguistics before turning to artificial intelligence. The road
map Psychology also contains articles that address philosophical
issues in brain theory and connectionism, including the notion of
consciousness, as well as articles that approach psychology from a
developmental perspective. The road map Linguistics and Speech
Processing presents connectionist models of human language performance as well as approaches (some more neural than others) to
technologies for speech processing. The central idea in connectionist linguistics is that rich linguistic representations can emerge
from the interaction of a relatively simple learning device and a
structured linguistic environment, rather than requiring the details
of grammar to be innate, captured in a genetically determined universal grammar. The road map Artificial Intelligence presents articles whose themes are similar to those in Psychology in what
they explain, but are part of artificial intelligence (AI) because the
attempt is to get a machine to exhibit some intelligent-like behavior, without necessarily meeting the constraints imposed by experimental psychology or psycholinguistics. “Classical” symbolic AI
is contrasted with a number of methods in addition to the primary
concentration on neural network approaches. The point is that,
whereas brain theory seeks to know “how the brain does it,” AI
must weigh the value of artificial neural networks (ANNs) as a
powerful technology for parallel, adaptive computation against that
of other technologies on the basis of efficacy in solving practical
problems on available hardware. The reader will, of course, find a
number of models that are of equal interest to psychologists and to
AI researchers.
The articles gathered in these three road maps will not exhaust
the scope of their subject matter, for at least two reasons. First, in
addition to connectionist models of psychological phenomena,
there are many biological models that embody genuine progress in
relating the phenomena to known parts of the brain, perhaps even
grounding a phenomenon in the behavior of identifiable classes of
biological neurons. Second, while Artificial Intelligence will focus
on broad thematic issues, a number of these also appear in applying
neural networks in computer vision, speech recognition, and elsewhere using techniques elaborated in articles of the road map
Learning in Artificial Networks.

Biological Neurons and Networks
Biological Neurons and Synapses
Neural Plasticity
Neural Coding
Biological Networks
The next four road maps, Biological Neurons and Synapses, Neural Plasticity, Neural Coding, and Biological Networks, are ones
that, for many readers, may provide the appropriate entry point for
the book as a whole, namely, an understanding of neural networks
from a biological point of view. The road map Biological Neurons
and Synapses gives us some sense of how sophisticated real biological neurons are, with each patch of membrane being itself a
subtle electrochemical structure. An appreciation of this complexity is necessary for the computational neuroscientist wishing to
address the increasingly detailed database of experimental neuroscience on how signals can be propagated, and how individual neurons interact with each other. But such complexity may also provide an eye opener for the technologist planning to incorporate new
capabilities into the next generation of ANNs. The road map Neural Plasticity then charts from a biological point of view a variety

of specific mechanisms at the level of synapses, or even finergrained molecular structures, which enable the changes in the
strength of connections that underlie both learning and development. A number of such mechanisms have already implied a variety
of learning rules for ANNs (see Learning in Artificial Networks),
but they also include mechanisms that have not seen technological
use. This road map includes articles that analyze mechanisms that
underlie both development and regeneration of neural networks and
learning in biological systems. However, I again stress to the reader
that one may approach the road maps, and the articles in Part III
of this Handbook, in many different orders, so that some readers
may prefer to study the articles described in the road map Learning
in Artificial Networks before or instead of studying those on neurobiological learning mechanisms.
Two more road maps round out our study of Biological Neurons
and Networks. The simplest models of neurons either operate on
a discrete-time scale or measure neural output by the continuous
variation in firing rate. The road map Neural Coding examines the
virtues of other alternatives, looking at both the possible gains in
information that may follow from exploiting the exact timing of
spikes (action potentials) as they travel along axonal branches from
one neuron to many others, and the way in which signals that may
be hard to discern from the firing of a single neuron may be reliably
encoded by the activity of a whole population of neurons. We then
turn to articles that chart a number of the basic architectures
whereby biological neurons are combined into Biological Networks—although clearly, this is a topic expanded upon in many
articles in Part III which are not explicitly presented in this road
map.

Dynamics and Learning in Artificial Networks
Dynamic Systems
Learning in Artificial Networks
Computability and Complexity
The next three road maps—Dynamic Systems, Learning in Artificial Networks, and Computability and Complexity—provide
a broad perspective on the dynamics of neural networks considered
as general information processing structures rather than as models
of a particular biological or psychological phenomenon or as solutions to specific technological problems. Our study of Dynamic
Systems is grounded in studying the dynamics of a neural network
with fixed inputs: does it settle down to an equilibrium state, and
to what extent can that state be seen as the solution of some problem
of optimization? Under what circumstances will the network exhibit a dynamic pattern of oscillatory behavior (a limit cycle), and
under what circumstances will it undergo chaotic behavior (traversing what is known as a strange attractor)? This theme is expanded by the study of cooperative phenomena. In a gas or a magnet, we do not know the behavior of any single atom with precision,
but we can infer the overall “cooperative” behavior—the pressure,
volume, and temperature of a gas, or the overall magnetization of
a magnet—through statistical methods, methods which even extend
to the analyses of such dramatic phase transitions as that of a piece
of iron from an unmagnetized lump to a magnet, or of a liquid to
a gas. So, too, can statistical methods provide insight into the largescale properties of neural nets, abstracting away from the detailed
function of individual neurons, when our interest is in statistical
patterns of behavior rather than the fine details of information processing. This leads us to the study of self-organization in neural
networks, in which we ask for ways in which the interaction between elements in a neural network can lead to the spontaneous
expression of pattern; whether this pattern is constituted by the
pattern of activity of the individual neurons or by the pattern of
synaptic connections which records earlier experience.

II.2. Grounding Models of Neurons and Networks
With this question of earlier experience, we have fully made the
transition to the study of learning, and we turn to the road map
which focuses on Learning in Artificial Networks, complementing the road map Neural Plasticity. (This replaces two road maps
from the first edition–Learning in Artificial Neural Networks, Deterministic, and Learning in Artificial Neural Networks, Statistical—for two reasons: (1) the use of statistical methods in the study
of learning in ANNs is so pervasive that the attempt to distinguish
deterministic and statistical approaches to learning is not useful,
and (2) the statistical analysis of learning in ANNs has spawned a
variety of statistical methods that are less closely linked to neurobiological inspiration, and we wish these, too, to be included in our
road map.) The study of Computability and Complexity then provides a rapprochement between neural networks and a number of
ideas developed within the mainstream of computer science, especially those arising from the study of complexity of computational structures. Indeed, it takes us back to the very foundations
of the theory of neural networks, in which the study of McCullochPitts neurons built on earlier work on computability to inspire the
later development of automata theory.

Sensory Systems
Vision
Other Sensory Systems
Vision has been the most widely studied of all sensory systems,
both in brain theory and in applications and analysis of ANNs, and
thus has a special road map of its own. Other Sensory Systems,
treated at less length in the next road map, include audition, touch,
and pain, as well a number of fascinating special systems such as
electrolocation in electric fish and echolocation in bats.

Motor Systems
Robotics and Control Theory
Motor Pattern Generators
Mammalian Motor Control
The next set of road maps—Robotics and Control Theory, Motor
Pattern Generators, and Mammalian Motor Control—addresses the control of movement by neural networks. In the study
of Robotics and Control Theory, the adaptive properties of neural
networks play a special role, enabling a control system, through
experience, to become better and better suited to solve a given
repertoire of control problems, guiding a system through a desired
trajectory, whether through the use of feedback or feedforward.
These general control strategies are exemplified in a number of
different approaches to robot control. The articles in the road map
Motor Pattern Generators focus on subconscious functions, such
as breathing or locomotion, in vertebrates and on a wide variety of

29

pattern-generating activity in invertebrates. The reader may wish
to turn back to the road map Neuroethology and Evolution for
other studies in animal behavior (neuroethology) which show how
sensory input, especially visual input, and motor behavior are integrated in a cycle of action and perception. Mammalian Motor
Control places increased emphasis on the interaction between neural control and the kinematics or dynamics of limbs and eyes, and
also looks at various forms of motor-related learning. In showing
how the goals of movement can be achieved by a neural network
through the time course of activity of motors or muscles, this road
map overlaps some of the issues taken up in the more applicationsoriented road map, Robotics and Control Theory. Much of the
material on biological motor control is of general relevance, but
the road map also includes articles on primate motor control that
examine a variety of movements of the eyes, head, arm, and hand
which are studied in a variety of mammals but are most fully expressed in primates and humans. Of course, as many readers will
be prepared to notice by now, Mammalian Motor Control will,
for some readers, be an excellent starting place for their study,
since, by showing how visual and motor systems are integrated in
a number of primate and human behaviors, it motivates the study
of the specific neural network mechanisms required to achieve
these behaviors.

Applications, Implementations, and Analysis
Applications
Implementation and Analysis
We then turn to a small set of Applications of neural networks,
which include signal processing, speech recognition, and visual
processing (but exclude the broader set of applications to astronomy, speech recognition, high-energy physics, steel making, telecommunications, etc., of the first edition, since The Handbook of
Neural Computation [Oxford University Press, 1996] now provides
a large set of articles on ANN applications). Since a neural network
cannot be applied unless it is implemented, whether in software or
hardware, we close with the road map Implementation and Analysis. The implementation methodologies include simulation on a
general-purpose computer, emulation on specially designed neurocomputers, and implementation in a device built with electronic
or photonic materials. As for analysis, we present articles in the
nascent field of neuroinformatics which combines database methodology, visualization, modeling, and data analysis in an attempt
to master the explosive growth of neuroscience data. (In Europe,
the term neuroinformatics is used to encompass the full range of
computational approaches to brain theory and neural networks. In
the United States, some people use neuroinformatics to refer solely
to the use of databases in neuroscience. Here we focus on the middle ground, where the analysis of data and the construction of models are brought together.)

II.2. Grounding Models of Neurons and Networks
The first two road maps expand the exposition of Part I by presenting basic models of neurons and networks that provide the
building blocks for many of the articles in Part III.

Grounding Models of Neurons

HEBBIAN SYNAPTIC PLASTICITY
PERCEPTRONS, ADALINES, AND BACKPROPAGATION
PERSPECTIVE ON NEURON MODEL COMPLEXITY
REINFORCEMENT LEARNING
SINGLE-CELL MODELS
SPIKING NEURONS, COMPUTATION WITH

AXONAL MODELING
DENDRITIC PROCESSING

This road map introduces classes of neuron models of increasing
complexity and attention to detail. The point is that much can be

30

Part II: Road Maps

learned even at high degrees of abstraction, while other phenomena
can be understood only by attention to subtle details of neuronal
function. The reader of this Handbook will find many articles exploring biological phenomena and technological applications at different levels of complexity. The implicit questions will always be,
“Do all the details matter?” and “Is the model oversimplified?” The
answers will depend both on the phenomena under question and
on the current subtlety of experimental investigations. After introducing articles that present neuron models across the range of
model complexity, the road map concludes with a brief look at the
most widely analyzed forms of synaptic plasticity.
Classes of neuron models can be defined by how they treat the
train of action potentials issued by a neuron (see the road map
Neural Coding). Many models assume that information is carried
in the average rate of pulses over a time much longer than a typical
pulse width, with the occurrence times of particular pulses simply
treated as jitter on an averaged analog signal. A neural model in
such a theory might be a mathematical function which produces a
real-valued output from its many real-valued inputs; that function
could be linear or nonlinear, static or adaptive, and might be instantiated in analog silicon circuits or in digital software. Examples
given of such models in SINGLE-CELL MODELS are the McCullochPitts model, the perceptron model, Hopfield neurons, and polynomial neurons. However, some models assume that each single neural pulse carries reliable, precisely timed information. A neural
model in such a theory fires only upon the exact coincidence of
several input pulses, and quickly “forgets” when it last fired, so
that it is always ready to fire upon another coincidence. The simplest such models are the integrate-and-fire models. The article concludes by briefly introducing the Hodgkin-Huxley model of squid
axon, based on painstaking analysis (without benefit of electronic
computers) of data from the squid giant axon, and then introduces
modified single-point models, compartmental models, and computation with both passive dendrites and active dendrites. SPIKING
NEURONS, COMPUTATION WITH provides more detail on those neuron models of intermediate complexity in which the output is a
spike whose timing is continuously variable as a result of cellular
interactions, providing a model of biological neurons that offers
more details than firing rate models but without the details of biophysical models. The virtues of such models include the ability to
transmit information very quickly through small temporal differences between the spikes sent out by different neurons. Information
theory can be used to quantify how much more information about
a stimulus can be extracted from spike trains if the precise timing
is taken into account. Moreover, computing with spiking neurons
may prove of benefit for technology.
AXONAL MODELING is centered on the Hodgkin and Huxley
model, arguably the most successful model in all of computational
neuroscience. The article shows how the Hodgkin-Huxley equations extend the cable equation to describe the ionic mechanisms
underlying the initiation and propagation of action potentials. The
vast majority of contemporary biophysical models use a mathematical formalism similar to that introduced by Hodgkin and Huxley, even though their model of the continuous, deterministic, and
macroscopic permeability changes of the membrane was achieved
without any knowledge of the underlying all-or-none, stochastic,
and microscopic ionic channels. The article also describes the differences between myelinated and nonmyelinated axons; and briefly
discusses the possible role of heavily branched axonal trees in information processing.
PERSPECTIVE ON NEURON MODEL COMPLEXITY then shows how
this type of modeling might be extended to the whole cell. The key
point is that one neuron with detailed modeling of dendrites (especially with nonuniform distributions of synapses and ion channels) can perform tasks that would require a network of many simple binary units to duplicate. The point is not to choose the most

complex model of a neuron but rather to seek an intermediate level
of complexity which preserves the most significant distinctions between different “compartments” of the neuron (soma, various portions of the dendritic tree, etc.). The challenge is to demonstrate a
useful computation or discrimination that can be accomplished
with a particular choice of compartments in a neuron model, and
then show that this useful capacity is lost when a coarser decomposition of the neuron is used. DENDRITIC PROCESSING especially
emphasizes developments in compartmental modeling of dendrites,
arguing that we are in the midst of a “dendritic revolution” that has
yielded a much more fascinating picture of the electrical behavior
and chemical properties of dendrites than one could have imagined
only a few years ago. The dendritic membrane hosts a variety of
nonlinear voltage-gated ion channels that endow dendrites with potentially powerful computing capabilities. Moreover, the classic
view of dendrites as carrying information unidirectionally, from
synapses to the soma, has been transformed: dendrites of many
central neurons also carry information in the “backward” direction,
via active propagation of the action potentials from the axon to the
dendrites. These “reversed” signals can trigger plastic changes in
the dendritic input synapses. Moreover, it is now known that the
fine morphology as well as the electrical properties of dendrites
change dynamically, in an activity-dependent manner.
If the most successful model in all of computational neuroscience is the Hodgkin-Huxley model, then the second most successful is Hebb’s model of “unsupervised” synaptic plasticity. The former was based on rigorous analysis of empirical data; the latter was
initially the result of theoretical speculation on how synapses might
behave if assemblies of cells were to work together to store and
reconstitute thoughts and associations. HEBBIAN SYNAPTIC PLASTICITY notes that predictions derived from Hebb’s postulate can be
generalized for different levels of integration (synaptic efficacy,
functional coupling, adaptive change in behavior) by simply adjusting the variables derived from various measures of neural activity and the time-scale over which it operates. The article addresses five major issues: Should the definition of “Hebbian”
plasticity refer to a simple positive correlational rule of learning,
or are there biological justifications for including additional
“pseudo-Hebbian” terms (such as synaptic depression due to disuse
or competition) in a generalized phenomenological algorithm?
What are the spatiotemporal constraints (e.g., input specificity, temporal associativity) that characterize the induction process? Do the
predictions of Hebbian-based algorithms account for most forms
of activity-dependent dynamics in synaptic transmission throughout phylogenesis? On which time-scales (perception, learning, epigenesis) and at which stage of development of the organism (embryonic, “critical” postnatal developmental periods, adulthood) are
activity-dependent changes in functional links predicted by Hebb’s
rule? Are there examples of correlation-based plasticity that contradict the predictions of Hebb’s postulate (termed anti-Hebbian
modifications)? The article thus frames many important issues to
be developed in the articles of the road map Neural Plasticity but
that are also implicit, for example, in articles reviewed in the road
maps Psychology and Linguistics and Speech Processing, in
which Hebbian (and other) learning rules are used for “formal neurons” that are psychological abstractions rather than representation
of real neurobiological neurons or even biological neuron pools.
Two other articles serve to introduce the basic learning rules that
have been most central in both biological analysis and connectionist modeling. Supervised learning adjusts the weights in an attempt
to respond to explicit error signals provided by a “teacher,” which
may be external, or another network in the same “brain.” This
model was introduced in the perceptron model, which is reviewed
in PERCEPTRONS, ADALINES, AND BACKPROPAGATION (of which
more details in the next road map, Grounding Models of Networks). On the other hand, REINFORCEMENT LEARNING (of which

II.3. Brain, Behavior, and Cognition
more details in the road map Learning in Artificial Networks)
shows how networks can improve their performance when given
general reinforcement (“that was good,” “that was bad”) by a critic,
rather than the explicit error information offered by a teacher.

Grounding Models of Networks
ASSOCIATIVE NETWORKS
COMPUTING WITH ATTRACTORS
PERCEPTRONS, ADALINES, AND BACKPROPAGATION
RADIAL BASIS FUNCTION NETWORKS
SELF-ORGANIZING FEATURE MAPS
SPIKING NEURONS, COMPUTATION WITH
The mechanisms and implications of association—the linkage of
information with other information—have a long history in psychology and philosophy. ASSOCIATIVE NETWORKS discusses association as realized in neural networks as well as association in
the more traditional senses. Many neural networks are designed as
pattern associators, which link an input pattern with the “correct”
output pattern. Learning rules are designed to construct useful linkages between input and output patterns whether in feedforward
neural network architectures or in a network whose units are recurrently interconnected. Special attention is given to the critical
importance of data representation at all levels of network operation.
PERCEPTRONS, ADALINES, AND BACKPROPAGATION introduces the
perceptron rule and the LMS (least-mean-squares) algorithm for
training feedforward networks with multiple adaptive elements,
where each element can be seen as an adaptive linear combiner of
its inputs followed by a nonlinearity which produces the output. It
then presents the major extension provided by the backpropagation
algorithm for training multilayer neural networks—which can be
viewed as dividing the input space into regions bounded by hyperplanes, one for the thresholded output of each neuron of the
output layer—and shows how this technique has been used to attack problems requiring neural networks with high degrees of nonlinearity and precision.
COMPUTING WITH ATTRACTORS shows how neural networks (often seen now as operating in continuous time) may be viewed as
dynamic systems (a theme developed in great detail by the articles
of the road map Dynamic Systems). This article describes how to
compute with networks with feedback, with the input of a computation being set as an initial state for the system and the result
read off a suitably chosen set of units when the network has “settled
down.” The state a dynamical system settles into is called an attractor, so this paradigm is called computing with attractors. It is
possible to settle down into an equilibrium state, or into periodic
or even chaotic patterns of activity. (An interesting possibility, not
considered in this article, is to perform computations based on the
transient approach to the attractor, rather than on the basis of the
attractor alone.) The Hopfield model for associative memory is
used as the key example, showing its dynamic behavior as well as
how the connections necessary to embed desired patterns can be

learned and how the paradigm can be extended to time-dependent
attractors.
SELF-ORGANIZING FEATURE MAPS (SOFMs) introduces a famous version of competitive learning based on a layer of adaptive
“neurons” that gradually develops into an array of feature detectors.
The learning method is an augmented Hebbian method in which
learning by the element most responsive to an input pattern is
“shared” with its neighbors. The result is that the resulting “compressed image” of the (usually higher-dimensional) input space
forms a “topographic map” in which distance relationships in the
input space (expressing, e.g., pattern similarities) are approximately
preserved as distance relationships between corresponding excitation sites in the map, while clusters of similar input patterns tend
to become mapped to areas in the neural layer whose size varies in
proportion to the frequency of the occurrence of their patterns.
From a statistical point of view, the SOFM provides a nonlinear
generalization of principal component analysis.
SPIKING NEURONS, COMPUTATION WITH discusses both the use
of spiking neurons as a useful approximation to biological neurons
and the study of networks of spiking neurons as a formal model of
computation for which the assumptions need not be biological (see
also “Integrate-and-Fire Neurons and Networks”). If the spiking
neurons are not subject to significant amounts of noise, then one
can carry out computations in networks of spiking neurons where
every spike matters, and some finite network of spiking neurons
can simulate a universal Turing machine. Spiking neurons can also
be used as computational units that function like radial basis functions in the temporal domain. Another code uses the order of firing
of different neurons as the relevant signal conveyed by these neurons. Firing rates of neurons in the cortex are relatively low, making
it hard for the postsynaptic neuron to “read” the firing rate of a
presynaptic neuron. However, networks of spiking neurons can
carry out complex analog computations if the inputs of the computation are presented in terms of a space rate or population code.
The last article in this road map gives an example of the utility
of studying networks in which the response properties of the individual units are designed not as abstractions from biological neurons, but rather because their response functions have mathematically desirable properties. A multilayer perceptron can be viewed
as dividing the input space into regions bounded by hyperplanes,
one for the thresholded output of each neuron of the output layer.
RADIAL BASIS FUNCTION NETWORKS describes an alternative approach to decomposition of a pattern space into regions, describing
the clusters of data points in the space as if they were generated
according to an underlying probability density function. Thus the
perceptron method concentrates on class boundaries, while the radial basis function approach focuses on regions where the data
density is highest, constructing global approximations to functions
using combinations of basis functions centered around weight vectors. The article shows that this approach not only has a range of
useful theoretical properties but also is practically useful, having
been applied efficiently to problems in discrimination, time-series
prediction, and feature extraction.

II.3. Brain, Behavior, and Cognition
Neuroethology and Evolution
COMMAND NEURONS AND COMMAND SYSTEMS
CRUSTACEAN STOMATOGASTRIC SYSTEM
ECHOLOCATION: COCHLEOTOPIC AND COMPUTATIONAL MAPS
ELECTROLOCATION

31

EVOLUTION AND LEARNING IN NEURAL NETWORKS
EVOLUTION OF ARTIFICIAL NEURAL NETWORKS
EVOLUTION OF GENETIC NETWORKS
EVOLUTION OF THE ANCESTRAL VERTEBRATE BRAIN
HIPPOCAMPUS: SPATIAL MODELS

32

Part II: Road Maps

LANGUAGE EVOLUTION AND CHANGE
LANGUAGE EVOLUTION: THE MIRROR SYSTEM HYPOTHESIS
LOCOMOTION, VERTEBRATE
LOCUST FLIGHT: COMPONENTS AND MECHANISMS IN THE MOTOR
MOTOR PRIMITIVES
NEUROETHOLOGY, COMPUTATIONAL
OLFACTORY CORTEX
SCRATCH REFLEX
SENSORIMOTOR INTERACTIONS AND CENTRAL PATTERN
GENERATORS
SOUND LOCALIZATION AND BINAURAL PROCESSING
SPINAL CORD OF LAMPREY: GENERATION OF LOCOMOTOR
PATTERNS
VISUAL COURSE CONTROL IN FLIES
VISUOMOTOR COORDINATION IN FROG AND TOAD
VISUOMOTOR COORDINATION IN SALAMANDER
Many readers will come to the Handbook with one of two main
motivations: to understand the human brain, or to explore the potential of ANNs as a technology for adaptive, parallel computation.
The present road map emphasizes a third motivation: to study neural mechanisms in creatures very different from humans and their
mammalian cousins—for the intrinsic interest of discovering the
diverse neural architectures that abound in nature, for the suggestions these provide for future technology, and for the novel perspective on human brain mechanisms offered by seeking to place
them in an evolutionary perspective.
Ethology is the study of animal behavior, in which our concern
is with the circumstances under which a particular motor pattern
will be deployed as an appropriate part of the animal’s activity.
Neuroethology, then, is the study of neural mechanisms underlying
animal behavior. The emphasis is thus on an integrative, systems
approach to the neuroscience of the animal being studied, as distinct from a reductionist approach to, for example, the neurochemistry of synaptic plasticity. Of course, a major aim of this Handbook
is to create a context in which the reader can see both approaches
to the study of nervous systems and ponder how best to integrate
them. In particular, the reader will find many examples of the neuroethology of mammalian systems in a wide variety of other road
maps, such as Cognitive Neuroscience, Vision, Other Sensory
Systems, and Mammalian Motor Control. However, the present
road map is designed to guide the reader to articles on a number
of fascinating nonmammalian systems—as well as a few “exotic”
mammalian systems—as a basis for a brief introduction of the evolutionary approach to biological and artificial neural networks.
NEUROETHOLOGY, COMPUTATIONAL suggests that computational neuroethology applies not only to animals but also to nonbiological autonomous agents, such as some types of robots and
simulated embodied agents operating in virtual worlds (see also
“Embodied Cognition”). The key element is the use of sophisticated computer-based simulation and visualization tools to study
the neural control of behavior within the context of “agents” that
are both embodied and situated within an environment. Other examples include specific neuroethological modeling directed toward
specific animals (the computational frog Rana computatrix and the
computational cockroach Periplaneta computatrix) and their implications for the rebirth of ideas first introduced by Grey Walter
in his 1950s design of Machina speculatrix and later developed in
the book Vehicles by Valentino Braitenberg.
If a certain interneuron is stimulated electrically in the brain of
a marine slug, the animal then displays a species-specific escape
swimming behavior, although no predator is present. If in a toad a
certain portion of the optic tectum is stimulated in this manner,
snapping behavior is triggered, although no prey is present. In both
cases, a stimulus produces a rapid ballistic response. Such command functions provide the sensorimotor interface between sensory

pattern recognition and localization, on the one side, and motor
pattern generation on the other. COMMAND NEURONS AND COMMAND SYSTEMS analyzes the extent to which a motor pattern generator (MPG) may be activated alone or in concert with others
through perceptual stimuli mediated by a single “command neuron” (as in the marine slug) or by more diffuse “command systems”
(as in the toad). Three articles then focus specifically on visuomotor
coordination. VISUAL COURSE CONTROL IN FLIES explains the
mechanisms underlying the extraction of retinal motion patterns in
the fly, and their transformation into the appropriate motor activity.
Rotatory large-field motion can signal unintended deviations from
the fly’s course and initiate a compensatory turn; image expansion
can signal that the animal approaches an obstacle and initiates a
landing or avoidance response; and discontinuities in the retinal
motion field indicate nearby stationary or moving objects. Since
many of the cells responsible for motion extraction are large and
individually identifiable, the fly is quite amenable to an analysis of
sensory processing. Similarly, the small number of muscles and
motor neurons used to generate flight maneuvers facilitates studies
of motor output. VISUOMOTOR COORDINATION IN SALAMANDER
shows how low-level mechanisms add up to produce complicated
behaviors, such as the devious approach of salamanders to their
prey. Coarse coding models demonstrate how the location of an
object may be encoded with high accuracy using only a few neurons with large, overlapping receptive fields. (This fits with the fact
that the brains of salamanders are anatomically the simplest among
vertebrates, containing only about 1 million neurons—frogs have
up to 10 times and humans 10 million times as many neurons.) The
models have been extended to the case where several objects are
presented to the animal by linking a segmentation network and a
winner-take-all-like object selection network to the coarse coding
network in a biologically plausible way. Compensation of background movement, selection of an object, saccade generation, and
approach and snapping behavior in salamanders have also been
modeled successfully, in line with behavioral and neurobiological
findings. Again, VISUOMOTOR COORDINATION IN FROG AND TOAD
stresses that visuomotor integration implies a complex transformation of sensory data, since the same locus of retinal activation
might release behavior directed toward the stimulus (as in prey
catching) or toward another part of the visual field (as in predator
avoidance). The article also shows how the efficacy of visual stimuli to release a response is determined by many factors, including
the stimulus situation, the motivational state of the organism itself,
and previous experience with the stimulus (learning and conditioning), and the physical condition of the animal’s CNS (e.g., brain
lesions). In addition, other types of sensory signals can modulate
frogs’ and toads’ response to certain moving visual stimuli. For
example, the efficacy of a visual stimulus may be greatly enhanced
by the presence of prey odor.
MOTOR PRIMITIVES and SCRATCH REFLEX are two of the articles
on nonmammalian animal behaviors that are described more fully
in the road map Motor Pattern Generators. These articles examine the behavior elicited in frogs and turtles, respectively, by an
irritant applied to the animal’s skin. The former article examines
the extent to which motor behaviors can be built up through a
combination of a small set of basic elements; the latter emphasizes
how the form of the scratch reflex changes dramatically, depending
on the locus of the irritant. Other articles in the road map Motor
Pattern Generators describe mechanisms underlying a variety of
forms of locomotion (swimming, walking, flying).
SOUND LOCALIZATION AND BINAURAL PROCESSING uses data
from owls, which are exquisitely skillful in using auditory signals
to locate their prey, even in the dark, to anchor models which explain how information from the two ears is brought together to
localize the source of a sound. The article focuses on the use of
interaural time difference (ITD) as one way to estimate the azi-

II.3. Brain, Behavior, and Cognition
muthal angle of a sound source. It describes one biological model
(ITD detection in the barn owl’s brainstem) and two psychological
models. The underlying idea is that the brain attempts to match the
sounds in the two ears by shifting one sound relative to the other,
with the shift that produces the best match assumed to be the one
that just balances the “real” ITD.
ECHOLOCATION: COCHLEOTOPIC AND COMPUTATIONAL MAPS
explores the highly specialized auditory system used by mustached
bats to analyze the return signals from the biosonar pulses they
emit for orientation and for hunting flying insects. Each biosonar
pulse consists of a long constant-frequency (CF) component followed by a short frequency-modulated (FM) component. The CF
components constitute an ideal signal for target detection and the
measurement of target velocity (relative motion in a radial direction
and wing beats of insects), whereas the short FM components are
suited for ranging, localizing, and characterizing a target. The article shows how different parameters of echoes received by the bat
carry different types of information about a target and how these
may be structured in computational maps via parallel-hierarchical
processing of different types of biosonar signals. These maps guide
the bat’s behavior. ELECTROLOCATION discusses another “exotic”
sensory system related to behavior, this time the electrosensory
systems of weakly electric fish. Animals with active electrosensory
systems generate an electrical field around their body by means of
an electrical organ located in the trunk and tail, and measure this
field via electroreceptors embedded in the skin. Distortions of the
electrical field due to animate or inanimate targets in the environment or signals generated by other fish provide inputs to the system,
and several distinct behaviors can be linked to patterns of electrosensory input. The article focuses on progress in understanding
electrolocation behavior and on the neural implementation of an
adaptive filter that attenuates the effects of the fish’s own
movements.
We now turn to motor systems. CRUSTACEAN STOMATOGASTRIC
SYSTEM shows that work on the rhythmic motor patterns of the
four areas of the crustacean stomach, the esophagus, cardiac sac,
gastric mill, and pylorus, has identified four widely applicable
properties. First, rhythmicity in these highly distributed networks
depends on both network synaptic connectivity and slow active
neuronal membrane properties. Second, modulatory influences can
induce individual networks to produce multiple outputs, “switch”
neurons between networks, or fuse individual networks into single
larger networks. Third, modulatory neuron terminals receive network synaptic input. Modulatory inputs can be sculpted by network
feedback and become integral parts of the networks they modulate.
Fourth, network synaptic strengths can vary as a function of pattern
cycle period and duty cycle.
The lamprey is a very primitive form of fish whose spinal cord
supports a traveling wave of activity that yields the swimming
movements of the animal’s body, yet also persists (“fictive swimming”) when the spinal cord is isolated from the body and kept
alive in a dish. SPINAL CORD OF LAMPREY: GENERATION OF LOCOMOTOR PATTERNS reviews the data which ground a circuit diagram for the spinal cord circuitry, then shows how the lamprey
locomotor network has been simulated. There are a number of neuromodulators present in the lamprey spinal cord that alter the output
of the locomotor network. These substances, such as serotonin,
dopamine, and tachykinins, offer good opportunities to test our
knowledge of the locomotor system by combining the cellular and
synaptic actions of the modulators into detailed network models.
However, models that do not depend on details of individual cells
have also proved useful in advancing our understanding of lamprey
locomotion such as the control of turning. Other models probe the
nature of the coupling among the rhythm generators, explaining
how it may be that the speed of the head-to-tail propagation of the
rhythmic activity down the spinal cord can vary with the speed of

33

swimming even though conduction delays in axons are fixed. LOCUST FLIGHT: COMPONENTS AND MECHANISMS IN THE MOTOR
stresses that locust flight motor patterns are generated by an interactive mixture of the intrinsic properties of flight neurons, the
operation of complex circuits, and phase-specific proprioceptive
input. These mechanisms are subject to the concentrations of circulating neuromodulators and are also modulated according to the
demands of a constantly changing sensory environment to produce
adaptive behaviors. The system is flexible and able to operate despite severe ablations, and then to recover from these lesions. SENSORIMOTOR INTERACTIONS AND CENTRAL PATTERN GENERATORS
analyzes basic properties of the biological systems performing sensorimotor integration. The article discusses both the impact of sensory information on central pattern generators and the less wellunderstood influence of motor systems on sensory activity.
Interaction between motor and sensory systems is pervasive, from
the first steps of sensory detection to the highest levels of processing. While there is no doubt that cortical systems contribute to
sensorimotor integration, the article questions the view that motor
cortex sends commands to a passively responsive spinal cord. Motor commands are only acted upon as spinal circuits integrate their
intrinsic activity with all incoming information.
Turning to evolution, we find two classes of articles. We first
look at those which focus on simulated evolution in ANNs, with
emphasis on the role of evolution as an alternative learning mechanism to fit network parameters to yield a network better adapted
to a given task. We then turn to articles more closely related to
comparative and evolutionary neurobiology.
When neural networks are studied in the broader biological context of artificial life (i.e., the attempt to synthesize lifelike phenomena within computers and other artificial media), they are sometimes characterized by genotypes and viewed as members of
evolving populations of networks in which genotypes are inherited
from parents to offspring. EVOLUTION OF ARTIFICIAL NEURAL
NETWORKS shows how ANNs can be evolved by using evolutionary algorithms (also known as genetic algorithms). An initial population of different artificial genotypes, each encoding the free parameters (e.g., the connection strengths and/or the architecture of
the network and/or the learning rules) of a corresponding neural
network, is created randomly. (An important challenge for future
research is to study models in which the genotypes are more “biological” in nature, and less closely tied to direct description of the
phenotype.) The population of networks is evaluated in order to
determine the performance (fitness) of each individual network.
The fittest networks are allowed to reproduce by generating copies
of their genotypes, with the addition of changes introduced by genetic operators such as mutations (i.e., the random change of a few
genes that are selected randomly) or crossover (i.e., the combination of parts of the genotype derived from two reproducing networks). This process is repeated for a number of generations until
a network that satisfies the performance criterion set by the experimenter is obtained. LOCOMOTION, VERTEBRATE shows that the
combination of neural models with biomechanical models has an
important role to play in addressing the evolutionary challenge of
seeing what modifications may have occurred in the locomotor
circuits between the generation of traveling waves for swimming
(the most ancestral vertebrates were close to the lamprey), the generation of standing waves for walking, and the generation of multiple gaits for quadruped locomotion, and on to biped locomotion.
One example uses “genetic algorithms” to model the transition
from a lamprey-like spinal cord that supports traveling waves to a
salamander-like spinal cord that supports both traveling waves for
swimming and “standing waves” for terrestrial locomotion, and
then shows how vision may modulate spinal activity to yield locomotion toward a goal (see also VISUOMOTOR COORDINATION IN
SALAMANDER).

34

Part II: Road Maps

EVOLUTION AND LEARNING IN NEURAL NETWORKS then extends
the analysis of ANN evolution to networks that are able to adapt
to the environment as a result of some form of lifetime learning.
Where evolution is capable of capturing relatively slow environmental changes that might encompass several generations, learning
allows an individual to adapt to environmental changes that are
unpredictable at the generational level. Moreover, while evolution
operates on the genotype, learning affects the phenotype, and phenotypic changes cannot directly modify the genotype. The article
shows how ANNs subjected both to an evolutionary and a lifetime
learning process have been studied to look at the advantages, in
terms of performance, of combining two different adaptation techniques and also to help understand the role of the interaction between learning and evolution in natural organisms. Continuing this
theme, LANGUAGE EVOLUTION AND CHANGE offers another style
of “connectionist evolution,” placing a number of connectionist
models of basic forms of language processing in an evolutionary
perspective. In some cases, connectionist networks are used as simulated agents to study how social transmission via learning may
give rise to the evolution of structured communication systems. In
other cases, the specific properties of neural network learning are
enlisted to help illuminate the constraints and processes that may
have been involved in the evolution of language. The article surveys this connectionist research, starting from the emergence of
early syntax, to the role of social interaction and constraints on
network learning in the subsequent evolution of language, to linguistic change within existing languages.
With this we turn to the study of evolution in the sense of natural
selection in biological systems, building on the insights of Charles
Darwin. Since brains do not leave fossils, evolutionary work is
more at the level of comparative neurobiology, looking at the nervous systems of currently extant species, then trying to build a
“family tree” of possible ancestors. The idea is that we may gain
deeper insights into the brains of animals of a given species if we
can compare them with the brains of other species, make plausible
inferences about the brain structure of their common ancestor, and
then seek to relate differences between the current brains and the
putative ancestral brains by relating these changes to the possible
evolutionary pressures that caused each species to adapt to a specific range of environments. EVOLUTION OF THE ANCESTRAL VERTEBRATE BRAIN notes that efforts to understand how the evolving
brain has adapted to specific environmental constraints are complicated because there are always several ways to implement a certain function within existing connections using molecular and cellular mechanisms. In any case, adult diversity is viewed as the
outcome of divergent genetic developmental mechanisms. Thus,
study of adult structures is aided by placing adult structures within
their developmental history as structured by the genes that guide
such development. The article introduces a possible prototype of
the ancestral vertebrate brain, followed by a scenario for mechanisms that may have diversified the ancestral vertebrate brain. Evolution of the brainstem oculomotor system is used as a focal case
study.
The study of gene expression patterns is playing an increasingly
important role in the empirical study of brains and neurons, and
the pace of innovation in this area has greatly accelerated with the
publication of two maps of the human genome as well as genome
maps for more and more other species. As of 2002, however, the
impact of “genomic neuroscience” on computational neuroscience
is still small. To help readers think about the promise of increasing
this impact, we not only have the discussion in EVOLUTION OF THE
ANCESTRAL VERTEBRATE BRAIN of how during development the
CNS becomes polarized and then subdivides into compartments,
each characterized by specific pattern of gene expression, but also
a companion article, EVOLUTION OF GENETIC NETWORKS, which
outlines some of the computational problems in modeling genetic

networks that can direct the establishment of a diversity of neuronal
networks in the brain. Since neuronal networks are composed of a
wide variety of different cell types, the final fate or end-stage of
each cell type represents the outcome of a dynamic amalgamation
of gene networks. Genetic networks not only determine the cell
fate acquisition from the original stem cell, they also govern contact
formation between the cell populations of a given neuronal network. There are intriguing parallels between the establishment and
functioning of genetic networks with those of neuronal networks,
which can range from simple (on-off switch) to complex. To give
some sense of the complexity of organismic development, the article outlines how intracellular as well as cell-cell interactions modify the complexity of gene interactions involved in genetic networks to achieve an altered status of cell function and, ultimately,
the connection alterations in the formation of neuronal networks.
OLFACTORY CORTEX describes how, during phylogeny, the paleocortex and archicortex develop in extent and complexity but
retain their three-layered character, whereas neocortex emerges in
mammals as a five- to six-layered structure. It stresses the evolutionary significance of the olfactory cortex and includes an account
for brain theorists interested in principles of cortical organization
of the early appearance of the olfactory cortex in phylogeny. Certainly, the cerebral cortex is a distinctive evolutionary feature of
the mammalian brain (which does not mean that it is “better” than
structures in other genera to which it may be more or less related),
and the next articles give two perspectives on its structure. “Grasping Movements: Visuomotor Transformations” presents the interactions of visual areas of parietal cortex with the F5 area of premotor cortex in the monkey brain in serving the visual control of
hand movements. The companion article, LANGUAGE EVOLUTION:
THE MIRROR SYSTEM HYPOTHESIS, starts from the observations
that monkey F5 contains a special set of “mirror neurons” active
not only when the monkey performs a specific grasp, but also when
the monkey sees others perform a similar task; that F5 is homologous to human Broca’s area, an area of cortex usually thought of
as related to speech production; but that Broca’s area also seems
to contain a mirror system for grasping. These facts are used to
ground a new theory of the evolution of the human brain mechanisms that support language. It adds a neurological “missing link”
to the long-held view that imitation and communication based on
hand signs may have preceded the emergence of human mechanisms for extensive vocal communication. With this example to
hand, the reader is invited to look through the book for articles that
study specific brain mechanisms or specific behaviors in a number
of species more or less related to the human. The challenge then is
to chart what aspects are common to human brains and the brains
more generally of primates, mammals, or even vertebrates; and
then, having done so, to see what, if any, distinctive properties
human brain and behavior possess. One can then seek an evolutionary account which illuminates these human capacities. For example, it is well known that the human hippocampus is crucial for
the creation of episodic memories, our memories of episodes located in specific contexts of space and time (though these memories
are eventually consolidated outside hippocampus). On the other
hand, HIPPOCAMPUS: SPATIAL MODELS emphasizes the role of the
hippocampus and related brain regions in building a map of spatial
relations in the rat’s world. To what extent can we come to better
understand human episodic memory by looking for the generalization from a spatial graph of the environment to one whose nodes
are linked in both space and time?

Mammalian Brain Regions
AUDITORY CORTEX
AUDITORY PERIPHERY AND COCHLEAR NUCLEUS
BASAL GANGLIA

1124

Part III: Articles

Figure 5. In support vector regression, a tube with radius e is fitted to the
data. The trade-off between model complexity and points lying outside of
the tube (with positive slack variables f) is determined by minimizing Equa-

tion 39. (From Schölkopf, B., and Smola, A. J., 2002, Learning with Kernels, Cambridge, MA: MIT Press. Reprinted with permission.):

Kernel Principal Component Analysis

with k ⬆ 0 must lie in the span of U-images of the training data.
Thus, we may expand the solution v as

The kernel trick can be used to develop nonlinear generalizations
of any algorithm that can be cast in terms of dot products, such as
PRINCIPAL COMPONENT ANALYSIS (PCA) (q.v.).
PCA in feature space leads to an algorithm called kernel PCA.
It is derived as follows. We wish to find eigenvectors v and eigenvalues k of the so-called covariance matrix C in the feature space,
where
C :⳱

1
m

m

兺 U(xi)U(xi)
i⳱1



(43)

In the case when Ᏼ is very high-dimensional, the computational
costs of doing this directly are prohibitive. Fortunately, one can
show that all solutions to
Cv ⳱ kv

(44)

m

v⳱

兺 ␣iU(xi)
i⳱1

(45)

thereby reducing the problem to that of finding the ␣i. It turns out
that this leads to a dual eigenvalue problem for the expansion
coefficients,
mk␣ ⳱ K␣

(46)

where ␣ ⳱ (␣1, . . . , ␣m) is normalized to satisfy 㛳␣㛳 ⳱ 1/k.
To extract nonlinear features from a test point x, we compute the
dot product between U(x) and the nth normalized eigenvector in
feature space,
2



m

vn, U(x) ⳱

兺

␣ink(xi, x)

(47)

i⳱1

A toy example is given in Figure 7. As in the case of SVMs, the
architecture can be visualized by Figure 6.

Implementation and Empirical Results

Figure 6. Architecture of SVMs and related kernel methods. The input x
and the expansion patterns (SVs) xi (we assume that we are dealing with
handwritten digits) are nonlinearly mapped (by U) into a feature space Ᏼ
where dot products are computed. Through the use of the kernel k, these
two layers are in practice computed in one step. The results are linearly
combined using weights vi, found by solving a quadratic program (in pattern
recognition, vi ⳱ yi␣i; in regression estimation, vi ⳱ ␣*i ⳮ ␣i) or an eigenvalue problem (kernel PCA). The linear combination is fed into the
function r (in pattern recognition, r(x) ⳱ sgn (x Ⳮ b); in regression estimation, r(x) ⳱ x Ⳮ b; in kernel PCA, r(x) ⳱ x). (From Schölkopf, B.,
and Smola, A. J., 2002, Learning with Kernels, Cambridge, MA: MIT Press.
Reprinted with permission.)

An initial weakness of SVMs was that the size of the quadratic
programming problem scaled with the number of SVs. This was
due to the fact that in Equation 32, the quadratic part contained at
least all SVs—the common practice was to extract the SVs by
going through the training data in chunks while regularly testing
for the possibility that patterns initially not identified as SVs become SVs at a later stage. This procedure is referred to as chunking.
Note that without chunking, the size of the matrix in the quadratic
part of the objective function would be m ⳯ m, where m is the
number of all training examples.
What happens if we have a high-noise problem? In this case,
many of the slack variables ni become non-zero, and all the corresponding examples become SVs. For this case, decomposition
algorithms were proposed, based on the observation that not only
can we leave out the non-SV examples (the xi with ␣i ⳱ 0) from
the current chunk, but also some of the SVs, especially those that
hit the upper boundary (␣i ⳱ C). The chunks are usually dealt with
using quadratic optimizers. Several public domain SV packages
and optimizers are listed on the web page http://www.kernelmachines.org.
Modern SVM implementations made it possible to train on some
rather large problems. Success stories include the 60,000 example

36

Part II: Road Maps

top of the layer. However, this discrepancy led to new experiments
and related changes in the model which resulted in a good replication of the actual physiological data and required only feedforward excitation. The article continues by analyzing the anatomical
substrates for orientation specificity and for surround modulation
of visual responses, and concludes by discussing the origins of
patterned anatomical connections. VISUAL SCENE PERCEPTION
moves beyond V1 to chart the bifurcation of V1 output in monkeys and humans into a pathway that ascends to the parietal cortex
(the dorsal “where/how” system involved in object location and
setting of parameters for action) and a pathway that descends to
inferotemporal cortex (the ventral “what” system involved in object recognition) (see also “Dissociations Between Visual Processing Modes”).
SOMATOSENSORY SYSTEM argues that the tactile stimulus representation changes from an original form (more or less isomorphic
to the stimulus itself) to a completely distributed form (underlying
perception) in a series of partial transformations in successive subcortical and cortical networks. At the level of primary somatosensory cortex, the neural image of the stimulus is sensitive to shape
and temporal features of peripheral stimuli, rather than simply reflecting the overall intensity of local stimulation. The processing
of somatosensory information is seen as modular on two different
scales: macrocolumnar in terms of “segregates” such as the cortical
barrels seen in rodent somatosensory cortex, each of which receives
its principal input from one of the facial whiskers; and minicolumnar, with each minicolumn in a segregate receiving afferent connections from a unique subset of the thalamic neurons projecting
to that segregate. The article argues that the causal factors involved
in body/object interactions are represented by the pyramidal cells
of somatosensory cortical areas in such a way that their ascending,
lateral, and feedback connections develop an internal working
model of mechanical interactions of the body with the outside
world. Such an internal model can endow the somatosensory cortex
with powerful interpretive and predictive capabilities that are crucial for haptic perception (i.e., tactile perception of proximal surroundings) and for control of object manipulation.
The auditory system is introduced in two articles. AUDITORY
PERIPHERY AND COCHLEAR NUCLEUS spells out how the auditory
periphery transforms a very high information rate acoustic stimulus
into a series of lower information rate auditory nerve firings, with
the incoming acoustic information split across hundreds of nerve
fibers to avoid loss of information. The transformation involves
complex mechanical-to-electrical transformations. The cochlear
nucleus continues this process of parallelization by creating multiple representations of the original acoustic stimulus, with each
representation presumably emphasizing different acoustic features
that are fed to other brainstem structures, such as the superior olivary complex, the nuclei of the lateral lemniscus, and the inferior
colliculus. These parallel pathways are believed to be specialized
for the processing of different auditory features used for sound
source classification and localization. From the inferior colliculus,
auditory information is passed via the medial geniculate body in
the thalamus to the auditory cortex. AUDITORY CORTEX stresses
the crucial role that auditory cortex plays in the perception and
localization of complex sounds. Although recent studies have expanded our knowledge of the neuroanatomical structure, the subdivisions, and the connectivities of all central auditory stages, relatively little is known about the functional organization of the
central auditory system. Nevertheless, a few auditory tasks have
been broadly accepted as vital for all mammals, such as sound
localization, timbre recognition, and pitch perception. The article
discusses a few of the functional and stimulus feature maps that
have been found or postulated, and relates them to the more intuitive and better understood case of the echolocating bats (cf. “Echolocation: Cochleotopic and Computational Maps”).

The olfactory system is distinctive in that paths from periphery
to cortex do not travel via a thalamic nucleus. The olfactory pathway begins with the olfactory receptor neurons in the nose, which
project their axons to the olfactory bulb. The function of the olfactory bulb is to perform the initial stages of sensory processing of
the olfactory signals before sending this information to the olfactory cortex. The study of the olfactory system offers prime examples of seeking a “basic circuit” that defines the irreducible minimum of neural components necessary for a model of the functions
carried out by a region. OLFACTORY BULB offers examples of information processing without impulses and of output functions of
dendrites (dendrodendritic synapses). The olfactory cortex is defined as the region of the cerebral cortex that receives direct connections from the olfactory bulb and is subdivided into several
areas that are distinct in terms of details of cell types, lamination,
and sites of output to the rest of the brain. The main area involved
in olfactory perception is the piriform (also called prepyriform)
cortex, which projects to the mediodorsal thalamus, which in turn
projects to the frontal neocortex. This is often regarded as the main
olfactory cortex, and is the subject of the article OLFACTORY CORTEX. Olfactory cortex is the earliest cortical region to differentiate
in the evolution of the vertebrate forebrain and is the only region
within the forebrain to receive direct sensory input. Models of olfactory cortex emphasize the importance of cortical dynamics, including the interactions of intrinsic excitatory and inhibitory circuits and the role of oscillatory potentials in the computations
performed by the cortex.
We now introduce motor cortex, then turn to three systems related to motor control and to visuomotor coordination in mammals
(cf. the road map Mammalian Motor Control): cortical areas involved in grasping, the basal ganglia, and cerebellum. MOTOR
CORTEX: CODING AND DECODING OF DIRECTIONAL OPERATIONS
spells out the relation between the direction of reaching and
changes in neuronal activity that have been established for several
brain areas, including the motor cortex. The cells involved each
have a broad tuning function, the peak of which is viewed as the
“preferred” direction of the cell. A movement in a particular direction will engage a whole population of cells. It is found that the
weighted vector sum of their neuronal preferences is a “population
vector” which points in (close to) the direction of the movement
for discrete movements in 2D and 3D space. GRASPING MOVEMENTS: VISUOMOTOR TRANSFORMATIONS shows the tight coupling
between (specific subregions of) parietal and premotor cortex in
controlling grasping. The AIP region of inferior parietal lobe appears to play a fundamental role in extracting intrinsic visual properties (“affordances”) from the object for organizing grasping
movements. The extracted visual information is then sent to the F5
region of premotor cortex, there activating neurons that code grip
types congruent to the size, shape, and orientation of the object. In
addition to visually activated neurons in AIP, there are AIP cells
whose activity is linked to motor activity, possibly reflecting corollary discharges sent by F5 back to the parietal cortex. (For the
possible relation of grasping to language, and the homology between F5 and Broca’s area, see “Language Evolution: The Mirror
System Hypothesis.”)
The basal ganglia include the striatum, the globus pallidus, the
substantia nigra, and the subthalamic nucleus. BASAL GANGLIA
stresses that all of these structures are functionally subdivided into
skeletomotor, oculomotor, associative, and limbic territories. The
basal ganglia can be viewed as a family of loops, each taking its
origin from a particular set of functionally related cortical fields,
passing through the functionally corresponding portions of the
basal ganglia, and returning to parts of those same cortical fields
by way of specific zones in the dorsal thalamus. The article reviews
models of the basal ganglia that attempt to incorporate appropriate
anatomical or physiological data, but not those that use only generic

II.3. Brain, Behavior, and Cognition
neural network architectures. Some models work at a comparatively low level of detail (membrane properties of individual neurons and microanatomical features) and restrict themselves to a
single component of the basal ganglia nucleus; others work at the
system level with the basal ganglia as a whole and with their interactions with related structures (e.g., thalamus and cortex). Since
dopamine neurons discharge in relation to conditions involving the
probability and imminence of behavioral reinforcement, dopamine
neurons have been seen as playing a role in striatal information
processing analogous to that of an “adaptive critic” in connectionist
networks (cf. “Reinforcement Learning” and “Dopamine, Roles
of”).
The division of function between cerebellum and basal ganglia
remains controversial. One view is that the basal ganglia play a
role in determining when to initiate one phase of movement or
another, and that the cerebellum adjusts the metrics of movement,
tuning different movements and coordinating them into a graceful
whole. CEREBELLUM AND MOTOR CONTROL reviews a number of
models for cerebellar mechanisms underlying the learning of motor
skills. Cerebellum can be decomposed into cerebellar nuclei and a
cerebellar cortex. The only output cells of the cerebellar cortex are
the Purkinje cells, and their only effect is to provide varying levels
of inhibition on the cerebellar nuclei. Each Purkinje cell receives
two types of input—a single climbing fiber, and many tens of thousands of parallel fibers. The most influential model of cerebellar
cortex has been the Marr-Albus model of the formation of associative memories between particular patterns on parallel fiber inputs
and Purkinje cell outputs, with the climbing fiber acting as “training
signal.” Later models place more emphasis on the relation between
the cortex and nuclei, and on the way in which the subregions of
this coupled cerebellar system can adapt and coordinate the activity
of specific motor pattern generators. The plasticity of the cerebellum is approached from a different direction in CEREBELLUM AND
CONDITIONING. Many experiments indicate that the cerebellum is
involved in learning and performance of classically conditioned
reflexes. The article reviews a number of models of the role of
cerebellum in rabbit eye blink conditioning, providing a useful
complement to models of the role of cerebellum in motor control.
The hippocampus has been implicated in a variety of memory
functions, both as working memory and as basis for long-term
memory. It was also the site for the discovery of long-term potentiation (LTP) in synapses (see “Hebbian Synaptic Plasticity”).
Structurally, hippocampus is the simplest form of cortex. It contains one projection cell type, whose cell bodies are confined to a
single layer, and receives inputs from all sensory systems and association areas. HIPPOCAMPUS: SPATIAL MODELS builds on the
finding that single-unit recordings in freely moving rats have revealed “place cells” in subfields of the hippocampus whose firing
is restricted to small portions of the rat’s environment (the corresponding “place fields”). These data underlie the seminal idea of
the hippocampus as a spatial map (cf. “Cognitive Maps”). The
article reviews the data and describes some models of hippocampal
place cells and of their role in circuits controlling the rat’s navigation through its environment. HIPPOCAMPAL RHYTHM GENERATION provides data and models on theta and other rhythms as well
as epileptic discharges, and also introduces the key cell types of
the hippocampus and a number of interconnections between the
hippocampus that seem to play a key role in the generation of these
patterns of activity.
Finally, we turn to prefrontal cortex, the association cortex of
the frontal lobes. It is one of the cortical regions to develop last
and most in the course of both primate evolution and individual
ontogeny. PREFRONTAL CORTEX IN TEMPORAL ORGANIZATION OF
ACTION suggests that the late morphological development of this
cortex in both cases is related to its support of higher cognitive
functions involving the capacity to execute novel and complex ac-

37

tions, which reaches its maximum in the adult human brain. The
lateral region of prefrontal cortex is involved in the representation
and temporal organization of sequential behavior. This article emphasizes the physiological functions of the lateral prefrontal cortex
in the temporal organization of behavior. Temporal integration of
sensory and motor information, through active short-term memory
(working memory) and prospective set, supports the goal-directed
performance of the perception-action cycle. This role extends to
the temporal organization of higher cognitive operations, including,
in the human, language and reasoning.

Cognitive Neuroscience
CORTICAL MEMORY
COVARIANCE STRUCTURAL EQUATION MODELING
EEG AND MEG ANALYSIS
EMOTIONAL CIRCUITS
EVENT-RELATED POTENTIALS
HEMISPHERIC INTERACTIONS AND SPECIALIZATION
IMAGING THE GRAMMATICAL BRAIN
IMAGING THE MOTOR BRAIN
IMAGING THE VISUAL BRAIN
IMITATION
LESIONED NETWORKS AS MODELS OF NEUROPSYCHOLOGICAL
DEFICITS
NEUROLINGUISTICS
NEUROLOGICAL AND PSYCHIATRIC DISORDERS
NEUROPSYCHOLOGICAL IMPAIRMENTS
PREFRONTAL CORTEX IN TEMPORAL ORGANIZATION OF ACTION
SEQUENCE LEARNING
STATISTICAL PARAMETRIC MAPPING OF CORTICAL ACTIVITY
PATTERNS
SYNTHETIC FUNCTIONAL BRAIN MAPPING
Cognitive neuroscience has been boosted tremendously in the last
decade by the rapid development and increasing use of techniques
to image the active human brain. The road map thus starts with
several articles on ways of observing activity in the human brain
and then examines various human cognitive functions.
The organization of large masses of neurons into synchronized
waves of activity lies at the basis of phenomena such as the electroencephalogram (EEG) and evoked potentials, as well as the magnetoencephalogram (MEG). The EEG consists of the electrical activity of relatively large neuronal populations that can be recorded
from the scalp, while the MEG can be recorded using very sensitive
transducers arranged around the head. EEG AND MEG ANALYSIS
reviews methods of quantitative analysis that have been applied to
extract information from these signals, providing an indispensable
tool for sleep and epilepsy research. Epilepsy is a neurological
disorder characterized by the occurrence of seizures, sudden
changes in neuronal activity that interfere with the normal functioning of neuronal networks, resulting in disturbances of sensory
or motor activity and of the flow of consciousness. During an epileptic seizure, the neuronal network exhibits typical oscillations
that usually propagate throughout the brain, involving progressively more brain systems. These oscillations are revealed in the
EEG (see also “Hippocampal Rhythm Generation”). In general, the
same brain sources account for the EEG and the MEG, with the
reservation that the MEG reflects magnetic fields perpendicular to
the skull that are caused by tangential current dipolar fields,
whereas the EEG/MEG reflects both radial and tangential fields.
This property can be used advantageously to disentangle radial
sources lying in the convexity of cortical gyri from tangential
sources lying in the sulci.
EVENT-RELATED POTENTIALS shows how cortical event-related
potentials (ERPs) arise from synchronous interactions among large

38

Part II: Road Maps

numbers of participating neurons. These include dense local interactions involving excitatory pyramidal neurons and inhibitory interneurons, as well as long-range interactions mediated by axonal
pathways in the white matter. Depending on the types of interaction
that occur in a specific behavioral condition, cortical networks may
display different states of synchrony, causing their ERPs to oscillate in different frequency bands, designated delta (0–4 Hz), theta
(5–8 Hz), alpha (9–12 Hz), beta (13–30 Hz), and gamma (31–100
Hz). Depending on the location and size of the recording and reference electrodes, recorded cortical field potentials integrate neural
activity over a range of spatial scales: from the intracortical local
field potential (LFP) to the intracranial electrocorticogram (ECoG)
to the extracranial electroencephalogram (EEG). ERP studies have
shown that local cortical area networks are able to synchronize and
desynchronize their activity rapidly with changes in cognitive state.
When incorporated into ANNs, the result could be a metastable
large-scale neural network design that recruits and excludes subnetworks according to their ability to reach consensual local patterns, with the ability to implement behavioral schemas and adapt
to changing environmental conditions.
Positron emission tomography (PET) and functional magnetic
resonance imaging (fMRI) provide means for seeing which brain
regions are significantly more active in one task rather than another.
Functional neuroimaging is generally used to make inferences
about functional anatomy on the basis of evoked patterns of cortical
activity. Functional anatomy involves an understanding of what
each part of the brain does, and how different brain systems interact
to support various sensorimotor and cognitive functions. Largescale organization can be inferred from techniques that image the
hemodynamic and metabolic sequelae of evoked neuronal responses. PET measures regional cerebral blood flow (rCBF) and
fMRI measures oxygenation changes. Their spatial resolution is on
the order of a few millimeters. Because PET uses radiotracers, its
temporal resolution is limited to a minute or more by the half-life
of the tracers employed. However, fMRI is limited only by the
biophysical time constants of hemodynamic responses themselves
(a few seconds).
STATISTICAL PARAMETRIC MAPPING OF CORTICAL ACTIVITY
PATTERNS considers the neurobiological motivations for different
designs and analyses of functional brain imaging studies, noting
that the principles of functional specialization and integration serve
as the motivation for most analyses. Statistical parametric mapping
(SPM) is used to identify functionally specialized brain regions that
respond selectively to experimental cognitive or sensorimotor
changes, irrespective of changes elsewhere. SPM is a voxel-based
approach (a voxel is a volume element of a 3D image) employing
standard inferential statistics. SPM is a mass-univariate approach,
in the sense that each data sequence, from every voxel, is treated
as a univariate response. The massive numbers of voxels are analyzed in parallel, and dependencies among them are dealt with using random field theory (see “Markov Random Field Models in
Image Processing”).
One approach to systems-level neural modeling aims at determining the network of brain regions mediating a specific cognitive
task. This means finding the nodes of the network (i.e., the brain
regions), and determining the task-dependent functional strengths
of their interregional anatomical linkages. COVARIANCE STRUCTURAL EQUATION MODELING describes techniques applied to the
correlations between PET- or fMRI-determined regional brain activities. These correlations are viewed as “functional connectivities.” They thus vary from task to task, as different patterns of
excitation and inhibition are routed through the anatomical connections of these regions. Examples of questions that can be answered using this approach are: (1) As one learns a task, do the
functional links between specific brain regions change their values?
(2) In cases of similar performance, are the same brain networks

being used by normals and patients? The method is illustrated with
studies of object and spatial vision showing cross-talk between the
dorsal and ventral streams (see “Visual Scene Perception”), which
implies that they need not be functionally independent. The article
stresses the concept of a neural context, where the functional relevance of a particular region is determined by its interactions with
other areas. Because the pattern of interactions with other connected areas differs from task to task, the resulting cognitive operations may vary within a single region as it engages in different
tasks.
SYNTHETIC FUNCTIONAL BRAIN MAPPING analyzes ways in
which models of neural networks grounded in primate neurophysiology can be used as the basis for predictions of the results of
human brain imaging. This is crucial for furthering our understanding of the neural basis of behavior. Covariance structural equation
modeling helps identify the nodes of the region-by-region network
corresponding to a cognitive task, especially when there is little or
no nonhuman data available (e.g., most language tasks). Synthetic
functional brain mapping uses primate data to form hypotheses
about the neural mechanisms whereby cognitive tasks are implemented in humans, with PET and fMRI data providing constraints
on the possible ways in which these neural systems function. This
is illustrated in relation to the mechanisms underlying saccadic eye
movements and working memory.
The next three articles focus on what we are learning about vision, motor activity, and language from functional brain imaging.
IMAGING THE VISUAL BRAIN addresses functional brain imaging of
visual processes, with emphasis on limits in spatial and temporal
resolution; constraints on subject participation; and trade-offs in
experimental design. The articles focuses on retinotopy, visual motion perception, visual object representation, and voluntary modulation of attention and visual imagery, emphasizing some of the
areas where modeling and brain theory might be testable using
current imaging tools. IMAGING THE MOTOR BRAIN shows that the
behavioral form and context of a movement are important determinants of functional activity within cortical motor areas and the
cerebellum, stressing that functional imaging of the human motor
system requires one to study the interaction of neurological and
cognitive processes with the biomechanical characteristics of the
effectors. Multiple neural systems must interact to successfully perform motor tasks, encode relevant information for motor learning,
and update behavioral performance in real time. The article discusses how evidence from functional imaging studies provides
insight into motor automaticity as well as the role of internal models in movement. The article also discusses novel mathematical
techniques that extend the scope of functional imaging experimentation.
IMAGING THE GRAMMATICAL BRAIN reviews brain imaging results that support the author’s view that linguistic rules are neurally
real and form a constitutive element of the human language faculty.
The focus is on linguistic combinations at the sentence level; but
an analysis of cerebral representation of phonological units and of
word meaning in its isolated and compositional aspects is provided
as background. The study of brain mechanisms supporting language is further advanced in NEUROLINGUISTICS. Neurolinguistics
began as the study of the language deficits occurring after brain
injuries, and is rooted in the conceptual model of Broca’s aphasia,
Wernicke’s aphasia, and other aphasic syndromes established over
a hundred years ago. The article presents data and analyses for
between-stage information flow, dynamics of within-stage processing, unitary representations and activation, and processing by
constraint satisfaction. (For more background on these two articles,
see the road map Linguistics and Speech Processing.)
PREFRONTAL CORTEX IN TEMPORAL ORGANIZATION OF ACTION
emphasizes the physiological functions of the lateral prefrontal cortex in the temporal organization of behavior, highlighting active

II.3. Brain, Behavior, and Cognition
short-term memory (working memory) and prospective set. The
two cooperate toward temporally integrating sensory and motor
information by mediating cross-temporal contingencies of behavior
(see also “Competitive Queuing for Planning and Serial Performance”). This temporal integration supports the goal-directed performance of the perception-action cycle. It is a role that extends to
the temporal organization of higher cognitive operations, including
language and reasoning in humans. CORTICAL MEMORY stresses
that some components of memory are localized in discrete domains
of cortex, while others are more widely distributed. It outlines a
view of network memory in the neocortex that is supported by
empirical evidence from neuropsychology, behavioral neurophysiology, and neuroimaging. Its essential features are the acquisition
of memory by the formation and expansion of networks of neocortical neurons through changes in synaptic transmission; and the
hierarchical organization of memory networks, with a hierarchy of
networks in posterior cortex for perceptual memory and another in
frontal cortex for executive memory. SEQUENCE LEARNING characterizes behavioral sequences in terms of their serial, temporal,
and abstract structure, and analyzes the associated neural processing systems (see also “Temporal Pattern Processing”). Temporal
structure is defined in terms of the durations of elements (and the
possible pauses that separate them), and intuitively corresponds to
the familiar notion of rhythm. Abstract structure is defined in terms
of generative rules that describe relations between repeating elements within a sequence. Thus, the two sequences A-B-C-B-A-C
and D-E-F-E-D-F are both generated from the same abstract structure 123-213. The article focuses on how the different dimensions
of sequence structure can be encoded in neural systems, citing behavioral studies in different patient and control groups and related
simulation studies. A recurrent network for manipulating abstract
structural relations is implemented in a distributed network that
potentially includes the perisylvian cortex in and around Broca’s
area. It is argued that both transfer of sequence knowledge between
domains and abstract rule representation are likely to be neurophysiological realities.
Complementing sequence learning is the study of imitation, the
ability to recognize and reproduce others’ actions. Imitation is also
related to fundamental capabilities for social cognition such as the
recognition of conspecifics, the attribution of others’ intentions,
and the ability to deceive and to manipulate others’ states of mind.
IMITATION bridges between biology and engineering, reviewing the
cognitive and neural processes behind the different forms of imitation seen in animals and showing how studies of biological processes influence the design of robot controllers and computational
algorithms. Theoretical models have been proposed to, e.g., distinguish between purely associative imitation (low-level) and sequential imitation (high-level). It is argued that modeling of imitation
will lead to a better understanding of the neural mechanisms at the
basis of social cognition and will offer new perspectives on the
evolution of animal abilities for social representation (see “Language Evolution: The Mirror System Hypothesis” for more on evolution and imitation).
EMOTIONAL CIRCUITS stresses the distinction between emotional
experiences and the underlying processes that lead to emotional
experiences. (See also “Motivation” for a discussion of the motivated or goal-directed behaviors that are often accompanied by
emotion or affect.) The article is grounded in studies of how the
brain detects and evaluates emotional stimuli and how, on the basis
of such evaluations, appropriate responses are produced, treating
emotion as a function that allows the organism to respond in an
adaptive manner to challenges in the environment rather than being
inextricably compounded with the subjective experience of emotion. The amygdala is shown to play a major role in the evaluation
process. It is argued that fearful stimuli follow two main routes.
The fast route involves the thalamo-amygdala pathway and responds best to simple stimulus features, while the slow route in-

39

volves the thalamo-cortical-amygdala pathway and carries more
complex features (such as context). The expression of fear is mediated by the outputs of the amygdala to brainstem and hypothalamus, while the experience of fear involves the prefrontal cortex.
One cerebral hemisphere may perform better than the other for
such diverse tasks as language, handedness, visuospatial processing, emotion and its facial expression, olfaction, and attention. Behavioral lateralization has not only been demonstrated in people,
but also in rodents, birds, primates, and other animals in areas such
as vocalization and motor preferences. Many anatomical, biochemical, and physiological asymmetries exist in the brain, but it is
generally unclear which, if any, of these asymmetries actually contribute to hemispheric specialization. Pathways such as the corpus
callosum connecting the hemispheres appear to mediate both excitatory and longer-term inhibitory interactions between the hemispheres. HEMISPHERIC INTERACTIONS AND SPECIALIZATION first
considers models of hemispheric interactions that do not incorporate hemispheric differences, and conversely, models examining
the effects of hemispheric differences that do not incorporate hemispheric interactions. It then looks in more detail at recent studies
demonstrating how both hemispheric interactions and differences
influence the emergence of lateralization in models where lateralization is not initially present.
As we already saw in, e.g., NEUROLINGUISTICS, cognitive neuropsychology uses neurological data on the performance of braindamaged patients to constrain models of normal cognitive function.
LESIONED NETWORKS AS MODELS OF NEUROPSYCHOLOGICAL
DEFICITS surveys how connectionist techniques have been employed to model the operation and interaction of “modules” inferred from the neurological data. The advantage over “box-andarrow” models is that removing neurons or connections in
connectionist models leads to natural analogues of real brain damage. Moreover, such models let one explore the possibility that
processing is actually more distributed and interactive than the
older models implied. The article discusses the effects of simulated
lesioning on various models, constructed either as feedforward networks or as attractor networks, paying special attention to the misleading artifacts that may arise when large brains are modeled by
small ANNs. Continuing with this theme, NEUROPSYCHOLOGICAL
IMPAIRMENTS cautions that the inferences that link a neuropsychological impairment to a particular theory in cognitive neuroscience
are not as direct as one might at first assume. The brain is a distributed and highly interactive system, such that local damage to
one part can unleash new modes of functioning in the remaining
parts of the system. The article emphasizes neural network models
of cognition and the brain that provide a framework for reasoning
about the effects of local lesions in distributed, interactive systems.
In many cases a model’s behavior after lesioning is somewhat
counterintuitive and so can lead to very different interpretations
regarding the nature of the normal system. A model of neglect
dyslexia shows how an impairment in a prelexical attentional process could nevertheless show a lexicality effect. Prosopagnosia is
an impairment of face recognition that can occur relatively independently of impairments in object recognition. The behavior of
some prosopagnosic patients seems to suggest that that recognition
and awareness depend on dissociable and distinct brain systems.
However, a model of covert face recognition demonstrates how
dissociation may occur without separate systems. NEUROLOGICAL
AND PSYCHIATRIC DISORDERS shows how neural modeling may be
harnessed to investigate the pathogenesis and potential treatment
of brain disorders by studying the relation between the “microscopic” pathological alterations of the underlying neural networks
and the “macroscopic” functional and behavioral disease manifestations that characterize the network’s function. The article reviews
computational studies of the neurological disorders of Alzheimer’s
disease, Parkinson’s disease, and stroke, and the psychiatric disorders of schizophrenia and affective disorders.

40

Part II: Road Maps

II.4. Psychology, Linguistics, and Artificial Intelligence
Psychology
ANALOGY-BASED REASONING AND METAPHOR
ASSOCIATIVE NETWORKS
COGNITIVE DEVELOPMENT
COGNITIVE MAPS
COGNITIVE MODELING: PSYCHOLOGY AND CONNECTIONISM
COMPOSITIONALITY IN NEURAL SYSTEMS
CONCEPT LEARNING
CONDITIONING
CONSCIOUSNESS, NEURAL MODELS OF
DEVELOPMENTAL DISORDERS
EMBODIED COGNITION
EMOTIONAL CIRCUITS
FACE RECOGNITION: PSYCHOLOGY AND CONNECTIONISM
MOTIVATION
PHILOSOPHICAL ISSUES IN BRAIN THEORY AND CONNECTIONISM
SCHEMA THEORY
SYSTEMATICITY OF GENERALIZATIONS IN CONNECTIONIST
NETWORKS
Much classical psychology was grounded in notions of association—of ideas, or of stimulus and response—which were well developed in the philosophy of Hume, but with roots going back as
far as Aristotle. ASSOCIATIVE NETWORKS shows how these old
ideas gain new power because neural networks can provide mechanisms for the formation of associations that automatically yield
many further properties. One of these is that neural networks will
in many cases have similar responses to similar inputs, a property
that is exploited in the study of ANALOGY-BASED REASONING AND
METAPHOR. Analogy and metaphor have been characterized as
comparison processes that permit one domain to be seen in terms
of another. Indeed, many of the advantages suggested for connectionist models—representation completion, similarity-based generalization, graceful degradation, and learning—also apply to analogy, yet analogical processing poses significant challenges for
connectionist models. Analogy and metaphor involve structured
pattern matching, structured pattern completion, and a focus on
common relational structure rather than on common object descriptions. The article analyzes current connectionist models of
analogy and metaphor in terms of representations and associated
processes, not in terms of brain function. Challenges for future
research include building analogical models that can preserve structural relations over incrementally extended analogies and that can
be used as components of a broader cognitive system such as one
that would perform problem solving. Indeed, people continually
deal with composite structures whether they result from aggregation of symbols in a natural language into syllables, words, and
sentences or aggregation of visual features into contour and regions, objects, and complete scenes. COMPOSITIONALITY IN NEURAL SYSTEMS addresses the question of what sort of neural dynamics allows composite structures to emerge, with the grouping and
binding of parts into interpretable wholes. To this day it is still
disputed whether ANNs are capable of adequately handling compositional data, and if so, which type of network is most suitable.
Basic results have been obtained with simple recurrent networks,
but some researchers argue that more complicated dynamics (see,
e.g., “Synchronization, Binding and Expectancy”) or dynamics
similar to classical symbolic processing mechanisms are necessary
for successful modeling of compositionality. In a related vein, SYSTEMATICITY OF GENERALIZATIONS IN CONNECTIONIST NETWORKS
presents the current “state of play” for Fodor and Pylyshyn’s critique of connectionist architecture. They claimed that human cog-

nitive abilities “come in clumps” (i.e., the abilities are systematically related), and that this systematic relationship does not hold
in connectionist networks. The present article examines claims and
counterclaims concerning the idea that learning in connectionist
architectures can engender systematicity, with special attention
paid to studies based on simple recurrent networks (SRNs) and
recursive auto-associative memory (RAAM). The conclusion is
that, for now, evidence for systematicity in such simple networks
is rather limited. (One may ponder the fact that animal brains are
vastly more complex than a single SRN or RAAM.)
The “units of thought” afforded by connectionist “neurons” are
quite high level compared to the fine-grain computation of the myriad neurons in the human brain, and their properties may hence be
closer to those of entire neural networks than to single biological
neurons. Moreover, future connectionist accounts of cognition will
certainly involve the coordination of connectionist modules (see,
e.g., “Hybrid Connectionist/Symbolic Systems”). SCHEMA THEORY complements neuroscience’s well-established terminology for
levels of structural analysis (brain region, neuron, synapse) with a
functional vocabulary, a framework for analysis of behavior with
no necessary commitment to hypotheses on the localization of each
schema (unit of functional analysis), but which can be linked to a
structural analysis whenever appropriate. The article focuses on
two issues: structuring perceptual and motor schemas to provide
an action-oriented account of behavior and cognition (as relevant
to the roboticist as the ethologist), and how schemas describing
animal behavior may be mapped to interacting regions of the brain.
Schema-based modeling becomes part of neuroscience when constrained by data provided by, e.g., human brain mapping, studies
of the effects of brain lesions, or neurophysiology. The resulting
model may constitute an adequate explanation in itself or may provide the framework for modeling at the level of neural networks
or below. Such a neural schema theory provides a functional/structural decomposition, in strong contrast to models that employ learning rules to train a single neural network to respond as specified
by some training set.
Connectionism can apply many different types of ANN techniques to explain psychological phenomena, and the article COGNITIVE MODELING: PSYCHOLOGY AND CONNECTIONISM places a
sample of these in perspective. The general idea is that much of
psychology is better understood in terms of parallel networks of
adaptive units than in terms of serial symbol processing, and that
connectionism gains much of its power from using very simple
units with explicit learning rules. The article points out that connectionist models of cognition can be used both to model cognitive
processes and to simulate the performance of tasks and that, unlike
many traditional computational models, they are not explicitly programmed by the investigator. However, important aspects of the
performance of a connectionist net are controlled by the researcher,
so that the achievement of a good fit to the psychological data
depends both on the way in which analogs to the data are derived
and on the results of “extensional programming,” such as decisions
about the selection and presentation of training data. The article
also notes the work of “cognitive connectionists,” whose computational experiments have demonstrated the ability of connectionist
representations to provide a promisingly different account of important characteristics of cognition (compositionality and systematicity), previously assumed to be the exclusive province of the
classical symbolic tradition. PHILOSOPHICAL ISSUES IN BRAIN THEORY AND CONNECTIONISM asks the following questions: (1) Do
neural systems exploit classical compositional and systematic representations, distributed representations, or no representations at
all? (2) How do results emerging from neuroscience help constrain

II.4. Psychology, Linguistics, and Artificial Intelligence
cognitive scientific models? (3) In what ways might embodiment,
action, and dynamics matter for understanding the mind and the
brain? There is a growing emphasis on the computational economies afforded by real-world action and the way larger structures
(of agents and artifacts) both scaffold and transform the shape of
individual reason. However, rather than seeing representations as
opposed to interactive dynamics, the article advocates a broader
vision of the inner representational resources themselves, stressing
the benefits of converging influences from robotics, systems-level
neuroscience, cognitive psychology, evolutionary theory, AI, and
philosophical analysis. This philosophical theme is further developed in CONSCIOUSNESS, NEURAL MODELS OF, which reviews the
basic ways in which consciousness has been defined, relevant neuropsychological data, and preliminary progress in neural modeling.
Among the characteristics needed for consciousness are temporal
duration, attentional focus, binding, bodily inputs, salience, past
experience, and inner perspective. Brain imaging, as well as insights into single-cell activity and the effects of brain deficits, is
leading to a clearer picture of the neural correlates of consciousness. The article presents a specific attention control model of the
emergence of awareness in which experience of the prereflective
self is identified with the corollary discharge of the attention movement control signal. This signal is posited to reside briefly in its
buffer until the arrival of the associated attended input activation
at its own buffer. The article concludes by reviewing other neural
models of consciousness.
Much of the early work on ANNs was inspired by the problem
of “Pattern Recognition” (q.v.). CONCEPT LEARNING provides a
general introduction to recent work, placing such ideas in a psychological perspective. Concepts are mental representations of
kinds of objects, events, or ideas. The article focuses on learning
mental representations of new concepts from experience and how
mental representations of concepts are used to make categorization
decisions and other kinds of judgments. The article reviews five
types of concept learning models: rule models, prototype models,
exemplar models, mixed models, and neuroscientific models. The
mechanisms discussed briefly here are developed at greater length
in many articles in the road map Learning in Artificial Networks.
The psychology of concept learning receives special application in
the study of FACE RECOGNITION: PSYCHOLOGY AND CONNECTIONISM, which relates connectionist approaches to face recognition to
psychological theories for the subtasks of representing faces and
retrieving them from memory, comparing human and model performance along these dimensions.
Many of the concepts of connectionist psychology are strongly
related to work in behaviorism, but neural networks provide a
stronger “internal structure” than stimulus-response probabilities.
Connectionist research has enriched a number of concepts that
seemed “anticognitive” by embedding them in mechanisms,
namely, neural nets, which can both support internal states and
yield stimulus-response pairs as part of a general input-output map.
This is shown in CONDITIONING. During conditioning, animals
modify their behavior as a consequence of their experience of the
contingencies between environmental events. This article presents
formal theories and neural network models that have been proposed
to describe classical and operant conditioning. During classical
conditioning, animals change their behavior as a result of the contingencies between the conditioned stimulus (CS) and the unconditioned stimulus (US). Contingencies may vary from very simple
to extremely complex ones. For example, in Pavlov’s proverbial
experiment, dogs were exposed to the sound of a bell (CS) followed
by food (US). At the beginning of training, animals salivated (generated an unconditioned response, UR) only when the US was presented. With an increasing number of CS-US pairings, CS presentations elicited a conditioned response (CR). The article discusses
variations in the effectiveness of the CS, the US, and the CS and

41

US together, as well as attentional models. During operant (or instrumental) conditioning, animals change their behavior as a result
of a triple contingency between its responses (R), discriminative
stimuli (SD), and the reinforcer (US). Animals are exposed to the
US in a relatively close temporal relationship with the SD and R.
As in “Reinforcement Learning” (q.v.), during operant conditioning animals learn by trial and error from feedback that evaluates
their behavior but does not indicate the correct behavior. The article
discusses positive reinforcement and negative reinforcement. Such
ideas are further developed in COGNITIVE MAPS. Tolman introduced the notion of a cognitive map to explain animals’ capacity
for place learning, latent learning, detours, and shortcuts. In some
models, Tolman’s vicarious trial-and-error behavior has been regarded as reflecting the animal’s comparison of different expectancies: at choice points, animals make a decision after sampling the
intensity of the activation elicited by the various alternative paths.
Other models still use Tolman’s stimulus-approach view and assume that animals approach the place with the strongest appetitive
activation, thereby performing a gradient ascent toward the goal.
In addition to storing the representation of the environment in the
terms of the contiguity between places, cognitive maps can store
information about differences in height and the type of terrain between adjacent places, contain a priori knowledge of the space to
be explored, distinguish between roads taken and those not taken,
and keep track of which places have been examined. Neural networks with more than two layers can also be used to represent both
the contiguity between places and the relative position of those
places. Hierarchical cognitive maps can represent the environment
at multiple levels. In contrast to their nonhierarchical counterparts,
hierarchical maps can plan navigation in large environment, use a
smaller number of connections in their networks, and have shorter
decision times.
Learning in neural nets can be either supervised or unsupervised,
and supervision can be in terms of a specific error signal or some
general reinforcement. However, in real animals, these signals
seem to have some “heat” to them, which brings us to the issues
of motivation and emotion. Motivated or goal-directed behaviors
are sets of motor actions that direct an animal toward a particular
goal object. Interaction with the goal either promotes the survival
of an individual or maintains the species. Motivated behaviors include sleep/wake, ingestive, reproductive, thermoregulatory, and
aggressive/defensive behaviors (see also “Pain Networks”). They
are often accompanied by emotion or affect. Given the difficulty
of defining the terms drive, instinct, and motivation with respect to
the neural substrates of behavior, MOTIVATION adopts a neural systems approach that discusses what and how particular parts of the
brain contribute to the expression of behaviors that have a motivated character. The approach is based on Hullian incentive models
of motivation, where the probability of a particular behavior depends on the integration of information from systems that control
circadian timing and regulate arousal state, inputs derived from
interosensory information that encode internal state (e.g., hydration
state, plasma glucose, leptin, etc.), modulatory hormonal inputs
such as gonadal steroids that mediate sexual behavior, and inputs
derived from classic sensory modalities. EMOTIONAL CIRCUITS analyzes the nature of emotion, emphasizing its role in behavior rather
than the subjective feelings that accompany human emotions, then
examines the role of brain structures such as the amygdala, the
interaction of body and cognitive states, and the status of neural
modeling. The expression of fear is seen as mediated by the outputs
of the amygdala to lower brain centers (brainstem, hypothalamus),
while the experience of fear involves the prefrontal cortex.
Finally, we turn to development, a theme of special concern in
connectionist linguistics (see the next road map). COGNITIVE DEVELOPMENT reviews connectionist models of the origins of knowledge, the mechanisms of change, and the task-dependent nature of

42

Part II: Road Maps

developing knowledge across a variety of domains. In each case,
the models provided explicit instantiations and controlled tests of
specific theories of development, and allowed the exploration of
complex, emergent phenomena. However, most connectionist
models are “fed” their input patterns regardless of what they output,
whereas even very young children shape their environments based
on how they behave. Moreover, most connectionist models are designed for and tested on a single task within a single domain,
whereas children face a multitude of tasks across a range of domains each day. Capturing such features of development will require future models to take in a variety of types of information and
learn how to perform successfully across a number of tasks. DEVELOPMENTAL DISORDERS uses the comparison of different abnormal phenotypes to explore further the modeling of the developing
mind/brain. The article reviews recent examples of connectionist
models of developmental disorders. Autism is a developmental disorder characterized primarily by deficits in social interaction, communication, and imagination, but also by a range of secondary deficits. One hypothesis suggests that these structural deficits are
consistent with too few neurons in some brain areas, such as the
cerebellum, and too many neurons in other areas, such as the amygdala and hippocampus. This grounds a simple connectionist model
trained on categorization tasks linking such differences in neurocomputational constraints to some of the secondary deficits found
in autism. Other models relate disordered feature maps or hidden
unit numbers to higher-level cognitive deficits that characterize autism. Developmental dyslexia has been modeled by changing parameters in models of the normal processes of reading. Another
model captures some features of specific language impairment, specifically the difficulty of affected patients in learning rule-based
inflectional morphology in verbs, using an attractor network mapping between semantic codes and phonological codes. The article
also reports new empirical findings on Williams syndrome patients
which reveal a deficit in generalizing knowledge of inflectional
patterns to novel forms. Alterations in the initial computational
constraints of a connectionist model of past tense development are
shown to account for some of the patterns seen in such data, demonstrating how different computational constraints interact in the
process of development. Connectionist models thus provide a powerful tool with which to investigate the role of initial computational
constraints in determining the trajectory of both typical and atypical
development, ensuring that selective deficits in developmental disorders are seen in terms of the outcome of the developmental process itself.

Linguistics and Speech Processing
CONSTITUENCY AND RECURSION IN LANGUAGE
CONVOLUTIONAL NETWORKS FOR IMAGES, SPEECH, AND TIME
SERIES
HIDDEN MARKOV MODELS
IMAGING THE GRAMMATICAL BRAIN
LANGUAGE ACQUISITION
LANGUAGE EVOLUTION AND CHANGE
LANGUAGE EVOLUTION: THE MIRROR SYSTEM HYPOTHESIS
LANGUAGE PROCESSING
MOTOR THEORIES OF PERCEPTION
NEUROLINGUISTICS
OPTIMALITY THEORY IN LINGUISTICS
PAST TENSE LEARNING
READING
SPEECH PROCESSING: PSYCHOLINGUISTICS
SPEECH PRODUCTION
SPEECH RECOGNITION TECHNOLOGY
The traditional grounding of linguistics is in grammar, a systematic
set of rules for structuring the sentences of a particular language.

Much modern work in linguistics has been dominated by the ideas
of Noam Chomsky, who placed the notion of grammar in a mathematical framework. His ideas have gone through successive stages
in which the formulation of grammars has changed radically. However, two themes have remained stable in the “generative linguistics” that has grown from his work:
• There is a universal grammar which defines what makes a language human, and each human language has a grammar that is
simply a parametric variation of the universal grammar.
• Language is too complicated for a child to learn from scratch;
instead a child has universal grammar as an innate mental capacity. When the child hears example sentences of a language,
they set parameters in the universal grammar so that the child
can then acquire the grammar of the particular language.
Connectionist linguistics attacks this reasoning on two fronts:
• It says that language processing is better understood in terms of
connectionist processing, which, as a performance model (i.e., a
model of behavior, as distinct from a competence model, which
gives a static representation of a body of knowledge), can give
an account of errors as well as regularities in language use.
• It notes that connectionism has powerful learning tools that
Chomsky has chosen to ignore. With those tools, connectionism
can model how children could acquire language on the basis of
far less specific mental structures than those posited in universal
grammar.
LANGUAGE PROCESSING reviews many application of connectionist modeling. Despite the insights gained into syntactic structure across languages, the formal study of language has revealed
relatively little about learning and development. Thus, as we shall
see later in this road map, the connectionist program for understanding language has concentrated on the process of change, exploring topics such as language development, language breakdown,
the dynamics of representation in complex systems which themselves may be receiving changing input, and even the evolution of
language. The article briefly reviews models of lexical processing
(reading single words, recognizing spoken words, and word production) as well as higher-level processing. It concludes that there
has been important progress in many areas of connectionist-based
research into language processing, and this modeling influences
both psychological and neuropsychological experimentation and
observation. However, it concedes that the major debates on topdown feedback, on the capacity of connectionist models to capture
the productivity and systematicity of human language, and on the
degree of modularity in language processing remain to be settled.
CONSTITUENCY AND RECURSION IN LANGUAGE then provides
more detail on connectionist approaches to syntax. Words group
together to form coherent building blocks, constituents, within a
sentence, so that “The girl liked a boy” decomposes into “the girl”
and “liked a boy,” forming a subject noun phrase (NP) and a verb
phrase (VP), respectively. In linguistics, grammar rules such as
Sentence S r NP VP determine how constituents can be put together to form sentences. To capture the full generativity of human
language, recursion needs to be introduced into the grammar. For
example, if we add the rules NP r (det) N(PP) (noun with optional
determiner and prepositional phrase) and PP r Preposition NP,
then the rules are recursive, because in this case, NP can invoke
rules that eventually call for another instance of NP. This article
discusses how constituency and recursion may fit into a connectionist framework, and the possible implications this may have for
linguistics and psycholinguistics.
LANGUAGE ACQUISITION presents models used by developmental connectionists to support the claim that rich linguistic represen-

II.4. Psychology, Linguistics, and Artificial Intelligence
tations can emerge from the interaction of a relatively simple learning device and a structured linguistic environment. The article
reviews connectionist models of lexical development, inflectional
morphology, and syntax acquisition, stressing that these models use
similar learning algorithms to solve diverse linguistic problems.
PAST TENSE LEARNING then presents issues in word morphology
as a backdrop for a detailed discussion of the prime debate between
a rule-based and a connectionist account of language processing,
over the forming of regular and irregular past tenses of verbs in
English. The dual mechanism model—use the general rule
“add-ed” unless an irregular past tense is found in a table of exceptions—was opposed by the view that all past tenses, even for
regular verbs, are formed by a connectionist network. The article
concludes that most researchers now agree that the mental processing of irregular inflections is not rule governed but rather works
much like a connectionist network. Certainly, rules provide an intuitively appealing explanation for regular behavior. Indeed, people
are clearly able to consciously identify regularities and describe
them with explicit rules that can then be deliberately followed, but
this does not imply that a neural encoding of these rules, rather
than a connectionist network which yields rule-like behavior, is the
better account of “mental reality.” The matter is subtle because the
brain is composed of neurons. Thus the issue is not “Does the
brain’s language processing use neural networks?” but whether or
not the activity of those networks is best described as explicitly
encoding a set of rules.
READING covers connectionist models of reading and associated
processes, including the reading disorder known as dyslexia. Where
a skilled reader can recognize many thousands of printed words,
each in a fraction of a second, with no noticeable effort, a dyslexic
child may need great effort to recognize a printed word as a particular word. Most connectionist networks for reading are models
of word recognition. However, word recognition is more than an
analytic letter-by-letter process that translates spelling into phonology, and so the synthetic-analytic debate provides the organizing theme for this article. The authors argue that, rather than see
modeling word recognition as a distinct, separable component of
reading, it may be better to investigate more integrative, nonlinear
iterative network models. However, SPEECH PROCESSING: PSYCHOLINGUISTICS reviews attempts to capture psycholinguistic data
using connectionist models, with the primary focus on speech segmentation and word recognition. This article analyzes how far the
problem of segmenting speech into words occurs independently of
word recognition; considers the interplay of connectionist models
of word recognition with empirical research and theory; and assesses the gap that remains between psycholinguistic studies of
speech processing and modeling of the human brain. Although data
from neuropsychology and functional imaging are becoming increasingly important (see IMAGING THE GRAMMATICAL BRAIN and
NEUROLINGUISTICS), the main empirical constraints on psycholinguistic models are derived from laboratory studies of human language processing that are unrelated to neural data. The article suggests that connectionist modeling helps bridge the gulf between
psycholinguistics and neuroscience by employing computational
models that embody at least some of the computational principles
of the brain.
IMAGING THE GRAMMATICAL BRAIN notes that there is little
agreement on the best way to analyze language. Contrary to the
connectionist approach (see, e.g., PAST TENSE LEARNING), the author sees inventories of combinatorial rules, and stores of complex
objects of several types over which these rules operate, as being at
the core of language. The “language faculty,” in this view, inheres
in a cerebrally represented knowledge base (rule system) and in
algorithms that instantiate it. It is divided into levels for the identification and segmentation of speech sounds (universal phonetics),
a system that enables the concatenation of phonetic units into se-

43

quences (phonology), then into words (morphology, where word
structure is computed), sentences (syntax), and meaning (lexical
and compositional semantics). The article reviews results emanating from brain imaging that support the neural reality of linguistic
rules as a constitutive element of the human language faculty. The
focus is on linguistic combinations at the sentence level, but an
analysis of cerebral representation of phonological units and of
word meaning in its isolated and compositional aspects is provided
as background. The study of brain mechanisms supporting language is further advanced in NEUROLINGUISTICS. Neurolinguistics
began as the study of the language deficits occurring after brain
injuries and is rooted in the conceptual model of Broca’s aphasia,
Wernicke’s aphasia, and other aphasic syndromes established over
a hundred years ago. However, thanks to recent research, critical
details are now seen differently, and finer details have been added.
Speech and language are now recognized as the products of interacting dynamic systems, with major implications for modeling normal and abnormal performance and for understanding their neural
substrates. The article analyzes between-stage information flow,
dynamics of within-stage processing, unitary representations and
activation, and processing by constraint satisfaction. How the cognitive elements (nodes) of psychological theorizing correspond to
actual neuronal activity is not known for certain. However, the
article suggests that the attractor states that can occur in recurrent
networks are viable candidates for behaving as nodes. Indeed,
many modeling efforts in neurolinguistics have been concerned
with the consequences of relatively large-scale assumptions about
stages and connections (see “Lesioned Networks as Models of Neuropsychological Deficits”).
On the output side, SPEECH PRODUCTION focuses on work in
motor control, dynamical systems and neural networks, and linguistics that is critical to understanding the functional architecture
and characteristics of the speech production system. The central
point is that spoken word forms are not unstructured wholes but
rather are composed from a limited inventory of phonological
units that have no independent meaning but that can be (relatively
freely) combined and organized in the construction of word forms.
The production of speech by the lips, tongue, vocal folds, velum,
and respiratory system can thus be understood as arising from
choreographed linguistic action units. However, when phonological units are made manifest in word and sentence production,
their spatiotemporal realization by the articulatory system, and
consequent acoustic character presented to the auditory system, is
highly variable and context dependent. The speech production system is sometimes viewed as having two components, one (traditionally referred to as phonology) concerned with categorical and
linguistically contrastive information, and the other concerned
with gradient, noncontrastive information (traditionally referred to
as phonetics). However, current work in connectionist and dynamical systems models blurs this dichotomy. MOTOR THEORIES OF
PERCEPTION reviews reasons why speech scientists have doubted
the claim that the speech motor system participates in speech perception and then argues against such doubts, showing that the
theory accrues credibility when it is set in the larger context of
investigations of perception, action, and their coupling. The mirror
neurons in primates (see LANGUAGE EVOLUTION: THE MIRROR
SYSTEM HYPOTHESIS) are seen as providing an existence proof of
neuronal perceptuomotor couplings. The article further argues
that, although the motor theory of speech perception was motivated by requirements of speaking and listening, real-world functional perception-action coupling is central to the “design” of animals more generally.
We have already contrasted connectionism with rule-based
frameworks that account for linguistic patterns through the sequential application of transformations to lexical entries. OPTIMALITY THEORY IN LINGUISTICS introduces optimality theory (OT) as

44

Part II: Road Maps

a framework for linguistic analysis that has largely supplanted rulebased frameworks within phonology; it has also been applied to
syntax and semantics, though not as widely. Generation of utterances in OT involves two functions, Gen and Eval. Gen takes an
input and returns a (possibly infinite) set of output candidates.
Some candidates might be identical to the input, others modified
somewhat, others unrecognizable. Eval chooses the candidate that
best satisfies a set of ranked constraints; this optimal candidate
becomes the output. The constraints can conflict, so the constraints’
ranking, which differs from language to language, determines the
outcome. One language might eliminate consonant clusters by deleting consonants; another might retain all input consonants. OT
was partly inspired by neural networks, employing as it does the
ideas of optimization, parallel evaluation, competition, and soft,
conflicting constraints. OT can be implemented in a neural network
with constraints that are implemented as connection weights. The
network implements a Lyapunov function that maximizes “harmony” (ijaiwijaj: the sum, for all pairs i, j of neurons, of the product of the neurons’ activations and their connection weight). Hierarchically structured representations (e.g., consonants and vowels
grouped into syllables) can be represented as matrices of neurons,
where each matrix is the tensor product of a vector for a linguistic
unit and a vector for its position in the hierarchy.
An approach to language that emphasizes the learning processes
of each new speaker rather than the existence of a set of immutable
rules shared by all humans seems well equipped to approach the
issue of how a language changes from generation to generation.
Computational modeling has been used to test competing theories
about specific aspects of language evolution under controlled circumstances. Connectionist networks have been used as simulated
agents to study how social transmission via learning may give rise
to the evolution of structured communication systems. In other
cases, properties of neural network learning are enlisted to help
illuminate the constraints and processes that may have been involved in the evolution of language. LANGUAGE EVOLUTION AND
CHANGE surveys this connectionist research, starting from the
emergence of early syntax and continuing on to the role of social
interaction and constraints on network learning in subsequent evolution of language. It also discusses linguistic change within existing languages, showing how the inherent generalization ability of
neural networks makes certain errors in language transmission from
one generation to the next more likely than others. (However, such
models say more about the simplification of grammars than about
how language complexity arises in the first place.) Where this article stresses computational efficacy of various models proposed
for the emergence of features characteristic of current human languages, LANGUAGE EVOLUTION: THE MIRROR SYSTEM HYPOTHESIS focuses on brain mechanisms shared by humans with other
primates, and seeks to explain how these generic mechanisms
might have become specialized during hominid evolution to support language. It is argued that imitation and pantomime provide a
crucial bridging capability between general primate capabilities for
action recognition and the language readiness of the human brain.
At present, the state of play may be summarized as follows:
generative linguistics has shown how to provide grammatical rules
that explain many subtle sentence constructions of English and
many other languages, revealing commonalities and differences between languages, with the differences in some cases being reduced
to very elegant and compact formulations in terms of general rules
with parametric variations. However, in offering the notion of universal grammar as the substrate for language acquisition, generative
linguistics ignores issues of learning that must, in any case, be faced
in explaining how children acquire the large and idiosyncratic vocabulary of their native tongue. Connectionist linguistics, on the
other hand, has made great strides in bringing learning to the center,
not only showing how specific language skills (e.g., use of the past

tense) may be acquired, but also providing insight into psycholinguistics, the study of language behavior. However, connectionist
linguistics still faces two major hurdles: it lacks the systematic
overview of language provided by generative linguistics, and little
progress has been made in developing a neurolinguistic theory of
the contributions of specific brain regions to language capabilities.
It is one thing to train an ANN to yield a convincing model of
performance on the past tense; it is quite another to offer an account
of how this skill interfaces with all the other aspects of language,
and what neural substrates are necessary for their acquisition by
the human child.
The remaining articles look at speech processing from a technological perspective rather than in relation to human psycholinguistic data. SPEECH RECOGNITION TECHNOLOGY introduces the
way computer systems that transcribe speech waveforms into
words rely on digital signal processing and statistical modeling
methods to analyze and model the speech signal. Although commercial technology is typically not based on connectionist methods,
neural network processing is commonly seen as a promising alternative to some of the current algorithms, and the article focuses on
speech recognizers that process large-vocabulary continuous
speech and that use multilayer feedforward neural networks. Traditional speech recognition systems follow a hierarchical architecture. A grammar specifies the sentences allowed by the application.
(Alternatively, for very large vocabulary systems, a statistical language model may be used to define the probabilities of various
word sequences in the domain of application.) Each word allowed
by the grammar is listed in a dictionary that specifies its possible
pronunciations in terms of sequences of phonemes which are further decomposed into smaller units whose acoustic realizations are
represented by statistical acoustic models. When a speech waveform is input to a recognizer, it is first processed by a front-end
unit that extracts a sequence of observations, or “features,” from
the raw signal. This sequence of observations is then decoded into
the sequence of speech units whose acoustic models best fit the
observations and that respect the constraints imposed by the dictionary and language model. Hidden Markov models (HMMs) have
been an essential part of the toolkit for continuous speech recognition, as well as other complex temporal pattern recognition problems such as cursive (handwritten) text recognition, time-series prediction, and biological sequence analysis. HIDDEN MARKOV
MODELS describes the use of deterministic and stochastic finite
state automata for sequence processing, with special attention to
HMMs as tools for the processing of complex piecewise stationary
sequences. It also describes a few applications of ANNs to further
improve these methods. HMMs allow complex learning problems
to be solved by assuming that the sequential pattern can be decomposed into piecewise stationary segments, with each stationary segment parameterized in terms of a stochastic function. The HMM is
called “hidden” because there is an underlying stochastic process
(i.e., the sequence of states) that is not directly observable but that
nonetheless affects the observed sequence of events. CONVOLUTIONAL NETWORKS FOR IMAGES, SPEECH, AND TIME SERIES shows
how shift invariance is obtained in convolutional networks by forcing the replication of weight configurations across space. This takes
the topology of the input into account, enabling such networks to
force the extraction of local features by restricting the receptive
fields of hidden units to be local, and enforcing a built-in invariance
with respect to translations, or local distortions of the inputs.

Artificial Intelligence
ARTIFICIAL INTELLIGENCE AND NEURAL NETWORKS
BAYESIAN NETWORKS
COMPETITIVE QUEUING FOR PLANNING AND SERIAL
PERFORMANCE

II.4. Psychology, Linguistics, and Artificial Intelligence
COMPOSITIONALITY IN NEURAL SYSTEMS
CONNECTIONIST AND SYMBOLIC REPRESENTATIONS
DECISION SUPPORT SYSTEMS AND EXPERT SYSTEMS
DYNAMIC LINK ARCHITECTURE
GRAPHICAL MODELS: PARAMETER LEARNING
GRAPHICAL MODELS: PROBABILISTIC INFERENCE
GRAPHICAL MODELS: STRUCTURE LEARNING
HYBRID CONNECTIONIST/SYMBOLIC SYSTEMS
MEMORY-BASED REASONING
MULTIAGENT SYSTEMS
SCHEMA THEORY
SEMANTIC NETWORKS
STRUCTURED CONNECTIONIST MODELS
SYSTEMATICITY OF GENERALIZATIONS IN CONNECTIONIST
NETWORKS
In the 1950s, the precursors of today’s fields of artificial intelligence and neural networks were still subsumed under the general
heading of cybernetics. Much of the work in the 1960s sought to
distance artificial intelligence (AI) from its cybernetic roots, emphasizing models of, e.g., logical inference, game playing, and
problem solving that were based on explicit symbolic representations manipulated by serial computer programs. However, work in
computer vision and in robotics (discussed in the road maps Vision
and Robotics and Control Theory, respectively) showed that this
distinction was never entirely convincing, since these were areas
of AI that made use of parallel computation and numerical transformations. For a while, a case could be made that the use of parallelism might be appropriate for peripheral sensing and motor control but not for the “central” processes involved in “real”
intelligence. However, work from at least the mid-1970s onward
has made this fallback position untenable. For example, in the
HEARSAY system, speech understanding was achieved not by serial manipulation of symbolic structures but by the action (implicitly distributed, though in the 1970s still implemented on a serial
computer) of knowledge sources (what we would now call
“agents”) to update numerical confidence levels of multiple hypotheses distributed across a set of “levels” in a data structure
known as a blackboard. MULTIAGENT SYSTEMS introduces the
methodology that has grown out of such beginnings. What constitutes an “individual” can be highly subjective: an individual to one
researcher can, to another, be a complex distributed system comprised of finer-grained agents. Research in brain theory has dealt
with different levels, from neurons to brain regions to humans
whereas AI work in multi-agent systems has focused on coarsegrained levels of individuality and interaction, where the goal is to
draw upon sociological, political, and economic insights. The article is designed to survey enough of this work on multi-agent
systems to foster comparisons between the ANN, brain theory, and
multi-agent approaches. A crucial notion is that agents either have
or learn models of the agents with which they interact. These models allow agents to avoid dealing with malicious or broken agents.
Agents may even build nested models of the other agents that include an agent’s models of other agents, and so on. By using their
models of each other, the agents loosely organize themselves into
self-reinforcing communities of trust, avoiding unproductive future
interactions with other agents. In another branch of AI, work on
expert systems—information systems that represent expert knowledge for a particular problem area as a set of rules, and that perform
inferences when new data are entered—provided an important application success in which numerical confidence values played a
role, but with the emphasis still on manipulation of hypotheses
through the serial application of explicit rules. As shown in DECISION SUPPORT SYSTEMS AND EXPERT SYSTEMS, we now see
many cases in which the application of separate rules is replaced
by transformations effected in parallel by (trainable) neural net-

45

works. A decision system is either a decision support system or an
expert system in the classic AI sense. The article reviews results
on connectionist-based decision systems. In particular, trainable
knowledge-based neural networks can be used to accumulate both
knowledge (rules) and data, building adaptive decision systems
with incremental, on-line learning.
As the general overview article ARTIFICIAL INTELLIGENCE AND
NEURAL NETWORKS makes clear, there are many problems for
which the (not necessarily serial) manipulation of symbolic structures can still outperform connectionist approaches, at least with
today’s software running on today’s hardware. Nonetheless, if we
define AI by the range of problems it is to solve—or the “packets
of intelligence” it is to implement—then it is no longer useful to
define it in opposition to connectionism. In general, the technologist facing a specific problem should choose between, or should
combine, connectionist and symbolic approaches on the basis of
efficacy, not ideology. On occasion, for rhetorical purposes, authors
will use the term AI for a serial symbolic methodology distinct
from connectionism. However, we will generally use it in an extended sense of a technology that seeks to realize aspects of intelligence in machines by whatever methods work best. The term
symbolic AI will then be used for the “classical” approach. The
article examines the relative merits of symbolic AI systems and
neural networks, and ways of attempting to bridge between the two.
In brain theory, everything, whether symbolic or not, is, in the final
analysis, implemented in a neural network. But even here, an analysis of the brain will often best be conducted in terms of interacting
subsystems that are not all fully explicated in neural network terms.
SCHEMA THEORY complements neuroscience’s well-established
terminology for levels of structural analysis (brain region, neuron,
synapse) with a framework for analysis of behavior with no necessary commitment to hypotheses on the localization of each
schema (unit of functional analysis), but which can be linked to a
structural analysis whenever appropriate. The article focuses on
two issues: structuring perceptual and motor schemas to provide
an action-oriented account of behavior and cognition (as relevant
to the roboticist as the ethologist), and how schemas describing
animal behavior may be mapped to interacting regions of the brain.
Schema-based modeling becomes part of neuroscience when constrained by data provided by, e.g., human brain mapping, studies
of the effects of brain lesions, or neurophysiology. The resulting
model may constitute an adequate explanation in itself or may provide the framework for modeling at the level of neural networks
or below. Such a neural schema theory provides a functional/structural decomposition, in strong contrast to models that employ learning rules to train a single, otherwise undifferentiated, neural network to respond as specified by some training set. HYBRID
CONNECTIONIST/SYMBOLIC SYSTEMS reviews work on hybrid systems that integrate neural (ANN) and symbolic processes. Cognitive processes are not homogeneous, and so some are best captured
by symbolic models and others by connectionist models. Correspondingly, from a technological viewpoint, AI systems for practical applications can benefit greatly from a proper combination of
different techniques combining, e.g., symbolic models (for capturing explicit knowledge) and connectionist models (for capturing
implicit knowledge).
Use of the term systematicity in relation to connectionist networks originated with Fodor and Pylyshyn’s critique of connectionist architecture. They claimed that human cognitive abilities are
systematically related in a way that does not hold in connectionist
networks, unlike formal systems akin to propositional logic. SYSTEMATICITY OF GENERALIZATIONS IN CONNECTIONIST NETWORKS
starts by noting that this critique made no reference to learningbased generalization, and then proceeds to examine claims and
counterclaims concerning the claim that learning in connectionist
architectures can engender systematicity. Special attention is paid

46

Part II: Road Maps

to studies based on simple recurrent networks (SRNs) and recursive
auto-associative memory (RAAM). The article suggests that, for
now, evidence for systematicity in such simple networks is rather
limited. Perhaps this is not so surprising, given that there is little
evidence of systematicity in most animals, and animal brains are
vastly more complex than SRNs or RAAMs. Compare “Language
Evolution: The Mirror System Hypothesis” for a discussion of how
evolution may have shaped the human brain to extend capabilities
shared with other species to yield novel human cognitive abilities.
The notion of representation plays a central role in AI. As discussed in SEMANTIC NETWORKS, one classic form of representation
in AI is the semantic network, in which nodes represent concepts
and links represent relations between them. Semantic networks
were originally developed for couching “semantic” information,
either in the psychologist’s sense of static information about concepts or in the semanticist’s sense of the meanings of natural language sentences. However, they are also used as a general knowledge representation tool. The more elaborate types of semantic
networks are similar in their representational abilities to sophisticated forms of symbolic logic. The article discusses various ways
of implementing or emulating semantic networks in neural networks, and of forming hybrid semantic network-neural network
systems. STRUCTURED CONNECTIONIST MODELS emphasizes those
neural networks in which the translation from symbolic to neural
is fairly direct: nodes become “neurons,” but now processing is
done by neural interactions rather than by an “inference engine”
acting on a passive representation. At the other extreme, certain
neural networks (connectionist, rather than biological) may transform input “questions” to output “answers” via the distributed activity of neurons whose firing conditions have no direct relationship
to the concepts that might normally arise in a logical analysis of
the problem (cf. “Past Tense Learning”). In the fully distributed
version of the latter approach, each “item” (concept or mental object) is represented as a pattern of activity distributed over a common pool of nodes. However, if “John” and “Mary,” for example,
are represented as patterns of activity over the entire network such
that each node in the network has a specific value in the patterns
for “John” and “Mary,” respectively, then how can the network
represent “John” and “Mary” at the same time? To address such
problems, the structured approach often employs small clusters of
nodes that act as “focal” nodes for concepts and provide access to
more elaborate structures that make up the detailed encoding of
concepts (cf. “Localized Versus Distributed Representations”). The
discussion of these varying styles of representation is continued in
CONNECTIONIST AND SYMBOLIC REPRESENTATIONS. In symbolic
representations, the heart of mathematics and many models of cognition, symbols are meaningless entities to which arbitrary significance may be assigned. Composing ordered tuples from symbols
and other tuples allows us to create an infinitude of complex structures from a finite set of tokens and combination rules. Inference
in the symbolic framework is founded on structural comparison
and rule-governed manipulation of these objects. However, AI
makes extensive use of nondeductive reasoning methods. Symbolists have moved to more complex formalizations of cognitive processes, using heuristic and unsound inference rules. Connectionists
explore a radical alternative: that cognitive processes are mere epiphenomena of a completely different type of underlying system,
whose operations can never be adequately formalized in symbolic
language. The article examines representation and processing issues in the connectionist move from classical discrete, set-theoretic
semantics to a continuous, statistical, vector-based semantics.
In symbolic AI, two concepts can be linked by providing a
pointer between them. In a neural net, the problem of “binding”
the two patterns of activity that represent the concepts is a more
subtle one, and several models address the use of rapidly changing
synaptic strengths to provide temporary “assemblages” of currently

related data. This theme is developed not only in STRUCTURED
CONNECTIONIST MODELS, but also in the articles COMPOSITIONALITY IN NEURAL SYSTEMS (how can inferences about a structure
be based on the way it is composed of various elements?), and
“Object Structure, Visual Processing” (combining visual elements
of an object into a recognizable whole). DYNAMIC LINK ARCHITECTURE, the basic methodology, views the brain’s data structure
as a graph composed of nodes connected by links. Both units and
links bear activity variables changing on the rapid time scale of
fractions of a second. The nodes play the role of symbolic elements.
The intensity of activity measures the degree to which a node is
active in a given time interval, signifying the degree to which the
meaning of the node is alive in the mind of the animal, while
correlations of activity between nodes quantify the degree to which
the signal of one node is related to that of others. The strength of
links can change on two time scales, represented by two variables
called temporary weight and permanent weight. The permanent
weight corresponds to the usual synaptic weight, can change on the
slow time scale of learning, and represents permanent memory. The
temporary weight can change on the same time scale as the node
activity—it is what makes the link dynamic. On this view, dynamic
links constitute the glue by which higher data structures are built
up from more elementary ones.
Complementing the theme of representation in symbolic AI has
been that of planning, going from (representations of) the current
state and some desired state to a sequence of operations that will
transform the former to the latter. COMPETITIVE QUEUING FOR
PLANNING AND SERIAL PERFORMANCE presents neural network
studies based on two assumptions: that more than one plan representation can be simultaneously active in a planning layer, and that
which plan to enact next is chosen as the most active plan representation by a competition in a second neural layer. Once a plan
wins the competition and is used to initiate a response, its representation is deleted from the field of competitors in the planning
layer, and the competition is re-run. This iteration allows the twolayer network to transform an initial activity distribution across
plan representations into a serial performance. Such models provide a very different basis for control of serial behavior than that
given by recurrent neural networks. The article suggests that such
a system was probably an ancient invention in the evolution of
animals yet may still serve as a viable core for the highest levels
of planning and skilled sequencing exhibited by humans.
The final articles in this road map are not on neural nets per se,
but instead provide related methods that add to the array of techniques extending AI beyond the serial, rule-based approach.
BAYESIAN NETWORKS provides an explicit method for following
chains of probabilistic inference such as those appropriate to expert
systems, extending Bayes’s rule for updating probabilities in the
light of new evidence. The nodes in a Bayesian network represent
propositional variables of interest and the links represent informational or causal dependencies among the variables. The dependencies are quantified by conditional probabilities for each node
given its parents in the network. The network supports the computation of the probabilities of any subset of variables given evidence about any other subset, and the reasoning processes can operate on Bayesian networks by propagating information in any
direction. GRAPHICAL MODELS: PROBABILISTIC INFERENCE introduces the graphical models framework, which has made it possible
to understand the relationships among a wide variety of networkbased approaches to computation, and in particular to understand
many neural network algorithms and architectures as instances of
a broader probabilistic methodology. Graphical models use graphs
to represent and manipulate joint probability distributions. The
graph underlying a graphical model may be directed, in which case
the model is often referred to as a belief network or a Bayesian
network, or the graph may be undirected, in which case the model

II.5. Biological Neurons and Networks
is generally referred to as a Markov random field. The articles
GRAPHICAL MODELS: STRUCTURE LEARNING and GRAPHICAL
MODELS: PARAMETER LEARNING present learning algorithms that
build on these inference algorithms and allow parameters and structures to be estimated from data. (A fuller précis of the three articles
on graphical models can be found in the road map Learning in
Artificial Networks.) Finally, MEMORY-BASED REASONING applies massively parallel computing to answer questions about a new
situation by searching for data on the most similar stored instances.
Memory-based reasoning (MBR) refers to a family of nearestneighbor-like methods for making decisions or classifications.
Where nearest-neighbor methods generally use a simple overlap
distance metric, MBR uses variants of the value distance metric.

47

MBR and neural nets form decision surfaces differently, and so
will perform differently. MBR can become arbitrarily accurate if
large numbers of cases are available, and if these cases are well
behaved and properly categorized, whereas neural nets cannot respond well to isolated cases but tend to be good at smooth extrapolation. For each article reviewed in this paragraph, the reader may
ponder whether these methods are alternatives to connectionist AI,
or whether they can contribute to the emergence of a technologically efficacious hybrid. As stated before, where brain theory seeks
to know “how the brain does it,” AI must weigh the value of ANNs
as a powerful technology for parallel, adaptive computation against
that of other technologies on the basis of efficacy in solving practical problems on available hardware.

II.5. Biological Neurons and Networks
Biological Neurons and Synapses
ACTIVITY-DEPENDENT REGULATION OF NEURONAL
CONDUCTANCES
AXONAL MODELING
BIOPHYSICAL MECHANISMS IN NEURONAL MODELING
BIOPHYSICAL MOSAIC OF THE NEURON
DENDRITIC PROCESSING
DENDRITIC SPINES
DIFFUSION MODELS OF NEURON ACTIVITY
ION CHANNELS: KEYS TO NEURONAL SPECIALIZATION
NEOCORTEX: BASIC NEURON TYPES
NEOCORTEX: CHEMICAL AND ELECTRICAL SYNAPSES
OSCILLATORY AND BURSTING PROPERTIES OF NEURONS
PERSPECTIVE ON NEURON MODEL COMPLEXITY
SINGLE-CELL MODELS
SYNAPTIC INTERACTIONS
SYNAPTIC NOISE AND CHAOS IN VERTEBRATE NEURONS
SYNAPTIC TRANSMISSION
TEMPORAL DYNAMICS OF BIOLOGICAL SYNAPSES
Nearly all the articles in the road maps Psychology, Linguistics
and Speech Processing, and Artificial Intelligence discuss networks made of very simple neurons describable by a single internal
variable, either binary or real-valued (the “membrane potential”)
and that communicate with other neurons by a simple (generally
nonlinear) function of that variable, sometimes referred to as the
firing rate. Incoming signals are usually summed linearly via “synaptic weights,” and these weights in turn may be adjusted by simple
learning rules, such as the Hebbian rule, the perceptron rule, or a
reinforcement learning rule. Such simplifications remain valuable
both for technological application of ANNs and for approximate
models of large biological networks. Nonetheless, biological neurons are vastly more complex than these single-compartment models suggest. An appreciation of this complexity is necessary for the
computational neuroscientist wishing to address the increasingly
detailed database of experimental neuroscience. It is also important
for the technologist looking ahead to the incorporation of new capabilities into the next generation of ANNs.
The neocortex is functionally parcellated into vertical columns
(0.5 mm in diameter) traversing all six layers. These columns
have no obvious anatomical boundaries, and the topographic mapping of afferent and efferent pathways probably determines their
locations and dimensions as well as their functions. NEOCORTEX:
BASIC NEURON TYPES shows that these apparently stereotypical
microcircuits are composed of a daunting variety of precisely and

intricately interconnected neurons and argues that this neuronal diversification may provide a foundation for maximizing the computational abilities of the neocortex. All anatomical cell types can
display multiple discharge patterns and molecular expression profiles. Different cell types are synaptically interconnected according
to complex organizational principles to form intricate stereotypical
microcircuits. The article challenges neural network modelers to
incorporate and account for this cellular diversity and the role of
different cells in the computational capability of cortical microcircuits. NEOCORTEX: CHEMICAL AND ELECTRICAL SYNAPSES summarizes the diverse functional properties of synapses in neocortex.
These synapses tend to be small, but their structure and biochemistry are complex. Both chemical and electrical synapses exist in
neocortex. Chemical synapses are the “usual synapses” of neural
network models, and are far more abundant. They use a chemical
neurotransmitter that is packaged presynaptically into vesicles, released in quantized (vesicle-multiple) amounts, and binds to postsynaptic receptors that either open an ion channel directly (voltagedependent ion channels) or modulate the channel through an
intracellular molecule that links the activated receptor to the opening or closing of the channel. The latter molecule is called a “second messenger,” to contrast it with the case in which the transmitter
itself provides a “primary message” that acts directly on the channel, in this case called “ligand-gated.” Second-messenger-based
synaptic interaction occurs on a slower time scale than ligand-gated
interaction and is called neuromodulation, since it may modulate
the behavior of the postsynaptic neuron over a time scale of seconds
or minutes rather than milliseconds (cf. “Neuromodulation in Invertebrate Nervous Systems” and “Neuromodulation in Mammalian Nervous Systems”). The essential element of an electrical synapse is a protein called a connexin; 12 connexins form a single
intercytoplasmic ion channel, and a cluster of such channels constitutes a gap junction. Electrical synapses provide a direct pathway
that allows ionic current or small organic molecules to flow from
the cytoplasm of one cell to that of another. Short-term dynamics
allow synapses to serve as temporal filters of neural activity. Longterm synaptic plasticity provides specific, localized substrates for
various forms of memory. Modulation of synaptic function by neurotransmitters (see “Neuromodulation in Mammalian Nervous Systems”) provides a mechanism for globally altering the properties
of a neural circuit during changes of behavioral state. Each of these
functions has diverse forms that vary between synapses, depending
on their site within the cortical circuit (and elsewhere in the brain).
PERSPECTIVE ON NEURON MODEL COMPLEXITY discusses the
wide range of model complexity, from very simple to rather complex neuron models. Which model to choose depends, in each case,

48

Part II: Road Maps

on the context, such as how much information we already have
about the neurons under consideration and what questions we wish
to answer. The use of more realistic neuron models when seeking
functional insights into biological nervous systems does not mean
choosing the most complex model, at least in the sense of including
all known anatomical and physiological details. Rather, the key is
to preserve the most significant distinctions between regions (soma,
proximal dendritic, distal dendritic, etc.), using “compartmental
modeling,” whereby one compartment represents each functionally
distinct region. SINGLE-CELL MODELS starts by reviewing the “simple” models of Part I (the McCulloch-Pitts, perceptron, and Hopfield models) and the slightly more complex polynomial neuron. It
then turns to more realistic biophysical models, most of which are
explored in further detail in this road map. These include the Hodgkin-Huxley model of squid axon, integrate-and-fire models, modified single-point models, cable and compartmental models, and
models of synaptic conductances.
Before turning to a detailed analysis of mechanisms of neuronal
function, we first consider an article that offers a high-level view
of the neuron, but this time a stochastic one. Most nerve cells encode their output as a series of action potentials, or spikes, that
originate at or close to the cell body and propagate down the axon
at constant velocity and amplitude. DIFFUSION MODELS OF NEURON ACTIVITY studies the membrane potential of a single neuron
as engaged in a stochastic process that will eventually bring it to
the threshold for spike initiation. This leads to the first-passagetime problem, inferring the distribution of neuronal spiking based
on the “first passage” of the membrane potential from its resting
value to threshold. In addition to using stochastic differential equations, the article shows how the Wiener and Ornstein-Uhlenbeck
neuronal models can be obtained as the limit of a Markov process
with discrete state spaces. Besides these models, characterized by
additive noise terms appearing in the corresponding stochastic differential equations, the article also reviews diffusion models with
multiplicative noise, showing that these can be used not only for
the description of steady-state firing under constant stimulation, but
also for effects of periodic stimulation.
Now for the details of neuronal function. The ionic mechanisms
underlying the initiation and propagation of action potentials were
elucidated in the squid giant axon by a number of workers, most
notably Hodgkin and Huxley. Variations on the Hodgkin-Huxley
equation underlie the vast majority of contemporary biophysical
models. AXONAL MODELING describes this model and its assumptions, introduces the two classes of axons (myelinated and nonmyelinated) found in most animals, and concludes by briefly commenting on the possible functions of axonal branching in
information processing. The Hodgkin-Huxley equation was brilliantly inferred from detailed experiments on conduction of nerve
impulses. Much research since then has revealed that the basis for
these equations is provided by “channels,” structures built from a
few macromolecules and embedded in the neuron which, in a
voltage-dependent way, can selectively allow different ions to pass
through the cell membrane to change the neuron’s membrane potential. Similarly, channels (also known in this case as receptors)
in the postsynaptic membrane can respond to neurotransmitters,
chemicals released from the presynaptic membrane, to change the
neuron’s local membrane potential in response to presynaptic input.
These changes, local to the synapse, must propagate down the dendrites and across the cell body to help determine whether or not
the axon will “pass threshold” and generate an action potential. ION
CHANNELS: KEYS TO NEURONAL SPECIALIZATION notes that channels not only produce action potentials but can set a particular firing
pattern, latency, rhythm, or oscillation for the firing of these spikes.
Each neuronal class is endowed with a different set of channels,
and the diversity of channels between different types of neurons
explains the functional classes of neurons found in the brain. Some

neurons fire spontaneously, some show adaptation, some fire in
bursts, and so on. Therefore, a channel-based cellular physiology
is relevant to questions about the role of different brain regions in
overall function.
Biophysically detailed compartmental models of single neurons
typically aim to quantitatively reproduce membrane voltages and
currents in response to some sort of “synaptic” input. We may think
of them as “Hodgkin-Huxley-Rall” models, based on the hypothesis of the neuron as a dynamical system of nonlinear membrane
channels distributed over an electrotonic cable skeleton. Such models can incorporate as much biophysical detail as desired (or practical), but in general, all include some explicit assortment of
voltage-dependent and transmitter-gated (synaptic) membrane
channels. BIOPHYSICAL MECHANISMS IN NEURONAL MODELING
first presents general issues regarding model formulations and data
interpretation. It then describes the modeling of various features of
Hodgkin-Huxley-Rall models, including Hodgkin-Huxley and
Markov kinetic descriptions of voltage- and second-messengerdependent ion channels as well as methods for describing intracellular calcium dynamics and the associated buffer systems and
membrane pumps. The models for each of these mechanisms are
at an intermediate level of biophysical detail, appropriate for describing macroscopic variables (e.g., membrane currents, ionic concentrations) on the scale of the entire cell or anatomical compartments thereof. Similar models of synaptic mechanisms are covered
in SYNAPTIC INTERACTIONS, which provides kinetic models of how
synaptic currents arise from ion channels whose opening and closing are controlled (gated) directly or indirectly by the release of
neurotransmitter. The article compares several models of synaptic
interaction, focusing on simple models based on the kinetics of
postsynaptic receptors, and shows how these models capture the
time courses of postsynaptic currents of several types of synaptic
responses, as well as synaptic summation, saturation, and
desensitization.
The membrane potential of central neurons undergoes synaptic
noise, fluctuations that depend on both the summed firing of action
potentials by neurons presynaptic to the investigated cell and the
spontaneous release of transmitter. SYNAPTIC NOISE AND CHAOS
IN VERTEBRATE NEURONS argues that, despite its random appearance, synaptic noise may be a true signal associated with neural
coding, possibly a chaotic one. In addition to reviewing tools for
detecting chaotic behavior, the article pays special attention to
Mauthner cells, a pair of identified neurons in the hindbrain of
teleost fishes. When the fish is subjected to an unexpected stimulus,
one of the cells triggers an escape reaction. Their excitability is
controlled by powerful inhibitory presynaptic interneurons that
continuously generate an intense synaptic noise. While it is still an
open question whether this synaptic noise exhibits deterministic
chaos or is truly random, it is worth stressing that the “noise” has
adaptive value for the fish: the variability along output pathways
introduces uncertainty in the expression of the reflex, and therefore
enhances the fish’s success in evading predators.
TEMPORAL DYNAMICS OF BIOLOGICAL SYNAPSES complements
the many studies of synaptic plasticity in the Handbook that focus
on long-term changes in synaptic strength by showing how synaptic
function can be profoundly influenced by activity over time scales
of milliseconds to seconds. Synapses that exhibit such short-term
plasticity are powerful computational elements that can have profound impact on cortical circuits (cf. “Dynamic Link Architecture”). Short-term plasticity includes both synaptic depression and
a number of components of short-term enhancement (facilitation,
augmentation, and posttetanic potentiation) acting over increasingly longer periods of time. Synaptic facilitation appears to result
from enhanced transmitter release due to elevated presynaptic calcium levels, while depression is believed to result, in part, from
depletion of a readily releasable pool of vesicles. Depression ap-

II.5. Biological Neurons and Networks
pears to be a particularly prominent feature of transmission at excitatory synapses onto pyramidal cells. In addition to having complex short-term dynamics, synapses are stochastic, and it is argued
that constructive roles for unreliable transmission become apparent
when short-term plasticity is considered in connection with stochastic transmission, with synapses acting as stochastic temporal
filters of their presynaptic spike trains. Indeed, SYNAPTIC TRANSMISSION is concerned with the uncertainties introduced by noise
and their relation to synaptic plasticity. The probability that a single
activated synapse will release neurotransmitter has a broad distribution, well fitted by a gamma function, with a mean near 0.3. The
dynamic regulation of synaptic strength depends on a complicated
set of mechanisms that record the history of synaptic use over many
time scales, and serve to filter the incoming spike train in a way
that reflects the past use of the synapse. The article provides equations which describe how synaptic use determines the number of
vesicles available for release, and for the release probability in turn.
OSCILLATORY AND BURSTING PROPERTIES OF NEURONS offers
a dynamic systems analysis of the linkage between a fascinating
variety of endogenous oscillations (neuronal rhythms) and appropriate sets of channels. However, membrane potential oscillations
with apparently similar characteristics can be generated by different
ionic mechanisms, and a given cell type may display several different firing patterns under different neuromodulatory conditions.
Here, membrane dynamics are described by coupled differential
equations, the behavior modes by attractors (cf. “Computing with
Attractors”), and the transitions between modes by bifurcations.
The rest state is represented by a time-independent steady state,
and repetitive firing is represented by a limit cycle. (“Silicon Neurons” shows how such differential equations can be directly
mapped into an electronic circuit built using analog VLSI, to allow
real-time exploration of the behavior of quite realistic neural
models.)
Roughly a dozen different types of ion channels contribute to
the membrane conductance of a typical neuron. ACTIVITYDEPENDENT REGULATION OF NEURONAL CONDUCTANCES takes as
its starting point the fact that the electrical characteristics of a neuron depend on the number of channels of each type active within
the membrane and on how these channels are distributed over the
surface of the cell. A complex array of biochemical processes controls the number and distribution of ion channels by constructing
and transporting channels, modulating their properties, and inserting them into and removing them from the neuron’s membrane.
The point to note here is that channels are small groupings of large
molecules, and they are assembled on the basis of genetic instructions in the cell nucleus. Thus, changing which genes are active
(i.e., regulating gene expression) can change the set of channels in
a cell, and thus the characteristics of the cell. In fact, electrical
activity in the cell can affect a range of processes, from activityinduced gene expression to activity-dependent modulation of assembled ion channels. Channel synthesis, insertion, and modulation are much slower than the usual voltage- and ligand-dependent
processes that open and close channels. Thus, consideration of
activity-dependent regulation of conductances introduces a dynamics acting on a new, slower time scale into neuronal modeling, a
feedback mechanism linking a neuron’s electrical characteristics to
its activity. A similar theme is developed in BIOPHYSICAL MOSAIC
OF THE NEURON, which is structured around the metaphor of the
mosaic neuron. A mosaic is a collection of discrete parts, each with
unique properties, that are fitted together in such a way that an
image emerges from the whole in a nonobvious way. Similarly, the
neuronal membrane is packed with a diversity of receptors and ion
channels and other proteins with a recognizable distribution. In
addition, the cytoplasm is not just water with ions, but a mosaic of
interacting molecular systems that can directly affect the functional
properties of membrane proteins. The argument is that, just as a

49

mosaic painting provokes perception of a complete image out of a
maze of individually diversified tiles, so a given neuron performs
a well-defined computational role that depends not only on the
network of cells in which it is embedded, but also to a large extent
on the dynamic distribution of macromolecules throughout the cell.
DENDRITIC PROCESSING focuses on dendrites as electrical inputoutput devices that operate on a time scale range of several to a
few hundred milliseconds. (See “Dendritic Learning” for modeling
of the plasticity of dendritic function and the assertion that the
concept of “overall connection strength between two neurons” is
ill-defined, since it is the distribution of synapses in relation to
dendritic geometry that proves crucial.) The input to a dendrite
consists of temporal patterns of synaptic inputs spatially distributed
over the dendritic surface, whereas the output is (except, for example, in the case of dendrodendritic interactions) an ionic current
delivered to the soma for transformation there, via a threshold
mechanism, to a train of action potentials at the axon. The article
discusses how the morphology, electrical properties, and synaptic
inputs of dendrites interact to perform their input-output operation.
It uses cable theory and compartmental modeling to model the
spread of electric current in dendritic trees. The variety of excitable
(voltage-gated) channels that are found in many types of dendrites
enrich the computational capabilities of neurons, with interaction
proceeding in both directions, away from and toward the soma.
Computer modeling methods for neurons offer numerical methods
for solving the equations describing branched cables. DENDRITIC
SPINES are short appendages found on the dendrites of many different cell types. They are composed of a bulbous “head” connected
to the dendrite by a thin “stem.” An excitatory synapse is usually
found on the spine head, and some spines also have a second,
usually inhibitory, synapse located on or near the spine stem. Models in which the spine is represented as a passive electrical circuit
show that the large resistance of a thin spine stem can attenuate a
synaptic input delivered to the spine head. Other models address
calcium diffusion and plasticity in spines. Current research focuses
on the hypothesis that the spine stem provides a diffusional resistance that allows calcium to become concentrated in the spine head
and calcium-dependent reactions to be localized to the synapse.
This could be very important for plasticity changes, such as those
that occur with long-term potentiation.

Neural Plasticity
AXONAL PATH FINDING
CEREBELLUM AND CONDITIONING
CEREBELLUM AND MOTOR CONTROL
CEREBELLUM: NEURAL PLASTICITY
CONDITIONING
DENDRITIC LEARNING
DEVELOPMENT OF RETINOTECTAL MAPS
DYNAMIC LINK ARCHITECTURE
HABITUATION
HEBBIAN LEARNING AND NEURONAL REGULATION
HEBBIAN SYNAPTIC PLASTICITY
INFORMATION THEORY AND VISUAL PLASTICITY
INVERTEBRATE MODELS OF LEARNING: APLYSIA AND
HERMISSENDA
NMDA RECEPTORS: SYNAPTIC, CELLULAR, AND NETWORK
MODELS
OCULAR DOMINANCE AND ORIENTATION COLUMNS
POST-HEBBIAN LEARNING ALGORITHMS
SHORT-TERM MEMORY
SOMATOTOPY: PLASTICITY OF SENSORY MAPS
TEMPORAL DYNAMICS OF BIOLOGICAL SYNAPSES
Most studies of learning in ANNs involve a variety of learning
rules, inspired in great part by the psychological hypotheses of

50

Part II: Road Maps

Hebb and Rosenblatt (cf. Section I.3) about ways in which synaptic
connections may change their strength as a result of experience. In
recent years, much progress has been made in tracing the processes
that underlie the plasticity of synapses of biological neurons. The
present road map samples this research together with related modeling. Although the emphasis will be on synaptic plasticity, several
articles stress the role of axonal growth in forming new connections, and the road map closes with an article suggesting that
changes in location of synapses may be just as important as changes
in synaptic strength.
Hebb’s idea was that a synapse (what we would now call a Hebbian synapse) strengthens when the presynaptic and postsynaptic
elements tend to be coactive. The plausibility of this hypothesis
has been enhanced by the neurophysiological discovery of a synaptic phenomenon in the hippocampus known as long-term potentiation (LTP), which is induced by a Hebbian mechanism. Hebb’s
postulate has received various modifications to address, e.g., the
saturation problem.
HEBBIAN SYNAPTIC PLASTICITY shows that a variety of experimental networks ranging from the abdominal ganglion in the invertebrate Aplysia to visual cortex and the CA1 region of hippocampus offer converging validation of Hebb’s postulate on
strengthening synapses by (more or less) coincident presynaptic
and postsynaptic activity. In these networks, similar algorithms of
potentiation can be implemented using different cascades of second
messengers triggered by activation of synaptic and/or voltagedependent conductances. Most cellular data supporting Hebb’s predictions have been derived from electrophysiological measurements of composite postsynaptic potentials or synaptic currents, or
of short-latency peaks in cross-correlograms, which cannot always
be interpreted simply at the synaptic level. The basic conclusion of
these experiments is that covariance between pre- and postsynaptic
activity upregulates and downregulates the “effective” connectivity
between pairs of functionally coupled cells. The article thus suggests that what changes according to a correlational rule is not so
much the efficacy of transmission at a given synapse, but rather a
more general coupling term mixing the influence of polysynaptic
excitatory and inhibitory circuits linking the two cells, modulated
by the diffuse network background activation. Replacing this composite interaction by a single coupling term defines an ideal Hebbian synapse.
The crucial role played in the CA1 form of LTP by channels
called NMDA receptors in the synapses is further explained in
NMDA RECEPTORS: SYNAPTIC, CELLULAR, AND NETWORK MODELS. NMDA receptors are subtypes of receptors for the excitatory
neurotransmitter glutamate and are involved in diverse physiological as well as pathological processes. They mediate a relatively
“slow” excitatory postsynaptic potential, and act as coincidence
detectors of presynaptic and postsynaptic activity. The interactions
between the slow NMDA-mediated and fast AMPA-mediated currents provide the basis for a range of dynamic properties that contribute to diverse neuronal processes. NMDA receptors have attracted much interest in neuroscience because of their role in
learning and memory. Their ability to act as coincidence detectors
make them an ideal molecular device for producing Hebbian synapses. The article reviews data related to the biological characteristics of NMDA receptors and models that have been used to describe their function in isolated membrane patches, in neurons, and
in complex circuits.
A classic problem with Hebb’s original rule is that it only
strengthens synapses. But this means that all synapses would eventually saturate, depriving the cell of its pattern separation ability.
A number of biologically inspired responses to this problem are
described in the next two articles. HEBBIAN LEARNING AND NEURONAL REGULATION stresses that, for both computational and biological reasons, Hebbian plasticity will involve many synapses of

the same neuron. Biologically, synaptic interactions are inevitable
as synapses compete for the finite resources of a single neuron.
Computationally, neuron-specific modifications of synaptic efficacies are required in order to obtain efficient learning, or to faithfully model biological systems. Hence neuronal regulation, a process modulating all synapses of a postsynaptic neuron, is a general
phenomenon that complements Hebbian learning. The article
shows that neuronal regulation may answer important questions,
such as: What bounds the positive feedback loop of Hebbian learning and guarantees some normalization of the synaptic efficacies
of a neuron? How can a neuron acquire specificity to particular
inputs without being prewired? How can memories be maintained
throughout life while synapses suffer degradation due to metabolic
turnover? In unsupervised learning, neuronal regulation allows for
competition between the various synapses on a neuron and leads
to normalization of their synaptic efficacies. In supervised learning,
neuronal regulation improves the capacity of associative memory
models and can be used to guarantee the maintenance of biological
memory systems. Our basic tour of Hebbian learning concludes
with POST-HEBBIAN LEARNING ALGORITHMS. This article starts by
observing that Hebb’s original postulate was a verbally described
phenomenological rule, without specification of detailed mechanisms. Subsequent work has shown the computational usefulness
of many variations of the original learning rule. This article presents background material on conditioning, neural development,
and physiologically realistic cellular-level learning phenomena as
a prelude to a review of several families of rules providing computational implementations of Hebbian-inspired rules.
CEREBELLUM AND MOTOR CONTROL reviews a number of models for cerebellar mechanisms underlying the learning of motor
skills. Cerebellum can be decomposed into cerebellar nuclei and a
cerebellar cortex. The only output cells of the cerebellar cortex are
the Purkinje cells, and their only effect is to provide varying levels
of inhibition on the cerebellar nuclei. Each Purkinje cell receives
two types of input: a single climbing fiber, and many tens of thousands of parallel fibers. The most influential model of cerebellar
cortex has been the Marr-Albus model of the formation of associative memories between particular patterns on parallel fiber inputs
and Purkinje cell outputs, with the climbing fiber acting as “training
signal.” Later models place more emphasis on the relation between
the cortex and nuclei, and on the way in which the subregions of
this coupled cerebellar system can adapt and coordinate the activity
of specific motor pattern generators. The plasticity of the cerebellum is approached from a different direction in CEREBELLUM AND
CONDITIONING. Many experiments indicate that the cerebellum is
involved in learning and performance of classically conditioned
reflexes; the present article reviews a number of models of the role
of cerebellum in rabbit eyelid conditioning. (A more general perspective on conditioning is given in CONDITIONING and described
more fully in the road map Psychology, which describes several
formal theories and neural network models for classical and operant
conditioning.) Inspired by the Marr-Albus hypothesis, neurophysiological research eventually showed that coincidence of climbing
fiber and parallel fiber activity on a Purkinje cell led to long-term
depression (LTD) of the synapse from parallel fiber to Purkinje
cell. CEREBELLUM: NEURAL PLASTICITY offers readers an exhaustive overview of the data on the neurochemical mechanisms underlying this form of plasticity. The authors conclude that the timing conditions for LTD induction may account for the temporal
specificity of cerebellar motor learning, and suggest that an important future development in the field will be to study developmental aspects of LTD in relation to acquisition of motor skills.
However, the article cites only one model of LTD. It is clear that
there are immense challenges to neural modelers in exploring the
implications of the plethora of neurochemical interactions swirling
about this single class of synaptic plasticity and, by implication,

II.5. Biological Neurons and Networks
the variety of different mechanisms expressed elsewhere in the nervous system.
There is now strong evidence for a process of short-term memory
(STM) involved in performing tasks requiring temporary storage
and manipulation of information to guide appropriate actions.
SHORT-TERM MEMORY addresses three issues: What are the different types of STM traces? How do intrinsic and synaptic mechanisms contribute to the formation of STM traces? How do STM
traces translate into long-term memory representation of temporal
sequences? The stress is on the computational mechanisms underlying these processes, with the suggestion that these mechanisms
may well underlie a wide variety of seemingly different biological
processes. The article examines both the short-term preservation of
patterns of neural firing in a circuit and ways in which short-term
maintained activity may be transferred into long-term memory
traces.
There is no hard and fast line between the cellular mechanisms
underlying the development of the nervous system and those involved in learning. Nonetheless, the former emphasizes the questions of how one part of the brain comes to be connected to another
and how overall patterns of connectivity are formed, while the latter
tends to regard the connections as in place, and asks how their
strengths can be modified to improve the network’s performance.
Studies of regeneration—the reforming of connections after damage to neurons or cell tracts—are thus associated more with developmental mechanisms than with learning per se. Another significant area of research that complements development is that of
aging, but there is still too little work relating aging to neural
modeling.
Study of the regeneration of retinotopic eye-brain maps in frogs
(i.e., neighboring points in the frog retina map, in a one-to-many
fashion, to neighboring points in the optic tectum) has been one of
the most fruitful areas for theory-experiment interaction in neuroscience. Following optic nerve section, optic nerve fibers tended to
regenerate connections with those target neurons to which they
were connected before surgery, even after eye rotation. This suggests that each cell in both retina and tectum has a unique chemical
marker signaling 2D location, and that retinal axons seek out tectal
cells with the same positional information. However, in experiments in which lesions were made in goldfish retina or tectum, it
was found that topographic maps regenerated in conformance with
whatever new boundary conditions were created by the lesions;
e.g., the remaining half of a retina would eventually connect in a
retinotopic way to the whole of the tectum, rather than just to the
half to which it was originally connected. Although there is wide
variation between species in the degree of order existing in the optic
nerve, it is almost always the case that the final map in the tectum
is ordered to a greater extent than is the optic nerve. Theory and
experiment paint a subtle view in which genetics sets a framework
for development, but the final pattern of connections depends both
on boundary conditions and on patterns of cellular activity. This
view is now paradigmatic for our understanding of how patterns of
neural connectivity are determined. The development of such maps
appears to proceed in two stages: the first involves axon guidance
independent of neural activity; the second involves the refinement
of initially crude patterns of connections by processes dependent
on neural activity. AXONAL PATH FINDING focuses on the former
events, while DEVELOPMENT OF RETINOTECTAL MAPS discusses
the latter. Understanding the molecular basis of retinotectal map
formation has been transformed since the appearance of the first
edition of the Handbook by discoveries centering on ephrins and
the corresponding Eph receptors. The Eph/ephrins come in two
families, A and B, with the A family important for mapping along
the rostral-caudal axis of the tectum, while the B family may be
important for mapping along the dorsal-ventral axis. Most models
of development of retinotectal maps take synaptic strengths as their

51

primary variable between arrays of retinal and tectal locations, with
initial synaptic strengths then updated according to rules that depend in various ways on correlated activity, competition for tectal
space, molecular gradients, and fiber-fiber interactions. However,
actual movement or branching of axons to find their correct targets
is rarely considered. Thus, future computational models of retinotectal map formation should take into account data on Eph receptors
and ephrin ligands, data on the guidance of retinal axons that enter
the tectum by ectopic routes, and the results of retinal and tectal
ablation and transplantation experiments. Up to now, the great majority of theoretical work in the neural network tradition has focused on changes in synaptic strengths within a fixed connectional
architecture, but how axons chart their initial path toward the correct target structure has generally not been addressed. AXONAL
PATH FINDING reviews recent experimental work addressing how
retinal ganglion cell axons find the optic disk, how they then exit
the retina, why they grow toward the optic chiasm, why some then
cross at the midline while others do not, and so on—a body of
knowledge that now has the potential to be framed and interpreted
in terms of theoretical models. Whereas work in neural networks
has usually focused on processes such as synaptic plasticity that
are dependent on neural activity, models for axon guidance must
generally be phrased in terms of activity-independent mechanisms,
particularly guidance by molecular gradients. Many fundamental
questions remain unresolved, for which theoretical models have the
potential to make an important contribution. What is the minimum
gradient steepness detectable by a growth cone, and how does this
vary with the properties of the receptor-ligand interaction and the
internal state of the growth cone? How is a graded difference in
receptor binding internally converted into a signal for directed
movement? And, how do axons integrate multiple cues?
OCULAR DOMINANCE AND ORIENTATION COLUMNS studies two
issues that go beyond basic map formation to provide further insight into activity-dependent development. When cells in layer IVc
of visual cortex are tested to see which eye drives them more
strongly, it is found that ocular dominance takes the form of a
zebra-stripe-like pattern of alternating dominance. Model and experiment support the view that the stripes are not genetically specified but instead form through network self-organization. Another
classic example is the formation of orientation specificity. A number of models are reviewed in light of current data, both theoretical
analysis based on the idea that leading eigenvectors dominate (cf.
“Pattern Formation, Biological” and “Pattern Formation, Neural”)
and computer simulations.
TEMPORAL DYNAMICS OF BIOLOGICAL SYNAPSES complements
the many studies of synaptic plasticity in the Handbook that focus
on long-term changes in synaptic strength by showing the importance of fast synaptic changes over time scales of milliseconds to
seconds. Short-term plasticity includes both synaptic depression
and a number of components of short-term enhancement (facilitation, augmentation, and posttetanic potentiation) acting over increasingly longer periods of time. In addition to having complex
short-term dynamics, synapses are stochastic (see “Synaptic Transmission”), and it is argued that constructive roles for unreliable
transmission become apparent when short-term plasticity is considered in connection with stochastic transmission, with synapses
acting as stochastic temporal filters of their presynaptic spike trains.
DYNAMIC LINK ARCHITECTURE develops the theme of fast synaptic
changes at the level of network function, viewing the brain’s data
structure as a graph composed of nodes connected by links whose
strength can change on two time scales, represented by two variables called temporary weight and permanent weight. The permanent weight corresponds to the usual synaptic weight, can change
on the slow time scale of learning, and represents permanent memory. The temporary weight can change on the same time scale as
the node activity, providing the dynamic links that, according to

52

Part II: Road Maps

this model, constitute the glue by which higher data structures are
built up from more elementary ones.
INFORMATION THEORY AND VISUAL PLASTICITY demonstrates
some features of information theory that are relevant to the relaying
of information in cortex and presents cases in which information
theory led people to seek methods for Gaussianizing the input distribution and, in other cases, to seek learning goals for nonGaussian distributions. The MDL principle (see “Minimum Description Length Analysis”) was presented as a learning goal which
takes into account the complexity of the decoding network. In particular, the article connects entropy-based methods, projection pursuit, and extraction of simple cells in visual cortex.
As can be seen from the above, neural network models of development and regeneration have been dominated by studies of the
visual system. The next article, however, takes us to the somatosensory system. Research in the past decade has demonstrated plastic changes at all levels of the adult somatosensory system in a
wide range of mammalian species. Changes in the relative levels
of sensory stimulation as a result of experience or injury produce
modifications in sensory maps. SOMATOTOPY: PLASTICITY OF SENSORY MAPS discusses which features of somatotopic maps change
and under what conditions, the mechanisms that may account for
these changes, and the functional consequences of sensory map
changes.
Just as the giant squid axon provided invaluable insights into the
active properties of neural membrane summarized in the HodgkinHuxley equation, so have invertebrates provided many insights into
other basic mechanisms (see “Neuromodulation in Invertebrate
Nervous Systems” and “Crustacean Stomatogastric System” for
two examples). INVERTEBRATE MODELS OF LEARNING: APLYSIA
AND HERMISSENDA does the same for basic learning mechanisms.
A ganglion (localized neural network) of these invertebrates can
control a variety of different behaviors, yet a given behavior such
as a withdrawal response may be mediated by 100 neurons or less.
Moreover, many neurons are relatively large and can be uniquely
identified, functional properties of an individual cell can be related
to a specific behavior, and changes in cellular properties during
learning can be related to specific changes in behavior. Biophysical
and molecular events underlying the changes in cellular properties
can then be determined and mathematically modeled. The present
article illustrates this with studies of two gastropod mollusks: associative and nonassociative modifications of defensive siphon and
tail withdrawal reflexes in Aplysia and associative learning in
Hermissenda.
HABITUATION describes one of the simplest forms of learning,
the progressive decrement in a behavioral response with repeated
presentations of the eliciting stimulus, and reveals the complexity
in this apparent simplicity. This article reviews the fundamental
characteristics of habituation and describes experimental preparations in which the neural basis of habituation has been examined
as well as attempts to model habituation. Experimental studies have
identified at least two important neural mechanisms of habituation,
homosynaptic depression within the reflex circuit and extrinsic descending modulatory input. A number of systems are put forward
as good candidates for future modeling. Habituation of defensive
reflexes was among the first types of learning explained successfully at the cellular level. Habituation in the crayfish tail-flip reflex,
due to both afferent depression as well as descending inhibition,
offers the opportunity to analyze the interaction and cooperativity
of mechanisms intrinsic and extrinsic to the reflex circuit. The nematode C. elegans offers the possibility of a genetic analysis of
habituation.
As shown in “Dendritic Processing,” dendrites are highly complex structures, both anatomically and physiologically, and are the
principal substrates for information processing within the neuron.
DENDRITIC LEARNING assesses the consequences of axodendritic

structural plasticity for learning and memory, countering the view
that neural plasticity is limited to the strengthening and weakening
of existing synaptic connections. In particular, the article supports
the view that long-term storage may involve the correlation-based
sorting of synaptic contacts onto the many separate dendrites of a
target neuron. In the models offered in this article, the output of
the cell represents the sum of a moderately large set of separately
thresholded dendritic subunits, so that a single neuron as modeled
here is equivalent to a conventional ANN built from two layers of
point neurons. As a result, the concept of “overall connection
strength between two neurons” is no longer well defined, for it is
the distribution of synapses in relation to dendritic geometry that
proves crucial.

Neural Coding
ADAPTIVE SPIKE CODING
INTEGRATE-AND-FIRE NEURONS AND NETWORKS
LOCALIZED VERSUS DISTRIBUTED REPRESENTATIONS
MOTOR CORTEX: CODING AND DECODING OF DIRECTIONAL
OPERATIONS
OPTIMAL SENSORY ENCODING
POPULATION CODES
RATE CODING AND SIGNAL PROCESSING
SENSORY CODING AND INFORMATION TRANSMISSION
SPARSE CODING IN THE PRIMATE CORTEX
SYNCHRONIZATION, BINDING AND EXPECTANCY
SYNFIRE CHAINS
In the McCulloch-Pitts neuron, the output is binary, generated on
a discrete-time scale; at the other extreme, the Hodgkin-Huxley
equations can create a dazzling array of patterns of axonal activity
in which the shape as well as the timing of each spike is continuously variable. In between, we have models such as the leaky integrator model, in which only the rate of firing of a cell is significant, while in the spiking neuron model the timing but not the shape
of spikes is continuously variable. This raises the question of how
sensory inputs and motor outputs, let alone “thoughts” and other
less mental intervening variables, are coded in neural activity. In
answering this question, we must not only seek to understand the
significance of the firing pattern of an individual neuron but also
probe how variables may be encoded in patterns of firing distributed across a whole population of neurons.
Retinotopic feature maps are the norm near the visual periphery
and up into the early stages of the visual cortex. Here, the firing of
a cell peaks for stimuli that fall on a specific patch of the retina
and also for a specific feature. Perhaps the most famous example
of this is provided by the simple cells discovered in visual cortex
by Hubel and Wiesel, which are edge-sensitive cells tuned both for
the retinal position and orientation of the edge. In such studies, the
cell is characterized by its firing rate during presentation of the
stimulus. Similar results are seen for other feature types (see “Feature Analysis”) and other sensory systems. The issue of how other
information may be coded by activity in the nervous systems of
animals is addressed in a number of articles. LOCALIZED VERSUS
DISTRIBUTED REPRESENTATIONS asks whether the final neural encoding of visual recognition of one’s grandmother, say, involves
neurons that respond selectively to “grandmother”—so-called
“grandmother cells”—or whether the sight of grandmother is never
made explicit at the single neuron level, with the representation
instead distributed across a large number of cells, none of which
responds selectively to “grandmother” alone. Few neuroscientists
argue that individual neurons might explicitly represent particular
objects, but many connectionists have used localist representations
to model phenomena that include word and letter perception, although they generally insist that the units in their models are not
real neurons. The article examines neurophysiological evidence

II.5. Biological Neurons and Networks
that both distributed and local coding are used in high-order visual
areas and then goes “against the stream” by forwarding computational reasons for preferring representations that are more localist
in some parts of the brain, before examining how work on temporal
coding schemes has changed the nature of the local versus distributed debate. SPARSE CODING IN THE PRIMATE CORTEX marshals
theoretical reasons and experimental evidence suggesting that the
brain adopts a compromise between distributed and local representations that is often referred to as sparse coding. This thesis is
illustrated with data on object recognition and face recognition in
inferotemporal cortex (the “what” pathway) in monkey.
Perhaps the best-known example of motor coding is that described in MOTOR CORTEX: CODING AND DECODING OF DIRECTIONAL OPERATIONS for the relation between the direction of reaching and changes in neuronal activity that have been established for
several brain areas, including the motor cortex. The cells involved
each have a broad tuning function the peak of which is considered
to be the “preferred” direction of the cell. A movement in a particular direction will engage a whole population of cells. It is found
that, during discrete movements in 2D and 3D space, the weighted
vector sum of these neuronal preferences is a “population vector”
which points in (close to) the direction of the movement. Such
examples underlie the more general analysis given in POPULATION
CODES. Population codes are computationally appealing both because the overlap among the neurons’ tuning curves allows precise
encoding of values that fall between the peaks of two adjacent
tuning curves and because many cortical functions, such as sensorimotor transformations, can be easily modeled with population
codes. The article focuses on decoding, or reading out, population
codes. Neuronal responses are noisy, leading to the need for good
estimators for the encoded variables. The article reviews the various estimators that have been proposed, and considers their neuronal implementations. Moreover, there are cases where it is reasonable to assume that population activity codes for more than just
a single value, and could even code for a whole probability distribution. The goal of decoding is then to recover an estimate of this
probability distribution.
INTEGRATE-AND-FIRE NEURONS AND NETWORKS shows how
these models offer potential principles of coding and dynamics. At
the single neuron level, it is shown that coherent input is more
efficient than incoherent spikes in driving a postsynaptic neuron.
Questions discussed for homogeneous populations include conditions under which it is possible, in the absence of an external stimulus, to stabilize a population of spiking neurons at a reasonable
level of spontaneous activity, and the relation of frequency of collective oscillations to neuronal parameters, and how rapidly population activity responds to changes in the input. An extension to
mixed excitatory/inhibitory populations as found in the cortex is
also discussed. SYNCHRONIZATION, BINDING AND EXPECTANCY argues that the “binding” of cells that correspond to features of a
given visual object may exploit another dimension of cellular firing, namely, the phase at which a cell fires within some overall
rhythm of firing. The article presents data consistent with the proposal that the synchronization of responses on a time scale of milliseconds provides an efficient mechanism for response selection
and binding of population responses. Synchronization also increases the saliency of responses because it allows for effective
spatial summation in the population of neurons receiving convergent input from synchronized input cells. SYNFIRE CHAINS were
introduced to account for the appearance of precise firing sequences
with long interspike delays, dealing with the ways in which such
chains might be generated, activity propagation along the chain,
how synfire chains can be used to compute, and how they might
be detected in electrophysiological recordings. A synfire chain is
composed of many pools (or layers) of neurons connected in a
feedforward fashion. In a random network with moderate connectivity, many synfire chains can be found by chance, but such ran-

53

dom synfire chains may not function reproducibly unless the synaptic connections are strengthened by some appropriate learning
rule. A given neuron can participate in more than one synfire chain.
The extent to which such repeated membership can take place without compromising reproducibility is known as the memory capacity
of synfire chains. Synfire chains may be considered a special case
of the “cell assembly” suggested by Hebb. However, in Hebb’s
concepts the cell assembly was a network with multiple feedback
connections, whereas the synfire chain is a feedforward net. This
allows for much faster computations by synfire chains. While noting that there have also been criticisms of the theory, the article
argues that classical anatomy and physiology of the cortex sustain
the idea that activity may be organized in synfire chains and that
one can create compositional systems from synfire chains.
RATE CODING AND SIGNAL PROCESSING investigates ways in
which the sequence of spike occurrence times may encode the information that a neuron communicates to its targets. Spike trains
are often quite variable under seemingly identical stimulation conditions. Does this variability carry information about the stimulus?
The term rate coding is applied in situations where the precise
timing of spikes is thought not to play a significant role in carrying
sensory information. The article analyzes the sensory information
conveyed by two types of rate codes, mean firing rate codes and
instantaneous firing rate codes, by adapting classical methods of
statistical signal processing to the analysis of neuronal spike trains.
While focusing on various examples of rate coding, such as that of
neurons of weakly electric fish sensitive to electrical field amplitude, the article also notes cases in which spike timing plays a
crucial role.
Recent years have seen an increasing number of quantitative
studies of neuronal coding based on Shannon’s information theory,
in which the “information” or “entropy” of a message is a purely
statistical measure based on the probability of the message within
an ensemble: the less likely the message is to occur, the greater its
information content. SENSORY CODING AND INFORMATION TRANSMISSION reviews two recent approaches to measuring transmitted
information. The first is based on direct estimation of the spike
train entropies in terms of which transmitted information is defined;
the second is based on an expansion to second order in the length
of the spike trains. The meaning of any signal that we receive from
our environment is modulated by the context within which it appears. ADAPTIVE SPIKE CODING explores the analysis of “context”
as the statistical ensemble in which the signal is embedded. Interpreting a message requires both registering the signal itself and
knowing something about this statistical ensemble. The relevant
temporal or spatial ensemble depends on the task. Information theoretically, representations that appropriately take into account the
statistical properties of the incoming signal are more efficient (see
OPTIMAL SENSORY ENCODING and “Information Theory and Visual
Plasticity”). The article focuses on neural adaptation, reversible
change in the response properties of neurons on short time scales.
Since the first observations of adaptation in spiking neurons, it had
been suggested that adaptation serves a useful function for information processing, preventing a neuron from continuing to transmit
redundant information, viewing both the filtering and the threshold
function of a neuron as adaptive functions of the input that may
implement the goal of increasing information transmission. Issues
include adaptation to the stimulus distribution, with the information
about the ensemble read off from the statistics of spike time differences; the separation of different time scales in adaptation; and
adaptation of receptive fields. The article also explores the role of
calcium and of channel dynamics in providing adaptation
mechanisms.
OPTIMAL SENSORY ENCODING focuses on the visual system,
seeking to understand what type of data encoding for signals passing from retina to cerebral cortex could reduce the data rate without
significant information loss, exploiting the fact that nearby image

54

Part II: Road Maps

pixels tend to convey similar signals and thus carry redundant information. One strategy is to transform the original redundant signal (e.g., in photoreceptors) to nonredundant signals in the retinal
ganglion cells or cortical neurons, as in the Infomax proposal. The
article presents different coding schemes with different advantages.
The retinal code has the advantage of small and identical receptive
field (RF) shapes, involving shorter neural wiring and easier specifications. The cortical multiscale code is preferred when invariance
is needed for objects moving in depth. Again, whereas the Infomax
principle applies well to explain the RFs of the more numerous
class of retinal ganglion cells, the P cells in monkeys or X cells in
cats, another class of ganglion cells, M cells in monkeys or Y cells
in cats, have RFs that are relatively larger, color unselective, and
tuned to higher temporal frequencies. These M cells do not extract
the maximum information possible (Infomax) about the input but
can serve to extract the information as quickly as possible. It is
argued that information theory is more likely to find its application
in the early stages of the sensory processing, before information is
selected or discriminated for any specific cognitive task, and that
optimal sensory coding in later stages of sensory pathways will
depend on cognitive tasks that require applications of alternative
theories.

Biological Networks
CORTICAL HEBBIAN MODULES
CORTICAL POPULATION DYNAMICS AND PSYCHOPHYSICS
DOPAMINE, ROLES OF
HIPPOCAMPAL RHYTHM GENERATION
INTEGRATE-AND-FIRE NEURONS AND NETWORKS
LAYERED COMPUTATION IN NEURAL NETWORKS
NEUROMODULATION IN INVERTEBRATE NERVOUS SYSTEMS
NEUROMODULATION IN MAMMALIAN NERVOUS SYSTEMS
RECURRENT NETWORKS: NEUROPHYSIOLOGICAL MODELING
SLEEP OSCILLATIONS
TEMPORAL INTEGRATION IN RECURRENT MICROCIRCUITS
We turn now to studies of biological neural networks, a study complemented by articles in the road map Mammalian Brain Regions
and in other road maps on sensory systems, memory, and motor
control.
CORTICAL HEBBIAN MODULES models the activity seen in cortical networks during the delay period following the presentation
of the stimulus in a delay match-to-sample or delay eye-movement
task. The rates observed are in the range of about 10–20 spikes/s,
with the subset of neurons that sustain elevated rates being selective
of the sample stimulus and concentrated in localized columns in
associative cortex. The article shows how to model these selective
activity distributions through the autonomous local dynamics in the
column. The model presents neural elements and synaptic structures that can reproduce the observed neuronal spike dynamics;
showing how Hebbian synaptic dynamics can give rise, in a process
of training, to a synaptic structure in the local module capable of
sustaining selective activity during the delay period. The mathematical framework for the analysis is provided by the mean field
theory of statistical mechanics.
LAYERED COMPUTTION IN NEURAL NETWORKS abstracts from
the biology to present a general framework for modeling computations performed in layered structures (which occur in many parts
of the vertebrate and invertebrate brain, including the optic tectum,
the avian visual wulst, and the cephalopod optic lobe, as well as
the mammalian cerebral cortex). A general formalism is presented
for the connectivity between layers and the dynamics of typical
units of each layer. Information processing capabilities of neural
layers include filter operations; lateral cooperativity and competition that can be used in, e.g., stereo vision and winner-take-all;

topographic mapping that underlies the allocation of cortical neurons to different parts of the visual field (fovea/periphery), or the
processing of optic flow patterns; and feature maps and population
coding, which may be applied both to sensory systems and to “motor fields” of neurons so that the flow of activity in motor areas can
predict initiated movements. In a related vein, CORTICAL POPULATION DYNAMICS AND PSYCHOPHYSICS describes cortical population dynamics in the form of structurally simple differential equations for the neurons’ firing activities, using a model class
introduced by Wilson and Cowan. The Wilson-Cowan model is
powerful enough to reproduce a variety of cortical phenomena and
captures the dynamics of neuronal populations seen in a variety of
experiments, yet simple enough to allow for analytical treatment
that yields an understanding of the mechanisms leading to the observed behavior. The model is applied here to explain dynamical
properties of the primate visual system on different levels, reaching
from single neuron properties like selectivity for the orientation of
a stimulus up to higher cognitive functions related to the binding
and processing of stimulus features in psychophysical discrimination experiments.
HIPPOCAMPAL RHYTHM GENERATION notes that global brain
states in both normal and pathological situations may be associated
with spontaneous rhythmic activities of large populations of neurons. This article presents data and models on the main such states
associated with the hippocampus: the two main normally occurring
states—the theta rhythm with the associated gamma oscillation,
and the irregular sharp waves (SPW) with the associated highfrequency (ripple) oscillation—and a pathological brain state associated with epileptic seizures. Several different modeling strategies are compared in studying rhythmicity in the hippocampal CA3
region.
SLEEP OSCILLATIONS analyzes cortical and thalamic networks at
multiple levels, from molecules to single neurons to large neuronal
assemblies, with techniques ranging from intracellular recordings
to computer simulations, to illuminate the generation, modulation,
and function of brain oscillations. Sleep is characterized by synchronized events in billions of synaptically coupled neurons in thalamocortical systems. The early stage of quiescent sleep is associated with EEG spindle waves, which occur at a frequency of 7 to
14 Hz; as sleep deepens, waves with slower frequencies appear on
the EEG. The other sleep state, associated with rapid eye movements (REM sleep) and dreaming, is characterized by abolition of
low-frequency oscillations and an increase in cellular excitability,
very much like wakefulness, although motor output is markedly
inhibited. Activation of a series of neuromodulatory transmitter
systems during arousal blocks low-frequency oscillations, induces
fast rhythms, and allows the brain to recover full responsiveness.
It is a truism that similarity of input-output behavior is no guarantee of similarity of internal function in two neural networks. In
particular, a recurrent neural network trained by backpropagation
to mimic some biological function may have little internal resemblance to the neural networks responsible for that function in the
living brain. Nonetheless, RECURRENT NETWORKS: NEUROPHYSIOLOGICAL MODELING demonstrates that dynamic recurrent network models (see “Recurrent Networks: Learning Algorithms” for
the formal background) can provide useful tools to help systems
neurophysiologists understand the neural mechanisms mediating
behavior. Biological experiments typically involve bits of the system; neural network models provide a method of generating working models of the complete system. Confidence in such models is
increased if they not only simulate dynamic sensorimotor behavior
but also incorporate anatomically appropriate connectivity. The
utility of such models is illustrated in the analysis of four types of
biological function: oscillating networks, primate target tracking,
short-term memory tasks, and the construction of neural
integrators.
As is evident in the road map Biological Neurons and Synapses, not all neurons are alike: they show a rich variety of conductances that endow them with different functional properties. These

II.6. Dynamics and Learning in Artificial Networks
properties and hence the collective activity of interacting groups of
neurons are not fixed, but are instead subject to modulation. The
term neuromodulation usually refers to the effect of neurochemicals such as acetylcholine, dopamine, norepinephrine, and serotonin, and other substances, including neuropeptides. By contrast
with the rapid transmission of information through the nervous
system by excitatory and inhibitory synaptic potentials, neuromodulators primarily activate receptor proteins, which do not contain
an ion channel (metabotropic receptors). These receptors in turn
activate enzymes, which change the internal concentration of substances called second messengers. Second messengers cause slower
and longer-lasting changes in the physiological properties of neurons, resulting in changes in the processing characteristics of the
neural circuit. NEUROMODULATION IN INVERTEBRATE NERVOUS
SYSTEMS stresses that the sensory information an animal needs
depends on a number of factors, including its activity patterns and
motivational state. The modulation of the sensitivities of many sensory receptors is shown for a stretch receptor in crustaceans. Modulators can activate, terminate, or modify rhythmic patterngenerating networks. One example of such “polymorphism” is that
neuromodulation can reconfigure the same network to produce either escape swimming or reflexive withdrawal in the nudibranch
mollusk Tritonia. Mechanisms and sites of neuromodulation include alteration of intrinsic properties of neurons, alteration of synaptic efficacy by neuromodulators, and modulation of neuromuscular junctions and muscles. All this makes clear the subtlety of
neuronal function that must be addressed by computational neuroscience and that may inspire the design of a new generation of
artificial neurons. Turning to the mammalian brain, we find that
the anatomical distribution of fibers releasing neuromodulatory
substances in the brain is usually very diffuse, with the activity of
a small number of neuromodulatory neurons influencing the functional properties of broad regions of the brain. NEUROMODULATION
IN MAMMALIAN NERVOUS SYSTEMS starts by summarizing physiological effects of neuromodulation, including effects on resting
membrane potential of pyramidal cells and interneurons, spike frequency adaptation, synaptic transmission, and long-term potentiation. It is stressed that the effect of a neurochemical is receptor
dependent: a single neuromodulator such as serotonin can have
dramatically different effects on different neurons, depending on
the type of receptor it activates. Indeed, a chemical may function
as a neurotransmitter for one receptor and as a neuromodulator for
another. The second half of the article reviews neural network models that help us understand how neuromodulatory effects that appear small at the single neuron level may have a significant effect
on dynamical properties when distributed throughout a network.
The article reviews several different models of the function of modulatory influences in neural circuits, including noradrenergic modulation of attentional processes (strangely, noradrenergic neurons
are those sensitive to norepinephrine), dopaminergic modulation
(by dopamine) of working memory, cholinergic modulation (by
acetylcholine) of input versus internal processing, and modulation
of oscillatory dynamics in cortex and thalamus. DOPAMINE, ROLES

then focuses specifically on roles of dopamine in both neuromodulation and in synaptic plasticity. Dopamine is a neuromodulator that originates from small groups of neurons in the ventral
tegmental area, the substantia nigra, and in the diencephalon. Dopaminergic projections are in general very diffuse and reach large
portions of the brain. The time scales of dopamine actions are diverse, from a few hundred milliseconds to several hours. The article
focuses on the mesencephalic dopamine centers because they are
the most studied, and because they are thought to be involved in
diseases such as Tourette’s syndrome, schizophrenia, Parkinson’s
disease, Huntington’s disease, drug addiction, and depression.
These centers are also involved in such normal brain functions as
working memory, reinforcement learning, and attention. The article
discusses the biophysical effects of dopamine, how dopamine levels influence working memory, the ways in which dopamine responses resemble the reward prediction signal of the temporal difference model of reinforcement learning, and the role of dopamine
in allocation of attention.
INTEGRATE-AND-FIRE NEURONS AND NETWORKS presents relatively simple models that take account of the fact that most biological neurons communicate by action potentials, or spikes (see
also “Spiking Neurons, Computation with”). In contrast to the standard neuron model used in ANNs, integrate-and-fire neurons do
not rely on a temporal average over the pulses. Instead, the pulsed
nature of the neuronal signal is taken into account and considered
as potentially relevant for coding and information processing.
However, integrate-and-fire models do not explicitly describe the
form of an action potential. Integrate-and-fire and similar spiking
neuron models are phenomenological descriptions on an intermediate level of detail. Compared to other single-cell models, they
allow coding principles to be discussed in a transparent manner.
Moreover, the dynamics in networks of integrate-and-fire neurons
can be analyzed mathematically, and large systems with thousands
of neurons can be simulated rather efficiently.
TEMPORAL INTEGRATION IN RECURRENT MICROCIRCUITS hypothesizes that the ability of neural computation in behaving organisms to produce a response at any time that depends appropriately on earlier sensory inputs and internal states rests on a common
principle by which neural microcircuits operate in different cortical
areas and species. The article argues that, while tapped delay lines,
finite state machines, and attractor neural networks are suitable for
modeling specific tasks, they appear to be incompatible with results
from neuroanatomy (highly recurrent diverse circuitry) and neurophysiology (fast transient dynamics of firing activity with few
attractor states). The authors thus view the transient dynamics of
neural microcircuits as the main carrier of information about past
inputs, from which specific information needed for a variety of
different tasks can be read out in parallel and at any time by different readout neurons. This approach leads to computer models of
generic recurrent circuits of integrate-and-fire neurons for tasks that
require temporal integration of inputs and, it is argued, provides a
new conceptual framework for the experimental investigation of
neural microcircuits and larger neural systems.
OF

II.6. Dynamics and Learning in Artificial Networks
Dynamic Systems
AMPLIFICATION, ATTENUATION, AND INTEGRATION
CANONICAL NEURAL MODELS
CHAINS OF OSCILLATORS IN MOTOR AND SENSORY SYSTEMS
CHAOS IN BIOLOGICAL SYSTEMS

55

CHAOS IN NEURAL SYSTEMS
COLLECTIVE BEHAVIOR OF COUPLED OSCILLATORS
COMPUTING WITH ATTRACTORS
COOPERATIVE PHENOMENA
DYNAMICS AND BIFURCATION IN NEURAL NETS
DYNAMICS OF ASSOCIATION AND RECALL

56

Part II: Road Maps

ENERGY FUNCTIONALS FOR NEURAL NETWORKS
OPTIMIZATION, NEURAL
PATTERN FORMATION, BIOLOGICAL
PATTERN FORMATION, NEURAL
PHASE-PLANE ANALYSIS OF NEURAL NETS
SELF-ORGANIZATION AND THE BRAIN
SHORT-TERM MEMORY
STATISTICAL MECHANICS OF NEURAL NETWORKS
STOCHASTIC RESONANCE
WINNER-TAKE-ALL NETWORKS
Much interest in ANNs has been based on the use of trainable
feedforward networks as universal approximators for functions f:
X r Y from the input space X to the output space Y. However, their
provenance was more general. The founding paper of Pitts and
McCulloch established the result that, by the mid-1950s, could be
rephrased as saying that any finite automaton could be simulated
by a network of McCulloch-Pitts neurons. A finite automaton is a
discrete-time dynamic system; that is, on some suitable time scale,
it specifies the next state q(t Ⳮ 1) as a function d(q(t), x(t)) of the
current state and input (for articles related to automata and theory
of computation, see the road map Computability and Complexity). But a neuron can be modeled as a continuous-time system (as
in a leaky integrator neuron with the membrane potential as the
state variable). A network of continuous-time neurons can then be
considered as a continuous-time system with the rate of change of
the state (which could, for example, be a vector whose elements
are the membrane potentials of the individual neurons) defined as
a function q̇(t) ⳱ f (q(t), x(t)) of the current state and input. When
the input is held constant, the network (whether discrete- or
continuous-time) may be analyzed by dynamical systems theory.
COMPUTING WITH ATTRACTORS shows some of the benefits of such
an approach. In particular, a net with internal loops may go to
equilibrium (providing a state from which the answer to some problem may be read out), enter a limit cycle (undergoing repetitive
oscillations which are useful in control of movement, and in other
situations in which a “clock cycle” is of value), or exhibit chaotic
behavior (acting in an apparently random way, even though it is
deterministic). In particular, the article builds on the notion of a
Hopfield network. Hopfield contributed much to the resurgence of
interest in neural networks in the 1980s by associating an “energy
function” with a network, showing that if only one neuron changed
state at a time, a symmetrically connected net would settle to a
local minimum of the energy, and that many optimization problems
could be mapped to energy functions for symmetric neural nets.
ENERGY FUNCTIONALS FOR NEURAL NETWORKS uses the notion of
Lyapunov function from the dynamical study of ordinary differential equations to show how the definition of energy function and
the conditions for convergence to a local minimum can be broadened considerably. (Of course, a network undergoing limit cycles
or chaos will not have an energy function that is minimized in this
sense.) OPTIMIZATION, NEURAL shows that this property can be
exploited to solve combinatorial optimization problems that require
a more or less exhaustive search to achieve exact solutions, with a
computational effort growing exponentially or worse with system
size. The article shows that ANN methods can provide heuristic
methods that yield reasonably good approximate solutions. Recurrent network methods based on deterministic annealing use an interpolating continuous (analog) space, allowing for shortcuts to
good solutions (compare “Simulated Annealing and Boltzmann
Machines”). The key to the approach offered here is the technique
of mean-field approximation from statistical mechanics. While
early neural optimizations were confined to problems encodable
with a quadratic energy in terms of a set of binary variables, in the
past decade the method has been extended to deal with more general problem types, both in terms of variable types and energy

functions, and has evolved to a general-purpose heuristic for combinatorial optimization.
DYNAMICS AND BIFURCATION IN NEURAL NETS notes that the
powerful qualitative and geometric tools of dynamical systems theory are most useful when the behavior of interest is stationary in
the sense that the inputs are at most time or space periodic. It then
shows how to analyze what kind of behavior we can expect over
the long run for a given neural network. In ANNs, the final state
may represent the recognition of an input pattern, the segmentation
of an image, or any number of machine computations. The stationary states of biological neural networks may correspond to cognitive decisions (e.g., binding via synchronous oscillations) or to
pathological behavior such as seizures and hallucinations. Another
important issue that is addressed by dynamical systems theory is
how the qualitative dynamics depends on parameters. The qualitative change of a dynamical system as a parameter is changed is
the subject of bifurcation theory, which studies the appearance and
disappearance of branches of solutions to a given set of equations
as some parameters vary. This article shows how to use these techniques to understand how the behavior of neural nets depends on
both the parameters and the initial states of the network. PHASEPLANE ANALYSIS OF NEURAL NETS complements the study of bifurcations with a technique for studying the qualitative behavior of
small systems of interacting neural networks whose neurons are,
essentially, leaky integrator neurons. A complete analysis of such
networks is impossible but when there are at most two variables
involved, a fairly complete description can be given. The article
introduces this qualitative theory of differential equations in the
plane, analyzing two-neuron networks that consist of two excitatory cells, two inhibitory cells, or an excitatory and inhibitory cell.
While planar systems may seem to be a rather extreme simplification, it is argued that in some local cortical circuits we can view
the simple planar system as representing a population of coupled
excitatory and inhibitory neurons. Computational methods are a
very powerful adjunct to this type of analysis. The article concludes
with comments on numerical methods and software.
CANONICAL NEURAL MODELS starts from the observation that
various models of the same neural structure could produce different
results. It thus shows how to derive results that can be observed in
a class or a family of models. To exemplify the utility of considering families of neural models instead of a single model, the article
shows how to reduce an entire family of Hodgkin-Huxley-type
models to a canonical model. A model is canonical for a family if
there is a continuous change of variables that transforms any other
model from the family into this one. As an example, a canonical
phase model is presented for a family of weakly coupled oscillators.
The change of variables does not have to invertible, so the canonical model is usually lower-dimensional, simple, and tractable, and
yet retains many important features of the family. For example, if
the canonical model has multiple attractors, then each member of
the family has multiple attractors.
Chaotic phenomena, in which a deterministic law generates complicated, nonperiodic, and unpredictable behavior, exist in many
real-world systems and mathematical models. Chaos has many intriguing characteristics, such as sensitive dependence on initial conditions. CHAOS IN BIOLOGICAL SYSTEMS provides a view of the
appearance of this phenomenon of “deterministic randomness” in
a variety of models of physical and biological systems. Features
used in assessing time series for chaotic behavior include the power
spectrum, dimension, Lyapunov exponent, and Poincaré map. Examples are given from ion channels through cellular activity to
complex networks, and “dynamical disease” is characterized by
qualitative changes in dynamics in biological control systems.
However, the high dimensions of biological systems and the environmental fluctuations that lead to nonstationarity make convincing demonstration of chaos in vivo (as opposed to computer

II.6. Dynamics and Learning in Artificial Networks
models) a difficult matter. CHAOS IN NEURAL SYSTEMS looks at
chaos in the dynamics of axons, neurons, and networks. An open
issue is to understand the significance, if any, of observed fluctuations that appear chaotic. Does a neuron function well despite
fluctuations in the timing between spikes, or are the irregularities
essential to its task? And if the irregularities are essential to the
task, is there any reason to expect that deterministic (chaotic) irregularities would be better than random ones? The vexing question
of whether chaos adds functionality to neural networks is still open
(see also “Synaptic Noise and Chaos in Vertebrate Neurons”). STOCHASTIC RESONANCE is a nonlinear phenomenon whereby the addition of a random process, or “noise,” to a weak incoming signal
can enhance the probability that it will be detected by a system.
Information about the signal transmitted through the system is also
enhanced. The information content or detectability of the signal is
degraded for noise intensities that are either smaller or larger than
some optimal value. Stochastic resonance has been demonstrated
at several levels in biology, from ion channels in cell membranes
to animal and human cognition, perception, and, ultimately,
behavior.
PATTERN FORMATION, BIOLOGICAL presents a general methodology, based on analysis of the largest eigenvalue, for tracing the
asymptotic behavior of a dynamical system, and applies it to the
problem of biological pattern formation. Turing originally considered the problem of how animal coat patterns develop. He suggested that chemical markers in the skin comprise a system of
diffusion-coupled chemical reactions among substances called
morphogens. Turing showed that in a two-component reactiondiffusion system, a state of uniform chemical concentration can
undergo a diffusion-driven instability, leading to the formation of
a spatially inhomogeneous state. In population biology, patchiness
in population densities is the norm rather than the exception. In
developmental biology, groups of previously identical cells follow
different developmental pathways, depending on their position, to
yield the rich spectrum of mammalian coat patterns and the patterns
found in fishes, reptiles, mollusks, and butterflies. The article closes
with a mechanical model of the process of angiogenesis (genesis
of the blood supply) and network formation of endothelial cells in
the extracellular matrix, as well as a new approach for predicting
brain tumor growth. PATTERN FORMATION, NEURAL shows that the
Turing mechanism for spontaneous pattern formation plays an important role in studying two key questions on the large-scale functional and anatomical structure of cortex: How did the structure
develop? What forms of spontaneous and stimulus-driven neural
dynamics are generated by such a cortical structure? In the neural
context, interactions are mediated not by molecular diffusion but
by long-range axonal connections. This neural version of the Turing instability has been applied to many problems concerning the
dynamics and development of cortex. In the former case, pattern
formation occurs in neural activity; in the latter it occurs in synaptic
weights. In most cases there exists some underlying symmetry in
the model that plays a crucial role in the selection and stability of
the resulting patterns.
Complementing this theme of pattern formation, SELFORGANIZATION AND THE BRAIN contrasts the algorithmic division
of labor between programmer and computer in most current manmade computers with the view of the brain as a dynamical system
in which ordered structures arise by processes of self-organization.
It argues that, whereas the theory of self-organization has so far
focused on the establishment of static structures, the nervous system is concerned with the generation of purposeful, nested processes evolving in time. However, if a self-organizing system is to
create the appropriate patterns, quite a few control parameters in a
system must all be put in the right ballpark. The article argues that,
in view of the variability of the physiological state of the nervous
system, evolution must have developed general mechanisms to ac-

57

tively and autonomously regulate its systems such as to produce
interesting self-organized processes and states. The process of brain
organization is seen as a cascade of steps, each one taking place
within the boundary conditions established by the previous one,
but the theory of such cascades is still nonexistent, posing massive
challenges for future research. COOPERATIVE PHENOMENA offers a
related perspective, developing what has been a major theme in
physics for the last century: statistical mechanics, which shows
how, for example, to average out the individual variations in position and velocity of the myriad molecules in a gas to understand
the relationship between pressure, volume, and temperature, or to
see how variations in temperature can yield dramatic phase transitions, such as from ice to water or from water to steam. The article
places these ideas in a general setting, stressing the notion of an
order parameter (such as temperature in the previous example) that
describes the macroscopic order of the system and whose variation
can yield qualitative changes in system behavior. Unlike a control
parameter, which is a quantity imposed on the system from the
outside, an order parameter is established by the system itself via
self-organization. The argument is mainly presented at a general
level, but the article concludes by briefly examining cooperative
phenomena in neuroscience, including pattern formation (see also
PATTERN FORMATION, BIOLOGICAL), EEG, MEG, movement coordination, and hallucinations (see also PATTERN FORMATION,
NEURAL).
STATISTICAL MECHANICS OF NEURAL NETWORKS introduces the
reader to some of the basic methods of statistical mechanics and
shows that they can be applied to systems made up of large numbers of (formal) neurons. Statistical mechanics has studied magnets
as lattices with an atomic magnet (modeled as, e.g., a spin that can
be up or down) at each lattice point, and this has led to the statistical
analysis of neural networks as “spin glasses,” where firing and
nonfiring correspond to “spin up” and “spin down,” respectively.
It has also led to the study of “Markov Random Field Models in
Image Processing,” in which the initial information at each lattice
site represents some local features of the raw image, while the final
state allows one to read off a processed image.
COLLECTIVE BEHAVIOR OF COUPLED OSCILLATORS explains the
use of phase models (here, the phase is the phase of an oscillation,
not the type of phase whose transition is studied in statistical mechanics) to help understand how temporal coherence arises over
populations of densely interconnected oscillators, even when their
frequencies are randomly distributed. The phase oscillator model
for neural populations exemplifies the idea that certain aspects of
brain functions seem largely independent of the neurophysiological
details of the individual neurons while trying to recover phase information, i.e., the kind of information encoded in the form of
specific temporal structures of the sequence of neuronal spikings.
The article reviews the collective behavior of coupled oscillators
using the phase model and assuming all-to-all type interconnections. Despite this simplification, a great variety of collective behaviors is exhibited. Special attention is given to the onset and
persistence of collective oscillation in frequency-distributed systems, splitting of the population into a few subgroups (clustering),
and the more complex collective behavior called slow switching.
Collections of oscillators that send signals to one another can phase
lock, with many patterns of phase differences. CHAINS OF OSCILLATORS IN MOTOR AND SENSORY SYSTEMS discusses a set of examples that illustrate how those phases emerge from the oscillator
interactions. Much of the work was motivated by spatiotemporal
patterns in networks of neurons that govern undulatory locomotion.
The original experimental preparation to which this work was applied is the lamprey central pattern generator (CPG) for locomotion, but the mathematics is considerably more general. The article
discusses several motor systems, then turns to the procerebral lobe
of Limax, the common garden slug, to illustrate chains of oscillators

58

Part II: Road Maps

in a sensory system. Since the details of the oscillators often are
not known and difficult to obtain, the object of the mathematics is
to find the consequences of what is known, and to generate sharper
questions to motivate further experimentation.
AMPLIFICATION, ATTENUATION, AND INTEGRATION focuses on
the computational role of the recurrent connections in networks of
leaky integrator neurons. Setting the transfer function f (u) to be
simply u in the network equations yields a linear network that can
be completely analyzed using the tools of linear systems theory.
The article describes the properties of linear networks and gives
some examples of their application to neural modeling. In this
framework, it is shown how recurrent synaptic connectivity can
either attenuate or speed up responses; both effects can occur simultaneously in the same network. Besides amplification and attenuation, a linear network can also carry out temporal integration,
in the sense of Newtonian calculus, when the strength of feedback
is precisely tuned for an eigenmode, so that its gain and time constant diverge to infinity. Finally, it is noted that the linear computations of amplification, attenuation, and integration can be ascribed
to a number of brain areas.
WINNER-TAKE-ALL NETWORKS presents a number of designs for
neural networks that solve the following problem: given a number
of networks, each of which provides as output some “confidence
measure,” find in a distributed manner the network whose output
is strongest. Two important variants of winner-take-all are kwinner-take-all, where the k largest inputs are identified, and softmax, which consists of assigning each input a weight so that all
weights sum to 1 and the largest input receives the biggest weight.
The article first describes softmax and shows how winner-take-all
can be derived as a limiting case; it then describes how they can
both be derived from probabilistic, or energy function, formulations; and it closes with a discussion of VLSI and biological mechanisms. “Modular and Hierarchical Learning Systems” addresses a
somewhat related topic: Given a complex problem, find a set of
networks, each of which provides an approximate solution in some
region of the state space, together with a gating network that can
combine these approximations to yield a globally satisfactory solution (i.e., blend the “good” solutions rather than extract the “best”
solution).
DYNAMICS OF ASSOCIATION AND RECALL uses dynamical studies to analyze the pattern recall process and its relation with the
choice of initial state, the properties of stored patterns, noise level,
and network architecture. For large networks and in global recall
processes, the strategy is to derive dynamical laws at a macroscopic
level (i.e., dependent on many neuron states). The challenge is to
find the smallest set of macroscopic quantities which will obey
closed deterministic equations in the limit of an infinitely large
network. The article focuses on simple Hopfield-type models, but
closes with a discussion of some variations and generalizations.
SHORT-TERM MEMORY asks: What are the different types of STM
traces? How do intrinsic and synaptic mechanisms contribute to the
formation of STM traces? How do STM traces translate into longterm memory representation of temporal sequences? The stress is
on computational mechanisms underlying these processes with the
suggestion that these mechanisms may well underlie a wide variety
of seemingly different biological processes. The article examines
both the short-term preservation of patterns of neural firing in a
circuit and ways in which short-term maintained activity may be
transferred into long-term memory traces.

Learning in Artificial Networks
ADAPTIVE RESONANCE THEORY
ASSOCIATIVE NETWORKS
BACKPROPAGATION: GENERAL PRINCIPLES
BAYESIAN METHODS AND NEURAL NETWORKS

BAYESIAN NETWORKS
COMPETITIVE LEARNING
CONVOLUTIONAL NETWORKS FOR IMAGES, SPEECH, AND TIME
SERIES
DATA CLUSTERING AND LEARNING
DYNAMICS OF ASSOCIATION AND RECALL
ENSEMBLE LEARNING
EVOLUTION AND LEARNING IN NEURAL NETWORKS
EVOLUTION OF ARTIFICIAL NEURAL NETWORKS
GAUSSIAN PROCESSES
GENERALIZATION AND REGULARIZATION IN NONLINEAR LEARNING SYSTEMS
GRAPHICAL MODELS: PARAMETER LEARNING
GRAPHICAL MODELS: PROBABILISTIC INFERENCE
GRAPHICAL MODELS: STRUCTURE LEARNING
HELMHOLTZ MACHINES AND SLEEP-WAKE LEARNING
HIDDEN MARKOV MODELS
INDEPENDENT COMPONENT ANALYSIS
LEARNING AND GENERALIZATION: THEORETICAL BOUNDS
LEARNING NETWORK TOPOLOGY
LEARNING AND STATISTICAL INFERENCE
LEARNING VECTOR QUANTIZATION
MINIMUM DESCRIPTION LENGTH ANALYSIS
MODEL VALIDATION
MODULAR AND HIERARCHICAL LEARNING SYSTEMS
NEOCOGNITRON: A MODEL FOR VISUAL PATTERN RECOGNITION
NEUROMANIFOLDS AND INFORMATION GEOMETRY
PATTERN RECOGNITION
PERCEPTRONS, ADALINES, AND BACKPROPAGATION
PRINCIPAL COMPONENT ANALYSIS
RADIAL BASIS FUNCTION NETWORKS
RECURRENT NETWORKS: LEARNING ALGORITHMS
REINFORCEMENT LEARNING
SELF-ORGANIZING FEATURE MAPS
SIMULATED ANNEALING AND BOLTZMANN MACHINES
STATISTICAL MECHANICS OF GENERALIZATION
STATISTICAL MECHANICS OF ON-LINE LEARNING AND
GENERALIZATION
STOCHASTIC APPROXIMATION AND EFFICIENT LEARNING
SUPPORT VECTOR MACHINES
TEMPORAL PATTERN PROCESSING
TEMPORAL SEQUENCES: LEARNING AND GLOBAL ANALYSIS
UNIVERSAL APPROXIMATORS
UNSUPERVISED LEARNING WITH GLOBAL OBJECTIVE FUNCTIONS
YING-YANG LEARNING
The majority of articles in this road map deal with learning in
artificial neural networks. Nonetheless, the road map is titled
“Learning in Artificial Networks” to emphasize the inclusion of a
body of research on statistical inference and learning that can be
seen either as generalizing neural networks or as analyzing other
forms of networks, such as Bayesian networks and graphical
models.
The fundamental difference between a system that learns and
one that merely memorizes is that the learning system generalizes
to unseen examples. Much of our concern is with supervised learning, getting a network to behave in a way that successfully approximates some specified pattern of behavior or input-output relationship. In particular, much emphasis has been placed on
feedforward networks which have no loops, so that the output of
the net depends on its input alone, since there is then no internal
state defined by reverberating activity. The most direct form of this
is a synaptic matrix, a one-layer neural network for which input
lines directly drive the output neurons and a “supervised Hebbian”
rule sets synapses so that the network will exhibit specified inputoutput pairs in its response repertoire. This is addressed in ASSO-

II.6. Dynamics and Learning in Artificial Networks
NETWORKS, which notes the problems that arise if the
input patterns (the “keys” for associations) are not orthogonal vectors. Association also extends to recurrent networks, but in such
systems of “dynamic memories” (e.g., Hopfield networks) there are
no external inputs as such. Rather the “input” is the initial state of
the network, and the “output” is the “attractor” or equilibrium state
to which the network then settles. For neurons whose output is a
sigmoid function of the linear combination of their inputs, the
memory capacity of the associative memory is approximately
0.15n, where n is the number of neurons in the net. Unfortunately,
such an “attractor network” memory model has many spurious
memories, i.e., equilibria other than the memorized patterns, and
there is no way to decide whether a recalled pattern was memorized
or not. DYNAMICS OF ASSOCIATION AND RECALL (see the road map
Dynamic Systems for more details) shows how to move away from
microscopic equations at the level of individual neurons to derive
dynamical laws at a macroscopic level that characterize association
and recall in Hopfield-type networks (with some discussion of variations and generalizations).
Historically, the earliest forms of supervised learning involved
changing synaptic weights to oppose the error in a neuron with a
binary output (the perceptron error-correction rule), or to minimize
the sum of squares of errors of output neurons in a network with
real-valued outputs (the Widrow-Hoff rule). This work is charted
in PERCEPTRONS, ADALINES, AND BACKPROPAGATION, which also
charts the extension of these classic ideas to multilayered networks.
In multilayered networks, there is the structural credit assignment
problem: When an error is made at the output of a network, how
is credit (or blame) to be assigned to neurons deep within the network? One of the most popular techniques is called backpropagation, whereby the error of output units is propagated back to yield
estimates of how much a given “hidden unit” contributed to the
output error. These estimates are used in the adjustment of synaptic
weights to these units within the network. BACKPROPAGATION:
GENERAL PRINCIPLES places this idea in a broader framework by
providing an overview of contributions that enrich our understanding of the pros and cons (such as “plateaus”) of this adaptive
architecture. It also assesses the biological plausibility of
backpropagation.
The underlying theoretical grounding is that, given any function
f : X r Y for which X and Y are codable as input and output patterns
of a neural network, then, as shown in UNIVERSAL APPROXIMATORS, f can be approximated arbitrarily well by a feedforward network with one layer of hidden units. The catch, of course, is that
many, many hidden units may be required for a close fit. It is thus
often treated as an empirical question whether there exists a sufficiently good approximation achievable in principle by a network
of a given size—an approximation that a given learning rule may
or may not find (it may, for example, get stuck in a local optimum
rather than a global one). Gradient descent methods have also been
extended to adapt the synaptic weights of recurrent networks. The
backpropagation algorithm for feedforward networks has been successfully applied to a wide range of problems, but what can be
implemented by a feedforward network is just a static mapping of
the input vectors. However, to model dynamical functions of brains
or machines, one must use a system capable of storing internal
states and implementing complex dynamics. RECURRENT NETWORKS: LEARNING ALGORITHMS presents, then, learning algorithms for recurrent neural networks that have feedback connections and time delays. In a recurrent network, the state of the system
can be encoded in the activity pattern of the units, and a wide
variety of dynamical behaviors can be programmed by the connection weights. A popular subclass of recurrent networks consists of
those with symmetric connection weights. In this case, the network
dynamics is guaranteed to converge to a minimum of some “energy” function (see “Energy Functionals for Neural Networks” and
CIATIVE

59

“Computing with Attractors”). However, steady-state solutions are
only a limited portion of the capabilities of recurrent networks. For
example, they can transform an input sequence into a distinct output sequence, and they can serve as a nonlinear filter, a nonlinear
controller, or a finite-state machine. This article reviews the learning algorithms for training recurrent networks, with the main focus
on supervised learning algorithms. (See “Recurrent Networks:
Neurophysiological Modeling” for the use of such networks in
modeling biological neural circuitry.)
One useful perspective for supervised learning views learning as
hill-climbing in weight space, so that each “experience” adjusts the
synaptic weights of the network to climb (or descend) a metaphorical hill for which “height” at a particular point in “weight
space” corresponds to some measure of the performance of the
network (or the organism or robot of which it is a part). When the
aim is to minimize this measure, the learning process is then an
example of what mathematicians call gradient descent. The term
reinforcement comes from studies of animal learning in experimental psychology, where it refers to the occurrence of an event,
in the proper relation to a response, that tends to increase the probability that the response will occur again in the same situation.
REINFORCEMENT LEARNING describes a form of “semisupervised”
learning where the network is not provided with an explicit form
of error at each time step but rather receives only generalized reinforcement (“you’re doing well”; “that was bad!”), which yields
little immediate indication of how any neuron should change its
behavior. Moreover, the reinforcement is intermittent, thus raising
the temporal credit assignment problem (see also “Reinforcement
Learning in Motor Control”): How is an action at one time to be
credited for positive reinforcement at a later time? The solution is
to build an “adaptive critic” that learns to evaluate actions of the
network on the basis of how often they occur on a path leading to
positive or negative reinforcement. Methods for this assessment of
future expected reinforcement include temporal difference (TD)
learning and Q-learning (see “Q-Learning for Robots”). Current
reinforcement learning research includes parameterized function
approximation methods; understanding how exploratory behavior
is best introduced and controlled; learning under conditions in
which the environment state cannot be fully observed; introducing
various forms of abstraction such as temporally extended actions
and hierarchy; and relating computational reinforcement learning
theories to brain reward mechanisms (see “Dopamine, Roles of”).
The task par excellence for supervised learning is pattern recognition—the problem of classifying objects, often represented as
vectors or as strings of symbols, into categories. Historically, the
field of pattern recognition started with early efforts in neural networks (see PERCEPTRONS, ADALINES, AND BACKPROPAGATION).
While neural networks played a less central role in pattern recognition for some years, recent progress has made them the method
of choice for many applications. As PATTERN RECOGNITION demonstrates, properly designed multilayer networks can learn complex
mappings in high-dimensional spaces without requiring complicated hand-crafted feature extractors. To rely more on learning, and
less on detailed engineering of feature extractors, it is crucial to
tailor the network architecture to the task, incorporating prior
knowledge to be able to learn complex tasks without requiring
excessively large networks and training sets. ENSEMBLE LEARNING
describes algorithms that, rather than finding one best hypothesis
to explain the data, construct a set (sometimes called a committee
or ensemble) of hypotheses and then have those hypotheses vote
to classify new patterns. Ensemble methods are often much more
accurate than any single hypothesis. For example, the representational problem arises when the hypothesis space does not contain
any hypotheses that are good approximations to the true decision
function f. In some cases, a weighted sum of hypotheses expands
the space of functions that can be represented. Hence, by taking a

60

Part II: Road Maps

weighted vote of hypotheses, the learning algorithm may be able
to form a more accurate approximation to f. The bulk of research
into ensemble methods has focused on constructing ensembles of
decision trees. The article introduces the techniques of bagging and
boosting, among others, and analyzes their relative merits under
different conditions.
Many specific architectures have been developed to solve particular types of learning problem. ADAPTIVE RESONANCE THEORY
(ART) bases learning on internal expectations. A pattern matching
process compares an external input with the internal memory of
various coded patterns. ART matching leads either to a resonant
state, which persists long enough to permit learning, or to a parallel
memory search. If the search ends at an established code, the memory representation may either remain the same or incorporate new
information from matched portions of the current input. When the
external world fails to match an ART network’s expectations or
predictions, a search process selects a new category, representing
a new hypothesis about what is important in the present
environment.
The neocognitron (see NEOCOGNITRON: A MODEL FOR VISUAL
PATTERN RECOGNITION) was developed as a neural network model
for visual pattern recognition that addresses the specific question,
“How can a pattern be recognized despite variations in size and
position?” by using a multilayer architecture in which local features
are replicated in many different scales and locations. More generally, as shown in CONVOLUTIONAL NETWORKS FOR IMAGES,
SPEECH, AND TIME SERIES, shift invariance in convolutional networks is obtained by forcing the replication of weight configurations across space. Moreover, the topology of the input is taken
into account, enabling such networks to force the extraction of local
features by restricting the receptive fields of hidden units to be
local, and enforcing a built-in invariance with respect to translations, or local distortions of the inputs. The idea of connecting units
to local receptive fields on the input goes back to the perceptron in
the early 1960s, and was almost simultaneous with Hubel and Wiesel’s discovery of locally sensitive, orientation-selective neurons in
the cat’s visual system.
Just as a polynomial of too high a degree is not useful for curve
fitting, a network that is too large will fail to generalize well, and
will require longer training times. Smaller networks, with fewer
free parameters, enforce a smoothness constraint on the function
found. For best performance, it is therefore desirable to find the
smallest network that will “fit” the training data. To create a neural
network, a designer typically fixes a network topology and uses
training data to tune its parameters such as connection weights.
The designer, however, often does not have enough knowledge to
specify the ideal topology. It is thus desirable to learn the topology
from training data as well. LEARNING NETWORK TOPOLOGY reviews algorithms that adjust network topology, adding neurons and
removing neurons during the learning process, to arrive at a network appropriate to a given task. For topology learning, a bias is
added to prefer smaller models. It is often found that this bias
produces a neural network that has better generalization and is more
interpretable. This framework is applied to learning the topologies
of both feedforward neural networks and BAYESIAN NETWORKS.
In Bayesian networks, all the nodes of the network are given and
set, and one searches for a topology by adding or deleting links.
Many articles in the Handbook emphasize situations where, e.g.,
learning from examples is stochastic in the sense that examples are
randomly generated and the network behavior is thus to be analyzed
from a statistical point of view. Statistical estimation identifies the
mechanism underlying stochastic phenomena. LEARNING AND STATISTICAL INFERENCE studies learning by using such statistical notions as Fisher information, Bayesian loss, and sequential estimation, as well as the Expectation-Maximization (EM) algorithm for
estimating hidden variables. Nonlinear neurodynamics, learning,

and self-organization are seen as adding new concepts to statistical
science. The article examines the dynamical behaviors of a learning
network under a general loss criterion. The behavior of learning
curves is related to neural network complexity to elucidate the discrepancy between training and generalization errors. This perspective is further developed in NEUROMANIFOLDS AND INFORMATION
GEOMETRY. A neural network is specified by its architecture and a
number of parameters such as synaptic weights and thresholds. Any
neural network of this architecture is specified by a point in the
parameter space. Learning takes place in the parameter space and
a learning process is represented by a trajectory. The article presents the approach of information geometry which sees the geometrical structure of the parameter space as given by a Riemannian
manifold. The article shows how dynamical behaviors of neural
learning on these “neuromanifolds” are related to the underlying
geometrical structures, using multilayer perceptrons and Boltzmann machines as examples.
GENERALIZATION AND REGULARIZATION IN NONLINEAR LEARNING SYSTEMS sets forth the essential relationship between multivariate function estimation in a statistical context and supervised
machine learning. Given a training set consisting of (input, output)
pairs (xi, yi), the task is to construct a map that generalizes well in
that, given a new value of x, the map will provide a reasonable
prediction for the hitherto unobserved output associated with this
x. Regularization simplifies the problem by applying constraints to
the construction of the map that reduce the generalization error (see
also “Probabilistic Regularization Methods for Low-Level Vision”). Ideally, these constraints embody a priori information concerning the true relationship between input and output, though various ad hoc constraints have sometimes been shown to work well
in practice. Feedforward neural nets, radial basis functions, and
various forms of splines all provide regularized or regularizable
methods for estimating “smooth” functions of several variables
from a given training set. Which method to use depends on the
particular nature of the underlying but unknown “truth,” the nature
of any prior information that might be available about this “truth,”
the nature of any noise in the data, the ability of the experimenter
to choose the various smoothing or regularization parameters well,
and so on.
MODULAR AND HIERARCHICAL LEARNING SYSTEMS solves a
complex learning problem by dividing it into a set of subproblems.
In the context of supervised learning, modular architectures arise
when the data can be described by a collection of functions, each
of which works well over a relatively local region of the input
space, allocating different modules to different regions of the space.
The challenge is that, in general, the learner is not provided with
prior knowledge of the partitioning of the input space. To solve
this, a “gating network” can learn which module to “listen to” in
different situations. The learning algorithms described here solve
the credit assignment problem by computing a set of posterior probabilities that can be thought of as estimating the utility of different
modules in different parts of the input space. An EM algorithm (cf.
LEARNING AND STATISTICAL INFERENCE), an alternative to gradient
methods, can be derived for estimating the parameters of both the
modular system and its extension to hierarchical architectures. The
latter arise when we assume that the data are well described by a
multiresolution model in which regions are divided recursively into
subregions.
BAYESIAN METHODS AND NEURAL NETWORKS shows how to
apply Bayes’s rule for the use of probabilities to quantify inferences
about hypotheses from given data. The idea is to take the predictions p(d|hi) made by alternative models hi about data d, and the
prior probabilities of the models p(hi), and obtain the posterior
probabilities p(hi|d) of the models given the data, using Bayes’s
rule in the form p(hi|d) ⳱ p(d|hi)p(hi)/p(d). To apply this to neural
networks, regard a supervised neural network as a nonlinear param-

II.6. Dynamics and Learning in Artificial Networks
eterized mapping from an input x to an output y ⳱ y(x; w), which
depends continuously on the “weights” parameter w. The idea is
to choose w from a weight space with some given probability distribution p(w) so as to maximize the likelihood of the nets yielding
the given set of (input, output) observations. The Bayesian framework deals with uncertainty in a natural, consistent manner by combining prior beliefs about which models are appropriate with how
likely each model would be to have generated the data. This results
in an elegant, general framework for fitting models to data that,
however, may be compromised by computational difficulties in carrying out the ideal procedure. There are many approximate Bayesian implementations, using methods such as sampling, perturbation
techniques, and variational methods. In the case of models linear
in their parameters, Bayesian neural networks are closely related
to GAUSSIAN PROCESSES (q.v.), where many of the computational
difficulties of dealing with more general stochastic nonlinear systems can be avoided. Traditionally, neural networks are graphical
representations of functions in which the computations at each node
are deterministic. By contrast, networks in which nodes represent
stochastic variables are called graphical models (see BAYESIAN
NETWORKS and GRAPHICAL MODELS: PROBABILISTIC INFERENCE).
RADIAL BASIS FUNCTION NETWORKS applies Bayesian methods
to the case where the approximation to the given y ⳱ y(x; w) is
based on a network using combinations of “radial basis” functions,
each of which is “centered” around a weight vector w, so that the
response to input x depends on some measure of “distance” of x
from w, rather than on the dot product w • x ⳱ iwixi as in many
formal neurons. The distribution of the w’s may be determined by
some form of clustering (see DATA CLUSTERING AND LEARNING).
Further learning adjusts the connection strengths to a neuron whose
outputs give an estimate of, e.g., the posterior probability p(c|x)
that class c is present given the observation (network input) x. However, it is easier to model other related aspects of the data, such as
the unconditional distribution of the data p(x) and the likelihood of
the data, p(x|c), and then recreate the posterior from these quantities
according to Bayes’s rule, p(ci|x) ⳱ p(ci)p(x|ci)/p(x).
GAUSSIAN PROCESSES continues the Bayesian approach to neural
network learning, placing a prior probability distribution over possible functions and then letting the observed data “sculpt” this prior
into a posterior using the available data. One can place a prior
distribution P(w) on the weights w of a neural network to induce
a prior over functions P( y(x;w)) but the computations required to
make predictions are not easy, owing to nonlinearities in the system. A Gaussian process, defined by a mean function and covariance matrix, can prove useful as a way of specifying a prior directly
over function space—it is often simpler to do this than to work
with priors over parameters. Gaussian processes are probably the
simplest kind of function space prior that one can consider, being
a generalization of finite-dimensional Gaussian distributions over
vectors. A Gaussian process is defined by a mean function (which
we shall usually take to be identically zero), and a covariance function C(x, x⬘) which indicates how correlated the value of the function y is at x and x⬘. This function encodes our assumptions about
the problem (e.g., that the function is smooth and continuous) and
will influence the quality of the predictions. The article shows how
to use Gaussian processes for classification problems, and describes
how data can be used to adapt the covariance function to the given
prediction problem.
MINIMUM DESCRIPTION LENGTH ANALYSIS shows how ideas relating to minimum description length (MDL) have been applied to
neural networks, emphasizing the direct relationship between MDL
and Bayesian model selection methods. The classic MDL approach
defined the information in a binary string to be the length of the
shortest program with which a general-purpose computer could
generate the string. The Bayes bridge is obtained by replacing the
Bayesian goal of inferring the “most likely” model M from a set

61

of observations by minimizing the length of an encoded message
which describe M as well as the data D expressed in term of M.
MDL and Bayesian methods both formalize Occam’s razor in that
a complex network is preferred only if its predictions are sufficiently more accurate.
UNSUPERVISED LEARNING WITH GLOBAL OBJECTIVE FUNCTIONS
makes the point that even unsupervised learning involves an implicit training signal based on the network’s ability to predict its
own input, or on some more general measure of the quality of its
internal representation. The main problem in unsupervised learning
research is then seen as the formulation of a performance measure
or cost function for the learning to generate this internal supervisory
signal. The cost function is also known as an objective function,
since it sets the objective for the learning process. The article reviews three types of unsupervised neural network learning procedures: information-preserving algorithms, density estimation techniques, and invariance-based learning procedures. The first method
is based on the preservation of mutual information Ix;y ⳱ H(x) ⳮ
H(x| y) between the input vector x and output vector y, where H(x)
is the entropy of random variable x and H(x| y) is the entropy of
the conditional distribution of x given y. The second approach is
to assume a priori a class of models that constrains the general
form of the probability density function and then to search for the
particular model parameters defining the density function (or mixture of density functions) most likely to have generated the observed data (cf. the earlier discussion of Bayesian methods). Finally, invariance-based learning extracts higher-order features and
builds more abstract representations. Once again, the approach is
to make constraining assumptions about the structure that is being
sought, and to build these constraints into the network’s architecture and/or objective function to develop more efficient, specialized
learning procedures.
The Bayesian articles stress the “global” statistical idea of “find
the weights which, according to given probability distributions
maximize some expectation” as distinct from the deterministic idea
of adjusting the weights at each time step to provide a local increment in performance on the current input. However, gradient descent provides an important tool for finding the weight settings
which decrease some stochastic expectation of error, too. STOCHASTIC APPROXIMATION AND EFFICIENT LEARNING shows that
gradient descent has a long tradition in the literature of stochastic
approximation. Any stochastic process that can be interpreted as
minimizing a cost function based on noisy gradient measurements
in a sequential, recursive manner may be considered to be a stochastic approximation. “Sequential” means that each estimate of
the location of a minimum is used to make a new observation,
which in turn immediately leads to a new estimate; “recursive”
means that the estimates depend on past gradient measurements
only through a fixed number of scalar statistics. Such on-line algorithms are useful because they enjoy significant performance advantages for large-scale learning problems. The article describes
their properties using stochastic approximation theory as a very
broad framework, and provides a brief overview of newer insights
obtained using information geometry (see NEUROMANIFOLDS AND
INFORMATION GEOMETRY) and replica calculations (see STATISTICAL MECHANICS OF ON-LINE LEARNING AND GENERALIZATION).
In order to understand the performance of learning machines,
and to gain insight that helps to design better ones, it is helpful to
have theoretical bounds on the generalization ability of the machines. The determination of such bounds is the subject of LEARNING AND GENERALIZATION: THEORETICAL BOUNDS. Here it is necessary to formalize the learning problem and turn the question of
how well a machine generalizes into a mathematical question. The
article adopts the formalization used in statistical learning theory,
which is shown to include both pattern recognition and function
learning. The road map Computability and Complexity gives

62

Part II: Road Maps

more information on this and related articles, such as “PAC Learning and Neural Networks” and “Vapnik-Chervonenkis Dimension
of Neural Networks,” which offer bounds on the performance of
learning methods. SUPPORT VECTOR MACHINES addresses the (binary) pattern recognition problem of learning theory: given two
classes of objects, to assign a new object to one of the two classes.
Trying to find the best classifier involves notions of similarity in
the set X of inputs. Support vector machines (SVMs) build a decision function as a kernel expansion corresponding to a separating
hyperplane in a feature space. SVMs rest on methods for the selection of the patterns on which the kernels are centered and in the
choice of weights that are placed on the individual kernels in the
decision function. SVMs and other kernel methods have a number
of advantages compared to classical neural network approaches,
such as the absence of spurious local minima in the optimization
procedure, the need to tune only a few parameters, and modularity
in the design. Kernel methods connect similarity measures, nonlinearities, and data representations in linear spaces where simple
geometric algorithms are performed.
The passage of the “energy” of a Hopfield network to a local
minimum can be construed as a means for solving an optimization
problem. The catch is the word “local” in local minimum—the
solution may be the best in the neighborhood, yet far better solutions may be located elsewhere. One resolution of this is described
in SIMULATED ANNEALING AND BOLTZMANN MACHINES. At the
expense of great increases in time to convergence, simulated annealing escapes local minima by adding noise, which is then gradually reduced (“lowering the temperature”). The initially high temperature (i.e., noise level) stops the system from getting trapped in
“high valleys” of the energy landscape, the lowering of temperature
allows optimization to occur in the “deepest valley” once it has
been found. The Boltzmann machine then applies this method to
design a class of neural networks. These machines use stochastic
computing elements to extend discrete Hopfield networks in two
ways: they replace the deterministic, asynchronous dynamics of
Hopfield networks with a randomized local search dynamics, and
they replace the Hebbian learning rule with a more powerful stochastic learning algorithm.
Turning from neural networks to another form of network structure, BAYESIAN NETWORKS (as distinct from BAYESIAN METHODS
AND NEURAL NETWORKS) provides an explicit method for following chains of probabilistic inference such as those appropriate to
expert systems, extending the Bayes’s rule for updating probabilities in the light of new evidence. The nodes in a Bayesian network
represent propositional variables of interest and the links represent
informational or causal dependencies among the variables. The dependencies are quantified by conditional probabilities for each
node, given its parents in the network. The network supports the
computation of the probabilities of any subset of variables, given
evidence about any other subset, and the reasoning processes can
operate on Bayesian networks by propagating information in any
direction. HELMHOLTZ MACHINES AND SLEEP-WAKE LEARNING
starts by observing that since unsupervised learning is largely concerned with finding structure among sets of input patterns, it is
important to take advantage of cases in which the input patterns
are generated in a systematic way, thus forming a manifold that
has many fewer dimensions than the space of all possible activation
patterns. The Helmholtz machine is an analysis-by-synthesis
model. The key idea is to have an imperfect generative model train
a better analysis or recognition model, and an imperfect recognition
model train a better generative model. The generative model for
the Helmholtz machine is a structured belief network (i.e., Bayesian
network) that is viewed as a model for hierarchical top-down connections in the cortex. New inputs are analyzed in an approximate
fashion using a second structured belief network (called the recognition model), which is viewed as a model for the standard,

bottom-up connections in cortex. The generative and recognition
models are learned from data in two phases. In the wake phase, the
recognition model is used to estimate the underlying generators for
a particular input pattern, and then the generative model is altered
so that those generators are more likely to have produced the input
that is actually observed. In the sleep phase, the generative model
fantasizes inputs by choosing particular generators stochastically,
and then the recognition model is altered so that it is more likely
to report those particular generators if the fantasized input were
actually to be observed. YING-YANG LEARNING further develops
this notion of simultaneously building up two pathways, a bottomup pathway for encoding a pattern in the observation space into its
representation in a representation space, and a top-down pathway
for decoding or reconstructing a pattern from an inner representation back to a pattern in the observation space. The theory of Bayesian Ying-Yang harmony learning formulates the two-pathway
approach in a general statistical framework, modeling the two pathways via two complementary Bayesian representations of the joint
distribution on the observation space and representation space. The
article shows how a number of major learning problems and methods can be seen as special cases of this unified perspective. Moreover, the ability of Ying-Yang learning for regularization and
model selection is placed in an information-theoretic perspective.
GRAPHICAL MODELS: PROBABILISTIC INFERENCE introduces a
generalization of Bayesian networks. The graphical models framework provides a clean mathematical formalism that has made it
possible to understand the relationships among a wide variety of
network-based approaches to computation, and in particular to understand many neural network algorithms and architectures as instances of a broader probabilistic methodology. Graphical models
use graphs to represent and manipulate joint probability distributions. The graph underlying a graphical model may be directed, in
which case the model is often referred to as a belief network or a
Bayesian network, or the graph may be undirected, in which case
the model is generally referred to as a Markov random field. A
graphical model has both a structural component, encoded by the
pattern of edges in the graph, and a parametric component, encoded
by numerical “potentials” associated with sets of edges in the
graph. General inference algorithms allow statistical quantities
(such as likelihoods and conditional probabilities) and informationtheoretic quantities (such as mutual information and conditional
entropies) to be computed efficiently. The article closes by noting
that many neural network architectures are special cases of the
general graphical model formalism, both representationally and algorithmically. Special cases of graphical models include essentially
all models of unsupervised learning, as well as Boltzmann machines, mixtures of experts, and radial basis function networks,
while many other neural networks, including the classical multilayer perceptron, can be profitably analyzed from the viewpoint of
graphical models. The next two articles present learning algorithms
that build on these inference algorithms and allow parameters and
structures to be estimated from data. GRAPHICAL MODELS: PARAMETER LEARNING discusses the learning of parameters for a fixed
graphical model. As noted, each node in the graph represents a
random variable, while the edges in the graph represent the qualitative dependencies between the variables; the absence of an edge
between two nodes means that any statistical dependency between
these two variables is mediated via some other variable or set of
variables. The quantitative dependencies between variables that are
connected via edges are specified via parameterized conditional
distributions, or more generally nonnegative “potential functions.”
The pattern of edges is the structure of the graph, while the parameters of the potential functions are parameters of the graph. The
present article assumes that the structure of the graph is given, and
shows how to then learn the parameters of the graph from data.
GRAPHICAL MODELS: STRUCTURE LEARNING turns to the simul-

II.6. Dynamics and Learning in Artificial Networks
taneous learning of parameters and structure. Real-world applications of such learning abound, the example presented being an analysis of data regarding factors that influence the intention of high
school students to attend college. For simplicity, the article focuses
on directed-acyclic graphical models, but the basic principles thus
defined can be applied more generally. The Bayesian approach is
emphasized, and then several common non-Bayesian approaches
are mentioned briefly.
COMPETITIVE LEARNING is a form of unsupervised learning in
which each input pattern comes, through learning, to be associated
with the activity of one or at most a few neurons, leading to sparse
representations of data that are easy to decode. Competitive learning algorithms employ some sort of competition between neurons
in the same layer via lateral connections. This competition limits
the set of neurons to be affected in a given learning trial. Hard
competition allows the final activity of only one neuron, the strongest one to start with, whereas in soft competition the activity of
the lateral neurons does not necessarily drive all but one to zero.
One form of competitive learning algorithm can be described as an
application of a successful single-neuron learning algorithm in a
network with lateral connections between adjacent neurons. The
lateral connections are needed so that each neuron can be inhibited
from adapting to a feature of the data already captured by other
neurons. A second family of algorithms uses the competition between neurons for improving, sharpening, or even forming the features extracted from the data by each single neuron. DATA CLUSTERING AND LEARNING emphasizes the related idea of data
clustering, discovering, and emphasizing structure that is hidden in
a data set (e.g., the pronounced similarity of groups of data vectors)
in an unsupervised fashion. There is a delicate trade-off: not to
superimpose too much structure, and yet not to overlook structure.
The choice of data representation predetermines what kind of cluster structures can be discovered in the data. Formulating the search
for clusters as an optimization problem then supports validation of
clustering results by checking that the cluster structures found in a
data set vary little from one data set to a second data set generated
by the same data source. The two tasks of clustering, density estimation and data compression, are tightly related by the fact that
the correct identification of the probability model of the source
yields the best code for data compression. PRINCIPAL COMPONENT
ANALYSIS shows how, in data compression applications like image
or speech coding, a distribution of input vectors may be economically encoded, with small expected values of the distortions, in
terms of eigenvectors of the largest eigenvalues of the correlation
matrix that describes the distribution of these patterns (these eigenvectors are the “principal components”). However, it is usually
not possible to find the eigenvectors on-line. The ideal solution is
then replaced by a neural network learning rule embodying a constrained optimization problem that converges to the solution given
by the principal components. INDEPENDENT COMPONENT ANALYSIS (ICA) is a linear transform of multivariate data designed to
make components of the resulting random vector as statistically
independent (factorial) as possible. In signal processing it is used
to attack the problem of the blind separation of sources, for example
of audio signals that have been mixed together by an unknown
process (the “cocktail party effect”). In the area of neural networks
and brain theory, it is an example of an information-theoretic unsupervised learning algorithm. When an ICA network is trained on
an ensemble of natural images, it learns localized-oriented receptive fields qualitatively similar to those found in area V1 of mammalian visual cortex. ICA has been used to decompose multivariate
brain data into components that help us understand task-related
spatial and temporal brain dynamics. Thus the same neural network
algorithm is being used both as an explanation of brain properties
and as a method of probing the brain. Where principal component
analysis (PCA) uses second-order statistics (the covariance matrix)

63

to remove correlations between the elements of a vector, ICA uses
statistics of all orders. PCA attempts to decorrelate the outputs,
while ICA attempts to make the outputs statistically independent.
The most widely used adaptive, on-line method for ICA is also the
most “neural-network-like” and is the one described in the body of
this article.
SELF-ORGANIZING FEATURE MAPS introduces the selforganizing feature map (SOFM or SOM; also known as a Kohonen
map), a nonlinear method by which features can be obtained with
an unsupervised learning process. It is based on a layer of adaptive
“neurons” that gradually develops into an array of feature detectors.
The linking of input signals to response locations in the map can
be viewed as a nonlinear projection from a signal or input space to
the (usually) 2D map layer. The learning method is an augmented
Hebbian method in which learning by the element most responsive
to an input pattern is “shared” with its neighbors. The result is that
the resulting “compressed image” of the (usually higher-dimensional) input space has the property of a topographic map that reflects important metric and statistical properties of the input signal
distribution: distance relationships in the input space (expressing,
e.g., pattern similarities) are approximately preserved as distance
relationships between corresponding excitation sites in the map,
and clusters of similar input patterns tend to become mapped to
areas of the neural array whose size varies in proportion to the
frequency of the occurrence of their patterns. This resembles in
many ways the structure of topographic feature maps found in
many brain areas, for which the SOFM offers a neural model that
bridges the gap between microscopic adaptation rules postulated at
the single neuron or synapse level and the formation of experimentally better accessible, macroscopic patterns of feature selectivity in neural layers. From a statistical point of view, the SOFM
provides a nonlinear generalization of principal component analysis and has proved valuable in many application contexts.
In order to give a quantitative answer to the question of how
well the trained network will be able to classify an input that it has
not seen before, it is common to assume that all inputs, both from
the training set and the test set, are produced independently and at
random. Clearly, the generalization error depends on the specific
algorithm that was used during the training, and its calculation
requires knowledge of the network weights generated by the learning process. In general, these weights will be complicated functions
of the examples, and an explicit form will not be available in most
cases. The methods of statistical mechanics provide an approach
to this problem, which often enables an exact calculation of learning curves in the limit of a very large network. In the statistical
mechanics approach one studies the ensemble of all networks that
implement the same set of input/output examples to a given accuracy. In this way the typical generalization behavior of a neural
network (in contrast to the worst or optimal behavior) can be described. We thus turn to two articles that apply the methods introduced in the article “Statistical Mechanics of Neural Networks”:
STATISTICAL MECHANICS OF ON-LINE LEARNING AND GENERALIZATION emphasizes on-line learning in which training examples
are dealt with one at a time, while STATISTICAL MECHANICS OF
GENERALIZATION emphasizes off-line or memory-based methods,
where learning is guided by the minimization of a cost function as
averaged over the whole training set. From a statistical physics
point of view, the distinction is between systems that can be
thought of as being in a state of thermal equilibrium (off-line 
on-equilibrium) and away-from-equilibrium situations where the
network is not allowed to extract all possible information from a
set of examples (on-line  off-equilibrium). While on-line learning
is an intrinsically stochastic process, the restriction to large networks, together with assumptions about the statistical properties of
the inputs, permits a concise description of the dynamics in terms
of coupled ordinary differential equations. These deterministic

64

Part II: Road Maps

equations govern the average evolution of quantities that completely define the macroscopic state of the ANN. The average is
taken with respect to the data, which is straightforward if the presented examples are statistically independent. The probability that
the network will make a mistake on the new input defines its generalization error for a given training set. Its average over many
realizations of the training set, as a function of the number of examples, gives the so-called learning curve. Calculation of the learning curve requires knowledge of the network weights generated by
the learning process, for which an explicit form will not be available in most cases. The methods of statistical mechanics provide
an approach to this problem, in many cases yielding an exact calculation of learning curves in the “thermodynamic limit” of a very
large network in which the network size increases in proportion to
the number of training examples, while the statistical or
information-theoretic approach is applicable to the learning curve
of a medium-size network (cf. LEARNING AND STATISTICAL
INFERENCE).
MODEL VALIDATION shows how the data analyst tries to infer a
“model” that summarizes functional dependencies that may be observed in a given set of empirical data. A good model fit should
reproduce the behavior of the studied system in the parameter range
to be explained by the model study. Model complexity has to be
controlled to avoid both missing essential features of the system
(underfitting) and adapting to irrelevant fluctuations in the data
(overfitting). Model validation provides the crucial step in modeling between model synthesis and analysis, assessing how appropriate the model is to gain insight into the real-world system. Model
validation can make use of bounds of the VC type (cf. “VapnikChervonenkis Dimension of Neural Networks”), which usually
contain a complexity term that accounts for the flexibility of the
hypothesis class and a fitting term that measures the contraction of
measure due to the large number of samples. It is shown how these
terms can be controlled either by numerical methods like crossvalidation and bootstrap or by analytical techniques from computational learning theory. The trade-off between model complexity
and goodness of fit and its relation to the computational complexity
of learning remains a deep challenge for research.
HIDDEN MARKOV MODELS describes the use of deterministic and
stochastic finite state automata for sequence processing, with special attention to hidden Markov models as tools for the processing
of complex piecewise stationary sequences. It also describes a few
applications of ANNs to further improve these methods. HMMs
allow complex sequential learning problems to be solved by assuming that the sequential pattern can be decomposed into piecewise stationary segments, with each stationary segment parameterized in terms of a stochastic function. The HMM is called “hidden”
because there is an underlying stochastic process (i.e., the sequence
of states) that is not observable but that affects the observed sequence of events.
TEMPORAL PATTERN PROCESSING notes that time is embodied
in a temporal pattern in two different ways: the temporal order
among the components of a sequence and the temporal duration of
the elements (see also “Sequence Learning”). A sequence is defined
as complex if it contains repetitions of the same subsequence, and
otherwise is simple. For the generation of complex sequences, the
correct successor can be determined only by knowing components
prior to the current one. We refer to the prior subsequence required
to determine the current component as the context of the component. Temporal processing requires that a neural network have a
capacity of short-term memory (STM) in order to maintain a component for some time. Time warping is challenging because we
would like to have invariance over limited warping, but dramatic
change in relative duration must be recognized differently. Another
fundamental ability of human information processing is chunking,
which, in the context of temporal processing, means that frequently
encountered and meaningful subsequences organize into chunks

that form basic units for further chunking at a higher level. TEMPORAL SEQUENCES: LEARNING AND GLOBAL ANALYSIS studies
how elementary pattern sequences may be represented in neural
structures at a low architectural and computational cost, seeking to
understand mechanisms to memorize spatiotemporal associations
in a robust fashion within model neural networks. The article focuses on formal neural networks where the interplay between neural and synaptic dynamics and, in particular, the role of transmission delays can be analyzed using methods from nonlinear
dynamics and statistical mechanics. Among the questions studied
are how to train a network so that its limit cycles will resemble
taught sequences. Such simplified systems are necessarily caricatures of biological structures yet suggest aspects that are important
for more elaborate approaches to real neural systems.
EVOLUTION OF ARTIFICIAL NEURAL NETWORKS adds another
temporal dimension to the biological process of adaptation,
namely, that of evolution. Rather than adapt the weights of a single
network to solve a problem in the network’s “lifetime,” the evolutionary approach applies the methodology of genetic algorithms
to evolve a population of neural networks over several generations
so that the population becomes better and better suited to some
computational ecology. EVOLUTION AND LEARNING IN NEURAL
NETWORKS extends this selection of networks on the basis of the
result of their adaptation to the environment through lifetime learning. The article shows how studies of ANNs that are subjected both
to an evolutionary and a lifetime learning process have been conducted to look at the advantages, in terms of performance, of combining two different adaptation techniques or to help understand
the role of the interaction between learning and evolution in natural
organisms.

Computability and Complexity
ANALOG NEURAL NETS: COMPUTATIONAL POWER
LEARNING AND GENERALIZATION: THEORETICAL BOUNDS
NEURAL AUTOMATA AND ANALOG COMPUTATIONAL COMPLEXITY
PAC LEARNING AND NEURAL NETWORKS
UNIVERSAL APPROXIMATORS
VAPNIK-CHERVONENKIS DIMENSION OF NEURAL NETWORKS
The 1930s saw the definition of an abstract notion of computability
when it was discovered that the set of functions on the natural
numbers, f : ⺞ r ⺞, computable by a Turing machine (an abstraction from following a finite set of rules to calculate on a finite but
extendible tape, each square of which could hold one of a fixed set
of symbols), lambda functions (which later came to be better
known as functions computable by programs written in LISP), and
general recursive functions (a class of functions obtained from very
simple numerical functions by repeated application of composition,
minimization, etc.), were identical. As general-purpose electronic
computers were developed and used in the 1940s and 1950s, it was
firmly established that these computable functions were precisely
the functions that could be computed by such computers with suitable programs, provided there were no limitations on computer
memory or computation time. This set the stage for the development of complexity theory in the 1960s and beyond: to chart the
different subsets of the computable functions that would be obtained when restrictions were placed on computing resources.
Many classification or pattern recognition tasks can be formulated as mappings between subsets of multidimensional vector
spaces by using a suitable coding of inputs and outputs, and many
types of feedforward networks are universal in the sense that, given
enough adjustable synaptic weights, they can approximate any
mapping between subsets of Euclidean spaces. UNIVERSAL APPROXIMATORS surveys recent developments in the mathematical
theory of feedforward networks and includes proofs of the universal
approximation capabilities of perceptron and radial basis function

II.7. Sensory Systems
networks with general activation and radial functions, and provides
estimates of rates of approximation. The article also characterizes
sets of multivariable functions that can be approximated without
the “curse of dimensionality,” which is an exponentially fast scaling of the number of parameters with the number of variables.
NEURAL AUTOMATA AND ANALOG COMPUTATIONAL COMPLEXITY explores what happens when the discrete operations of conventional automata theory are replaced by a computing model in
which operations on real numbers are treated as basic. Whereas
classical automata describe digital machines, neural models frequently require a framework of analog computation defined on a
continuous phase space, with a dynamics characterized by the existence of real constants that influence the macroscopic behavior
of the system. Moreover, unlike the flow in digital computation,
analog models do not include local discontinuities. Neural networks with real weights are more powerful than traditional models
of computation in that they can compute more functions within
given time bounds. However, the practicality of an approach based
on infinite precision real operations remains to be seen. Nonetheless, the new attention to real numbers has renewed complexity
theory and introduced many open problems in computational learning theory and neural network theory. The article thus pays special
attention to analog computation in the presence of noise. ANALOG
NEURAL NETS: COMPUTATIONAL POWER then analyzes the exact
and approximate representational power of feedforward and recurrent neural nets with synchronous update, with a brief discussion
of networks of spiking neurons and their relation to sigmoidal nets.
Learning complexity increases with increasing representational
power of the underlying neural model and care has to be exercised
to strike a balance between representational power on the one hand
and learning complexity on the other. However, the emphasis of
the article is on representational power, i.e., on what can be represented with networks using a given set of activation functions,
rather than on learning complexity. Splines (i.e., piecewise polynomial functions) have turned out to be powerful approximators,
and they are used here as the benchmark class of activation functions. Much attention is given to studying the properties that a class
of activation functions needs to reach the approximation power of
splines.
PAC LEARNING AND NEURAL NETWORKS discusses the “probably approximately correct” (PAC) learning paradigm as it applies
to ANNs. Roughly speaking, if a large enough sample of randomly drawn training examples is presented, then it should be

65

likely that, after learning, the neural network will classify most
other randomly drawn examples correctly. The PAC model formalizes the terms “likely” and “most.” The two main issues in
PAC learning theory are how many training examples should be
presented, and whether learning can be achieved using a fast algorithm. These are known, respectively, as the sample complexity
and computational complexity problems. PAC learning makes use
of the Vapnik-Chervonenkis dimension (VC-dimension) as a combinatorial parameter that measures the “expressive power” of a
family of functions. This parameter is described more fully in
VAPNIK-CHERVONENKIS DIMENSION OF NEURAL NETWORKS.
Bounds for the VC-dimension of a neural net N provide estimates
for the number of random examples that are needed to train N so
that it has good generalization properties (i.e., so that the error of
N on new examples from the same distribution is very small, with
probability very close to 1). Typically, the VC-dimension for a
class of networks grows polynomially (in many cases, between
linearly and quadratically) with the number of adjustable parameters of the neural network. In particular, if the number of training
examples is large compared to the VC-dimension, the network’s
performance on training data is a reliable indication of its future
performance on subsequent data. The bounds on training set size
tend to be large, since they provide generalization guarantees simultaneously for any probability distribution on the examples and
for any training algorithm that minimizes disagreement on the
training examples. Tighter bounds are available for some special
distributions and specific training algorithms. This theme is further
developed in LEARNING AND GENERALIZATION: THEORETICAL
BOUNDS in relation to three learning problems: pattern recognition, regression estimation, and density estimation. Because of the
looseness of its bounds as well as the difficulty of evaluating
them, VC theory was until recently largely neglected by practitioners. This has changed markedly with the development of support vector machines. Using nonlinear similarity measures, referred to as kernels, one can reduce a large class of learning
algorithms to linear algorithms in an associated feature space. For
the linear algorithms, a VC analysis can be carried out, identifying
precisely the factors that need to be controlled to achieve high
generalization ability in a variety of learning tasks. “Support Vector Machines” casts these factors into a convex optimization
framework, leading to efficient and mathematically well-founded
algorithms that have been shown to produce state-of-the-art results
on a large variety of problems.

II.7. Sensory Systems
Vision
ADAPTIVE RESONANCE THEORY
COLLICULAR VISUOMOTOR TRANSFORMATIONS FOR GAZE
CONTROL
COLOR PERCEPTION
CONTOUR AND SURFACE PERCEPTION
CORTICAL POPULATION DYNAMICS AND PSYCHOPHYSICS
DIRECTIONAL SELECTIVITY
DISSOCIATIONS BETWEEN VISUAL PROCESSING MODES
DYNAMIC LINK ARCHITECTURE
DYNAMIC REMAPPING
FACE RECOGNITION: NEUROPHYSIOLOGY AND NEURAL
TECHNOLOGY
FACE RECOGNITION: PSYCHOLOGY AND CONNECTIONISM
FAST VISUAL PROCESSING

FEATURE ANALYSIS
GABOR WAVELETS AND STATISTICAL PATTERN RECOGNITION
GLOBAL VISUAL PATTERN EXTRACTION
IMAGING THE VISUAL BRAIN
INFORMATION THEORY AND VISUAL PLASTICITY
KALMAN FILTERING: NEURAL IMPLICATIONS
LAMINAR CORTICAL ARCHITECTURE IN VISUAL PERCEPTION
MARKOV RANDOM FIELD MODELS IN IMAGE PROCESSING
MOTION PERCEPTION: ELEMENTARY MECHANISMS
MOTION PERCEPTION: NAVIGATION
NEOCOGNITRON: A MODEL FOR VISUAL PATTERN RECOGNITION
OBJECT RECOGNITION
OBJECT RECOGNITION, NEUROPHYSIOLOGY
OBJECT STRUCTURE, VISUAL PROCESSING
OCULAR DOMINANCE AND ORIENTATION COLUMNS
ORIENTATION SELECTIVITY

66

Part II: Road Maps

PERCEPTION OF THREE-DIMENSIONAL STRUCTURE
PROBABILISTIC REGULARIZATION METHODS FOR LOW-LEVEL
VISION
PURSUIT EYE MOVEMENTS
RETINA
SENSOR FUSION
STEREO CORRESPONDENCE
SYNCHRONIZATION, BINDING AND EXPECTANCY
TENSOR VOTING AND VISUAL SEGMENTATION
VISUAL ATTENTION
VISUAL CORTEX: ANATOMICAL STRUCTURE AND MODELS OF
FUNCTION
VISUAL SCENE PERCEPTION
VISUAL SCENE SEGMENTATION
The topic of Vision has provided one of the most fertile fields of
investigation both for brain theorists and for technologists constructing ANNs. Six articles in the road map Mammalian Brain
Regions—RETINA, COLLICULAR VISUOMOTOR TRANSFORMATIONS FOR GAZE CONTROL, “Thalamus,” VISUAL CORTEX: ANATOMICAL STRUCTURE AND MODELS OF FUNCTION, and VISUAL
SCENE PERCEPTION—introduce various brain regions associated
with vision. It is important to emphasize the role of “active vision”
in gaining information relevant for animals and robots considered
as real-time perception-action systems. This is a theme that is further developed in the road maps Neuroethology and Evolution
and Mammalian Motor Control. Nonetheless, many articles in
the present road map will analyze vision as the process of discovering from images what is present in the world: we may see active
vision as more like the mode of vision employed by the “where/
how” system described in VISUAL SCENE PERCEPTION, whereas
“passive” vision may be closer to the role of the “what” pathway.
DISSOCIATIONS BETWEEN VISUAL PROCESSING MODES explores
the notion that the visual system has two kinds of jobs to do. One
is to support visual cognition, the other is to drive visually guided
behavior. Qualitative information about location may be adequate
for cognition, but the sensorimotor function needs quantitative egocentrically calibrated spatial information to guide motor acts. The
article reviews evidence from neurophysiology, neurological analysis of patients, and psychophysics that the two systems should be
modeled as separate maps of visual space rather than as a single
visual representation with two readouts. Moreover, spatial information can flow from the cognitive to the sensorimotor representation, but not in the other direction.
However, even “passive” vision is not so passive, since attentional mechanisms are constantly moving the eyes to foveate on
items of particular relevance to the current interests of the organism. COLLICULAR VISUOMOTOR TRANSFORMATIONS FOR GAZE
CONTROL reviews the role of the superior colliculus in the control
of gaze shifts (combined eye-head movements) and its possible
involvement in the control of eye movements in 3D space (direction and depth). During attentive fixation, the “Vestibulo-Ocular
Reflex” (VOR) and slow vergence maintain binocular foveal fixation to correct for body movements. When the task requires inspection of an eccentric stimulus, a complex synergy of coordinated
movements comes into play. Such refixations typically involve a
rapid combined eye-head movement (saccadic gaze shift) and often
require binocular adjustment in depth (vergence). By virtue of its
topographical organization, the superior colliculus has become a
key area for experimental and modeling approaches to the question
of how sensory signals can be transformed into goal-directed movements. Interestingly, the superior colliculus is not driven by visual
input alone. Auditory and somatosensory cues are transformed to
register with the visual map in the colliculus for the control of
saccades. SENSOR FUSION picks up this theme of ways in which
sensory information can be brought together in the brains of diverse

animals (snakes, cats, monkeys, and humans) and surveys biologically inspired technological implementations (such as the use of
infrared to enhance vision). PURSUIT EYE MOVEMENTS takes us
from saccadic “jumps” to those smooth eye movements involved
in following a moving target. Current models of pursuit include
“image motion” models, “target velocity” models, and models that
address the role of prediction in pursuit. These models make no
explicit reference to the neural structures that might be responsible,
but the article analyzes the neural pathways for pursuit, stressing
the importance of both visual areas of the cerebral cortex and oculomotor regions of the cerebellum.
The RETINA, the outpost of the brain that contains both lightsensitive receptors and several layers of neurons that “preprocess”
these responses, transforms visual signals in a multitude of ways
to code properties of the visual world such as contrast, color, and
motion. The article suggests that much of the retina’s signal coding
and structural detail is derived from the need to optimally amplify
the signal and eliminate noise. But retinal circuitry is diverse. The
exact details are probably related to the ecological niche occupied
by the organism. In mammals, the retinal output branches into two
pathways, the collicular pathway and the geniculostriate pathway.
The destination of the former is the midbrain region known as the
superior colliculus, discussed above. VISUAL CORTEX: ANATOMICAL STRUCTURE AND MODELS OF FUNCTION reviews features of
the microcircuitry of the target of the geniculostriate pathway, the
primary visual cortex (area V1), and discusses the physiological
properties of cells in its different laminae. It then outlines several
hypotheses as to how the anatomical structure and connections
might serve the functional organization of the region. For example,
a connectionist model of layer IVc of V1 demonstrated that the
gradient of change in properties of the layer could indeed be replicated using dendritic overlap through the lower two-thirds of the
IVc layer. However, it was insufficient to explain the continuous
and sharply increasing field size and contrast sensitivity observable
near the top of the layer. The article shows how this discrepancy
led to new experiments and related changes in the model which
resulted in a good replication of the actual physiological data and
required only feedforward excitation. The article goes on to analyze
the anatomical substrates for orientation specificity and for surround modulation of visual responses, and concludes by discussing
the origins of patterned anatomical connections. OCULAR DOMINANCE AND ORIENTATION COLUMNS discusses further properties
of cells in layer IVc of V1. When these cells are tested to see which
eye drives them more strongly, it is found that ocular dominance
takes the form of a zebra-stripe-like pattern of alternating dominance. Within this high-level organization are “hypercolumns” devoted to a particular retinotopic region of visual space, each hypercolumn being further refined into columns whose cells are best
responsive to edges of the same specific orientation. The article
also presents models for how these structures might form through
self-organization during development. The article reviews data on
the orientation specificity of cells of V1 and their columnar organization, and offers models for the way in which development may
yield such features of cortical structure. GABOR WAVELETS AND
STATISTICAL PATTERN RECOGNITION shows how the response
properties of many cells in primary visual cortex may be better
described by what are called “Gabor wavelets” than as simple edge
detectors. Each Gabor wavelet responds best to patterns of a given
spatial frequency and orientation within a given neighborhood. The
article relates this notion to both biology and technology. The detection of edge information from within a visual scene is an essential component of visual processing. This processing is believed to
be initiated in the primary visual cortex, where individual neurons
are known to act as feature detectors of the orientation of edges
within the visual scene. Individual neurons can have an orientation
preference (which states that neuron’s preferred orientation of the

II.7. Sensory Systems
angle of edges) and orientation selectivity (which measures the
neuron’s sensitivity as a detector of orientation). ORIENTATION SELECTIVITY focuses on mechanisms of orientation selectivity in the
visual cortex, arguing that the orientation preference of each neuron
and the orderly orientation preference map in cortex are likely to
be consequences of a pattern of feedforward convergence. However, the selectivity observed in steady-state and orientation dynamics experiments cannot be achieved by a purely feedforward
model. Corticocortical inhibition is a crucial ingredient in the emergence of orientation selectivity in the visual cortex, while the relative importance of corticocortical excitation in enhancing orientation selectivity is still under investigation but appears to be more
significant for the function of complex cells than for simple cells
in V1. Moving beyond the orientation features of primary visual
cortex, INFORMATION THEORY AND VISUAL PLASTICITY demonstrates some aspects of information theory that are relevant to relaying information in cortex and connects entropy-based methods,
projection pursuit, and extraction of simple cells in visual cortex.
FEATURE ANALYSIS offers a more general view of the characterization of visual features based on the redundancy of the visual
signal and the transformation of the signal as it passes along the
visual pathway. Describing a particular cell as an “x detector” implies that the cell responds when and only when that particular
feature is present (e.g., an edge detector responds only in the presence of an edge), but the article argues that describing cells in the
early visual system as “detectors” of any type of feature is misleading. Features are useful for describing natural images because
the latter have massive informational redundancy. Image space itself is too vast to search directly. Feature analysis depends on the
proposition that the search for particular objects can be concentrated in a subspace of image space, the feature space. Localized
receptive fields in primary visual cortex provide the primitive basis
set for the feature space of vision. These form the basis for the
elaboration of neurons responding selectively to geometrical features in area TE of the inferotemporal cortex (IT), and these in turn
from the basis for object recognition in different but overlapping
areas of IT.
Given that cells in the early stages of the visual system, at least,
provide a distributed (more or less retinotopic) set of “features” (in
some suitably general sense, given the above caution, of patterns
that yield the best response rather than patterns that yield the only
response), the issue arises of how those features that correspond to
a single object in the visual scene are bound together. CONTOUR
AND SURFACE PERCEPTION introduces parallel interacting subsystems that follow complementary processing strategies. Boundary
formation proceeds by spatially linking oriented contrast measures
along smooth contour patterns, while perceptual surface attributes,
such as lightness or texture, are derived from local ratio measures
of image contrast of regions lying within contours. Mechanisms of
both subsystems mutually interact to resolve initial ambiguities and
to generate coherent representations of surface layout. Representations of intrinsic scene characteristics are constrained in terms of
the consistency of the set of solutions, which often involve smoothness assumptions for correlated feature estimates. These consistency constraints are typically based on the laws of physical image
generation. The article reviews fundamental approaches to computation of intrinsic scene characteristics and various neural models
of boundary and surface computation. Each model involves lateral
propagation of signals to interpolate and smooth sparse estimates.
ADAPTIVE RESONANCE THEORY (ART) bases learning on internal expectations. A pattern matching process (both for visual patterns and in other domains) compares an external input with the
internal memory code for various patterns. ART matching leads
either to a resonant state, which persists long enough to permit
learning, or to a parallel memory search. If the search ends at an
established code, the memory representation may either remain the

67

same or incorporate new information from matched portions of the
current input. When the external world fails to match an ART network’s expectations or predictions, a search process selects a new
category, representing a new hypothesis about what is important in
the present environment. LAMINAR CORTICAL ARCHITECTURE IN
VISUAL PERCEPTION uses the LAMINART model (an extension of
ART) to propose functional roles for cortical layers in visual perception. Neocortex has an intricate design that exhibits a characteristic organization into six distinct cortical layers, but few models
have addressed the functional utility of the laminar organization
itself in the control of behavior. LAMINART integrates data about
visual perception and neuroscience for such processes as preattentive grouping and attention. It is suggested that the functional roles
for cortical layers proposed here—binding together distributed cortical data through a combination of bottom-up adaptive filtering
and horizontal associations, and modulating it with top-down
attention—generalize, with appropriate specializations, to other
forms of sensory and cognitive processing.
CORTICAL POPULATION DYNAMICS AND PSYCHOPHYSICS models cortical population dynamics to explain dynamical properties
of the primate visual system on different levels, reaching from single neuron properties like selectivity for the orientation of a stimulus up to higher cognitive functions related to the binding and
processing of stimulus features in psychophysical discrimination
experiments. On the other hand, SYNCHRONIZATION, BINDING AND
EXPECTANCY argues that the “binding” of cells that correspond to
a given visual object may exploit another dimension of cellular
firing, namely, the phase at which a cell fires within some overall
rhythm of firing. The article presents data consistent with the proposal that the synchronization of responses on a time scale of milliseconds provides an efficient mechanism for response selection
and binding of population responses. Synchronization also increases the saliency of responses because it allows for effective
spatial summation in the population of neurons receiving convergent input from synchronized input cells. VISUAL SCENE SEGMENTATION tackles the segmentation of a visual scene into a set of
coherent patterns corresponding to objects. Objects appear in a natural scene as the grouping of similar sensory features and the segregation of dissimilar ones. Studies in visual perception, in particular Gestalt psychology, have uncovered a number of principles
for perceptual organization, such as proximity, similarity, connectedness, and relatedness in memory. Scene segmentation requires neural networks to address the binding problem. The temporal correlation approach is to encode the binding by the
correlation of temporal activities of feature-detecting cells. A special form of temporal correlation is oscillatory correlation, where
the basic units are neural oscillators. The article first reviews nonoscillatory approaches in scene segmentation, and then turns to
oscillatory approaches. The temporal correlation approach is further developed in DYNAMIC LINK ARCHITECTURE, which views the
brain’s data structure as a graph composed of nodes connected by
links, where both units and links bear activity variables changing
on the rapid time scale of fractions of a second. The nodes play the
role of symbolic elements. Dynamic links constitute the glue by
which higher data structures are built up from more elementary
ones.
Beyond the basic issue of how the visual scene is segmented
(how visual elements are grouped) into possibly meaningful wholes
lies the question of determining for a region so determined its color,
motion, distance, shape, etc. These issues are addressed in the next
set of articles. COLOR PERCEPTION stresses that color is not a local
property inferred from the wavelength of light hitting a patch of
retina but is a property of regions of space that depends both on
the light they reflect and on the surrounding context. Our visual
system “recreates” the world in the form of boundaries that contain
surfaces, and color perception involves the perception of aspects

68

Part II: Road Maps

of these surfaces. Matching surfaces with the same reflectance
properties in different parts of the visual scene or under different
illuminants are the two problems of color constancy. In addition,
wavelength signals can be used in the course of perceiving form
or motion independent of their role in the subjective experience of
color. DIRECTIONAL SELECTIVITY first reviews models of retinal
direction selectivity (which contributes to oculomotor responses
rather than motion perception). Older models depend on the way
in which amacrine and other cells of the retina are connected to the
ganglion cells, the retinal output cells. A newer model is based on
the directionality of synaptic interactions on the dendrites of amacrine cells, involving a spatial asymmetry in the inputs and outputs
of a dendrodendritic synapse, and its shunting inhibition. It is argued that development of this latter mechanism might involve Hebbian processes driven by spontaneous activity and light. Cortical
directional selectivity (which does contribute to motion perception
as well as the control of eye movements) involves many cortical
regions. Directionally sensitive cells in primary visual cortex (V1)
project to middle temporal cortex (MT) where directional selectivity becomes more complex, MT cells typically having larger receptive fields. From MT, the motion pathway projects to middle
superior temporal cortex. Cortical directional selectivity has been
modeled in three manners: as a spatially asymmetric excitatory
drive followed by multiplication or squaring, via a spatially asymmetric nonlinear inhibitory drive, and through a spatially asymmetric linear inhibitory drive followed by positive feedback. This
selectivity might involve Hebbian processes driven by spontaneous
activity and binocular interactions. The issues in this article have
some overlap with those presented in MOTION PERCEPTION: ELEMENTARY MECHANISMS, which emphasizes measurement of the
direction and speed of movement of features in the 2D image linking successive views to infer optic flow, which is the pattern of
image velocities that is projected onto the retina. The article discusses the cortical correlates of these various representations. MOTION PERCEPTION: NAVIGATION shows how, when an observer
moves through the world, the optic flow can inform him about his
own motion through space and about the 3D structure and motion
of objects in the scene. This information is essential for tasks such
as the visual guidance of locomotion through the environment and
the manipulation and recognition of objects. This article focuses
on the recovery of observer motion from optic flow. It includes
strategies for detecting moving objects and avoiding collisions, discusses how optic flow may be used to control actions, and describes
the neural mechanisms underlying heading perception. GLOBAL VISUAL PATTERN EXTRACTION continues the study of neural mechanisms which mediate between the extraction of local edge and
contour information by orientation-selective simple cells in primary visual cortex (V1) and the high levels of cortical form vision
in inferior temporal cortex (IT), where many neurons are sensitive
to complex global patterns, including objects and faces. The ventral
form vision pathway includes at least areas V1, V2, V4, TEO, and
TE (the highest level of IT), raising the question of what processes
occur at these intervening stages to transform local V1 orientation
information into global pattern representations. Essentially the
same question may be posed in cortical motion processing along
the dorsal pathway comprising V1, V2, MT, MST, and higher parietal areas. V1 neurons extract only local motion vectors perpendicular to moving edge segments, while MST neurons are sensitive
to complex optic flow patterns, including expansion. This article
suggests answers to these analogous questions about transitions
from local to global processing in both motion and form vision by
focusing on intermediate levels of these two pathways, mainly V4
and MST.
PERCEPTION OF THREE-DIMENSIONAL STRUCTURE reviews various computational models for inferring an object’s 3D structure
from different types of optical information, such as shading, tex-

ture, motion, and stereo, and examines how the performance of
these models compares with the capabilities and limitations of human observers in judging different aspects of 3D structure under
varying viewing conditions. In particular, stereoscopic vision exploits the fact that points in a 3D scene will in general project to
different positions in the images formed in the left and right eyes.
The differences in these positions are termed disparities. The stereo
correspondence problem is to identify which points in a pair of
stereo images correspond to a single point in 3D space. Solving
this problem allows the stereo pair to be mapped into a single
representation, called a disparity map, that makes explicit the disparities of various points common to both images, thus revealing
the distance of various visual elements from the observer. Depth
perception is then completed by determining depth values for all
points in the images. STEREO CORRESPONDENCE notes that various
constraints have been used to help determine which features on the
two eyes should be matched in inferring depth. These include compatibility of matching primitives, cohesivity, uniqueness, figural
continuity, and the ordering constraint. Various neural network
stereo correspondence algorithms are then reviewed, and the problems of surface discontinuities and uncorrelated points, and of
transparency, are addressed. The article also reviews neurophysiological studies of disparity mechanisms.
A more abstract approach to the correspondence problem, from
the perspective of computer vision rather than psychology or neurophysiology, is offered in TENSOR VOTING AND VISUAL SEGMENTATION. In 3D, as we have seen, surfaces are inferred from binocular images by obtaining depth hypotheses for points and/or
edges. In image sequence analysis, the estimation of motion and
shape starts with local measurements of feature correspondences,
which gives noisy data for the subsequent computation of scene
information. Hence, any salient structure estimator must be able to
handle the presence of multiple structures and their interaction in
the presence of noisy data. This article analyzes approaches to address early to midlevel vision problems, emphasizing the tensor
voting methodology for the robust inference of multiple salient
structures such as junctions, curves, regions, and surfaces from any
combination of points, curve elements, and surface patch element
inputs in 2D and 3D. The article describes two regularization formalisms, one that imposes certain physical constraints so that the
search space can be constrained and algorithmically tractable, and
another using a Bayesian formalism to transform an ill-posed problem into one of functional optimization.
PROBABILISTIC REGULARIZATION METHODS FOR LOW-LEVEL
VISION offers regularization theory (cf. “Generalization and Regularization in Nonlinear Learning Systems”) as a general mathematical framework to deal with the fact that the problem of inferring 3D structure from 2D images is ill-posed: there are many
spatial configurations compatible with a given 2D image or set
(motion sequence, stereo pair, etc.) of images. The issue then becomes to find which spatial configuration is most probable. We
have already seen a number of constraints associated with stereo
vision. Deterministic regularization theory defines a “cost function,” which combines a measure of how close a spatial configuration comes to yielding the given image (set) with a measure of
the extent to which the configuration violates the constraints, and
then seeks that configuration which minimizes this cost. The present article emphasizes a more general probabilistic approach in
which the “actual” field f and the observed field g are considered
as realizations of random fields, with the reconstruction of f understood as an estimation problem. MARKOV RANDOM FIELD MODELS IN IMAGE PROCESSING views the task of image modeling as
being one of finding an adequate representation of the intensity
distribution of a given image. What is adequate often depends on
the task at hand. The general properties of the local spatiotemporal
structure of images or image sequences are characterized by a Mar-

II.7. Sensory Systems
kov random field (MRF) in which the probability distribution for
the image intensity and a further set of other attributes (edges,
texture, and region labels) at a particular location are conditioned
on values in a neighborhood of pixels (picture elements or image
points). The observed quantities are usually noisy, blurred images.
The article presents five steps of MRF image modeling within
a Bayesian estimation/inference paradigm, and provides a number
of examples. Particular attention is paid to maximum a posteriori
(MAP) estimates. MRF image models have proved versatile
enough to be applied to image and texture synthesis, image restoration, flow field segmentation, and surface reconstruction.
KALMAN FILTERING: NEURAL IMPLICATIONS introduces Kalman
filtering, which, under linear and Gaussian conditions, produces a
recursive estimate of the hidden state of a dynamic system, i.e.,
one that is updated with each subsequent (noisy) measurement of
the observed system. The article shows how Kalman filtering provides insight into visual recognition and the role of the cerebellum
in motor control. In particular, it presents a hierarchically organized
neural network for visual recognition, with each intermediate level
of the hierarchy receiving two kinds of information: bottom-up
information from the preceding level, and top-down information
from the higher level. For its implementation, the model uses a
multiscale estimation algorithm that may be viewed as a hierarchical form of the extended Kalman filter that is used to simultaneously learn the feedforward, feedback, and prediction parameters
of the model on the basis of visual experiences in a dynamic environment. The resulting adaptive process involves a fast dynamic
state-estimation process that allows the dynamic model to anticipate incoming stimuli, as well as a slow Hebbian learning process
that provides for synaptic weight adjustments in the model.
IMAGING THE VISUAL BRAIN addresses functional brain imaging
of visual processes, with emphasis on limits in spatial and temporal
resolution, constraints on subject participation, and trade-offs in
experimental design. The articles focuses on retinotopy, visual motion perception and visual object representation, and voluntary
modulation of attention and visual imagery, emphasizing some of
the areas where modeling and brain theory might be testable using
current imaging tools. VISUAL ATTENTION offers data and hypotheses for cortical mechanisms to overtly and covertly shift attention
(i.e., with and without eye movements). Attention guides where to
look next based on both bottom-up (image-based) and top-down
(task-dependent) cues—and indeed, the anatomy of the visual system includes extensive feedback connections from later stages and
horizontal connections within each layer. Vision appears to rely on
sophisticated interactions between coarse, massively parallel, fullfield preattentive analysis systems and the more detailed, circumscribed, and sequential attentional analysis system. The articles
focus on the brain area involved in visual attention and then analyzes a variety of relevant mechanisms. Yet, having stressed the
way in which we normally take a number of shifts of attention to
fully take in the details of a visual scene, it is intriguing to learn
how much can be absorbed in a single fixation. FAST VISUAL PROCESSING notes that much information can be extracted from briefly
glimpsed scenes, even at presentation rates of around 10 frames/s,
a technique known as rapid sequential visual presentation (RSVP).
Since interspike intervals for neurons are seldom shorter than 5 ms,
the underlying algorithms should involve no more than about 20
sequential, though massively parallel, steps. There is an important
distinction in neural computation between feedforward processing
models and those with recurrent connections that allow feedback
and iterative processing. Pure feedforward models (e.g., multilayer
perceptrons, MLPs) can operate very quickly in parallel hardware.
The article argues that even in systems that use extensive recurrent
connections, the fastest behavioral responses may essentially depend on a single feedforward processing wave. It looks at how
detailed measurements of processing speed can be combined with

69

anatomical and physiological constraints to constrain models of
how the brain performs such computations.
There is a vast literature on pattern recognition in neural networks (see, for example, “Pattern Recognition” and “Concept
Learning”). Here we discuss articles on face recognition and object
recognition. The recognition of other individuals, and in particular
the recognition of faces, is a major prerequisite for human social
interaction and indeed has been shown to employ specific brain
mechanisms. The ability to recognize people from their faces is
part of a spectrum of related skills that include face segmentation
(i.e., finding faces in a scene or image) and estimation of the pose,
direction of gaze, and the person’s emotional state. FACE RECOGNITION: NEUROPHYSIOLOGY AND NEURAL TECHNOLOGY starts with
a review of relevant neurophysiology. Brain injury can lead to prosopagnosia, the loss of ability to recognize individual faces, while
leaving intact the ability to recognize general objects. Single-unit
recordings in the IT cortex of macaque monkeys have revealed
neurons with a high responsiveness to the presence of a face, an
individual, or the expression on the face, and neural models for
face recognition are reviewed in relation to such data. The article
then focuses on computational theories that are inspired by neural
ideas (see DYNAMIC LINK ARCHITECTURE; GABOR WAVELETS AND
STATISTICAL PATTERN RECOGNITION) but that find their justification in the construction of successful computer systems for the
recognition of human faces even when the gallery of possible faces
is very large indeed. FACE RECOGNITION: PSYCHOLOGY AND CONNECTIONISM provides a brief history of connectionist approaches
to face recognition and surveys the broad range of tasks to which
these models have been applied. The article relates the models to
psychological theories for the subtasks of representing faces and
retrieving them from memory, comparing human and model performance along these dimensions.
OBJECT RECOGNITION focuses on models of viewpoint-invariant
object recognition that are constrained by psychological data on
human object recognition. It present three main approaches to object recognition—invariant based, model based, and appearance
based—and analyzes the strengths of each of these in a framework
of decision complexity, noting the trade-off between representations that emphasize invariance and those designed for discriminability. The analysis shows that it is unlikely for a single form of
representation to satisfy all kinds of object recognition tasks a human or other visual animal may encounter. The article thus argues
that a key ingredient in a comprehensive brain theory for object
recognition is a computational framework that allows on-demand
selection or adaptation of representations based on the current task
and proposes a simple “first past the post” scheme (a temporal
winner-take-all scheme) for self-selecting the most appropriate
level of abstraction, given a finite set of available representations
along a visual processing pathway.
OBJECT STRUCTURE, VISUAL PROCESSING emphasizes structureprocessing tasks that call for separate treatment of various fragments of the visual stimulus, each of which spans only a fraction
of the visual extent of the object or scene under consideration.
Examples of structural tasks include recognition of part-part similarities, and identifying a region in an object toward which an
action can be directed. After discussing object form processing in
computer vision and relevant neurophysiological data on primate
vision, the article focuses on two neuromorphic models of visual
structure processing. The JIM model implements a recognition-bycomponents scenario based on geons (“geometrical elements,”
which are generalized cylinders formed by moving a cross-section
along a possibly curved axis). The Chorus of Fragments model
exploits both the “what” and the “where” streams of visual cortex
to recognize fragments no matter what their position, but then uses
their approximate spatial relationships to see whether they together
form cues for the recognition of an object. In particular, then, it

70

Part II: Road Maps

avoids the binding problem of explicitly linking neural activity
related to a specific object as a prerequisite to analysis of that object’s characteristics. (By contrast, SYNCHRONIZATION, BINDING
AND EXPECTANCY argues that the brain does solve the binding
problem, and does so by synchronization of neural firing for those
neurons related to a single object.)
OBJECT RECOGNITION, NEUROPHYSIOLOGY reviews some theoretical approaches to object recognition in the context of mainly
neurophysiological evidence. It also considers briefly the analysis
of visual scenes. Scene analysis is relevant to object recognition
because scenes may themselves be recognized initially at a holistic,
object-like level, providing a context or “gist” that influences the
speed and accuracy of recognition of the constituent objects. The
article proposes that object recognition is based on a distributed,
view-based representation in which objects are recognized on the
basis of multiple, 2D-feature-selective neurons. Specialist cells appear to play a role in associating such feature combinations into
certain nontrivial image transformations, coding for a certain percentage of all stimuli in a largely view-invariant manner. The article
offers evidence that a convergent hierarchy is used to build invariant representations over several stages, and that at each stage lateral competitive processes are at work between the neurons. It is
argued that the association of views of objects observed over the
course of time could play a key role in building up object representations. The review focuses mainly on the “what” stream of IT
cortex, seen as the center of object recognition. VISUAL SCENE
PERCEPTION also brings in the “where/how” stream of parietal cortex as it analyzes how mechanisms that integrate schemas for recognition of different objects into the perception of some overall
scene may be linked to the distributed planning of action. It also
presents recent neurophysiology suggesting how the context of a
natural scene may modify the response properties of neurons responsive to visual features. The article compares three approaches—the slide-box metaphor, short-term memory in the VISIONS system, and the visuospatial scratchpad—for creating a
theory of how the visual perception of objects may be integrated
with the perception of spatial layout. The first two stress a schematheoretic approach, while the latter is strongly tied to visual neurophysiology and modeling in terms of quasi-neural attractor networks. The aim is to open the way to future research that will
embed the study of visual scene perception in an action-oriented
integration of IT and parietal visual systems.

Other Sensory Systems
AUDITORY CORTEX
AUDITORY PERIPHERY AND COCHLEAR NUCLEUS
AUDITORY SCENE ANALYSIS
ECHOLOCATION: COCHLEOTOPIC AND COMPUTATIONAL MAPS
ELECTROLOCATION
OLFACTORY BULB
OLFACTORY CORTEX
PAIN NETWORKS
PROSTHETICS, SENSORY SYSTEMS
SENSOR FUSION
SOMATOSENSORY SYSTEM
SOMATOTOPY: PLASTICITY OF SENSORY MAPS
SOUND LOCALIZATION AND BINAURAL PROCESSING
Here we analyze sensory systems other than vision—e.g., touch,
audition, and pain. Moreover, when one sense cannot provide all
the necessary information, complementary observations may be
provided by another sense. For example, touch complements vision
in placing a peg in a hole when the effector occludes the agent’s
view. Also, senses may offer competing observations, such as the
competition between vision and the vestibular system in maintain-

ing balance (and its occasional side effect of seasickness). Another
type of interplay between the senses is the use of information extracted by one sense to focus the attention of another sense, coordinating the two, as in audition cueing vision. SENSOR FUSION explores a number of ways sensory information is brought together
in the brains of diverse animals (snakes, cats, monkeys, humans)
and surveys biologically inspired technological implementations
(such as the use of infrared to enhance vision). (See also “Collicular
Visuomotor Transformations for Gaze Control” for an important
example of sensor fusion—the transformation of auditory and somatosensory cues into a visual map for the control of rapid eye
movements.)
The road map Mammalian Brain Regions introduced a number
of regions linked to sensory systems other than vision, but we will
now meet a number of related and additional topics as well. SOMATOSENSORY SYSTEM shows how the somatosensory system
changes the tactile stimulus representation from a form more or
less isomorphic to the stimulus to a completely distributed form in
a series of partial transformations in successive subcortical and
cortical networks. It further argues that the causal factors involved
in body/object interactions are explicitly represented by an internal
model in the pyramidal cells of somatosensory cortex that is crucial
for haptic perception of proximal surroundings and for control of
object manipulation. Somatotopy, a dominant feature of subdivisions of the somatosensory system, is defined by a topographic
representation, or map, in the brain of sensory receptors on the body
surface. SOMATOTOPY: PLASTICITY OF SENSORY MAPS shows that
these orderly representations of cutaneous receptors in the spinal
cord, lower brainstem, thalamus, and neocortex represent both the
peripheral distribution of receptors and dynamic aspects of brain
function. The article reviews evidence for somatosensory plasticity
involving cortical reorganization after peripheral injury and as a
result of training. The article analyzes the features of somatotopic
maps that change, the contribution of subcortical changes to cortical plasticity, the mechanisms involved, and the functional consequences of sensory map changes. An important issue is the relation between the plasticity of the sensory and motor systems.
PAIN NETWORKS adds a new dimension to bodily sensation. The
pain system encodes information on the intensity, location, and
dynamics of tissue-threatening stimuli but differs from other sensory systems in its “emotional-motivational” factors (see also “Motivation”). In the pain system, these factors strongly modulate the
relation between stimulus and felt response. At one extreme is allodynia, a state in which the slightest touch with a cotton wisp is
agonizing. People display wide individual and trial-to-trial variability in the amount of pain reported following administration of
calibrated noxious stimuli; pain sensation is subject to ongoing
modulation by a complex of extrinsic (stimulus-generated) and intrinsic (CNS-generated) state variables. The article spells out how
these act in the CNS as well as the periphery.
AUDITORY PERIPHERY AND COCHLEAR NUCLEUS spells out how
the auditory periphery parcels out acoustic stimulus across hundreds of nerve fibers, and how the cochlear nucleus continues this
process by creating multiple representations of the original acoustic
stimulus. The article emphasizes monaural signal processing,
whereas SOUND LOCALIZATION AND BINAURAL PROCESSING
shows how information from the two ears is brought together. The
article focuses on the use of interaural time difference (ITD) as one
way to estimate the azimuthal angle of a sound source. It describes
one biological model (ITD detection in the barn owl’s brainstem)
and two psychological models. The underlying idea is that the brain
attempts to match the sounds in the two ears by shifting one sound
relative to the other, with the shift that produces the best match
assumed to be the one that just balances the “real” ITD. AUDITORY
CORTEX stresses the crucial role that auditory cortex plays in the
perception and localization of complex sounds, examining auditory

II.8. Motor Systems
tasks vital for all mammals, such as sound localization, timbre recognition, and pitch perception. AUDITORY SCENE ANALYSIS discusses how the auditory system parses the acoustic mixture that
reaches the ears of an animal to segregate a targeted sound source
from the background of other sounds. The first stage, segmentation,
decomposes the acoustic mixture into its constituent components.
In the second stage, acoustic components that are likely to have
arisen from the same environmental event are grouped, forming a
perceptual representation (stream) that describes a single sound
source. At the physiological level, segmentation corresponds (at
least in part) to peripheral auditory processing, which performs a
frequency analysis of the acoustic input, whereas the physiological
substrate of auditory grouping is much less well understood. The
article focuses on models that are at least physiologically plausible,
while noting that other models of auditory scene analysis adopt a
more abstract information processing perspective.
ECHOLOCATION: COCHLEOTOPIC AND COMPUTATIONAL MAPS
provides us with a more detailed understanding of the auditory
system in a very special class of mammals, the bats. Mustached
bats emit echolocation (ultrasonic) pulses for navigation and for
hunting flying insects. On the basis of the echo, prey must be detected and distinguished from the background clutter of vegetation,
characterized as appropriate for consumption, and localized in
space for orientation and prey capture. The bats emit ultrasonic
pulses that consist of a long constant-frequency component followed by a short frequency-modulated component. Each pulseecho combination provides a discrete sample of the continuously
changing auditory scene. The auditory network contains two key
design features: neurons that are sensitive to combinations of pulse
and echo components, and computational maps that represent systematic changes in echo parameters to extract the relevant
information.
Electrolocation is another sense that helps the animal locate
itself in its world, but this time the animals are electric fishes
rather than bats, and the signals are electrical rather than auditory.
ELECTROLOCATION relates its topic to the general issue of mechanisms that facilitate the processing of relevant signals while rejecting noise, and of attentional processes that select which stimuli
are to be attended to. Weakly electric fish generate an electrical
field around their body and measure this field via electroreceptors
embedded in the skin to “electrolocate” animate or inanimate targets in the environment. The article emphasizes a widespread but
poorly understood characteristic of sensory processing circuits,
namely, the presence of massive descending or feedback connections by which higher centers presumably modulate the operation
of lower centers. Not only are response gain and receptive field

71

organization controlled by these descending connections, but there
are adaptive filtering mechanisms that can reject stimuli that otherwise might mask critical functions. This use of stored sensory
expectations for the cancellation or perhaps the identification of
specific input patterns may yield insights into diverse neural circuits, including the cochlear nuclei and the cerebellum, in other
species.
Two articles introduce data and models for the olfactory system
(see also the road map Mammalian Brain Regions). OLFACTORY
BULB describes the special circuitry involved in basic preprocessing, while OLFACTORY CORTEX presents a dynamical systems analysis of further olfactory processing. The olfactory bulb receives
input from the sensory neurons in the olfactory epithelium and
sends its outputs to the olfactory cortex, among other brain regions.
The bulb was one of the first regions of the brain for which compartmental models of neurons were constructed, which led to some
of the first computational models of functional microcircuits. OLFACTORY BULB gives an overview of olfactory bulb cells and circuits, current ideas about the computational functions of the bulb,
and modeling studies to investigate these functions. The olfactory
cortex is defined as the region of the cerebral cortex that receives
direct connections from the olfactory bulb. It is the earliest cortical
region to differentiate in the evolution of the vertebrate forebrain
and the only region within the forebrain to receive direct sensory
input. Moreover, the olfactory cortex has the simplest organization
among the main types of cerebral cortex. OLFACTORY CORTEX thus
views it as a model for understanding basic principles underlying
cortical organization.
Finally, a very different view of sensory systems is provided by
PROSTHETICS, SENSORY SYSTEMS, which discusses how information collected by electronic sensors may be delivered directly to
the nervous system by electrical stimulation. After assessing the
amenability of all sensory modalities (hearing, vision, touch, proprioception, balance, smell, and taste), the article focuses on auditory and visual prostheses. The great success story has been with
cochlear implants. Here the article reviews improved temporospatial representations of speech sounds, combined electrical and
acoustic stimulation in patients with residual hearing, and psychophysical correlates of performance variability. Since a prosthesis
does not necessarily match natural neural encoding of a stimulus,
the success of the prosthesis depends in part on the plasticity of the
human brain as it remaps to accommodate this new class of signals.
For example, the success of cochlear implants rests in part on the
ability of auditory cortex to remap itself in a similar fashion to the
remapping of somatosensory cortex described in SOMATOTOPY:
PLASTICITY OF SENSORY MAPS.

II.8. Motor Systems
Robotics and Control Theory
ARM AND HAND MOVEMENT CONTROL
BIOLOGICALLY INSPIRED ROBOTICS
IDENTIFICATION AND CONTROL
MOTOR CONTROL, BIOLOGICAL AND THEORETICAL
POTENTIAL FIELDS AND NEURAL NETWORKS
Q-LEARNING FOR ROBOTS
REACTIVE ROBOTIC SYSTEMS
REINFORCEMENT LEARNING IN MOTOR CONTROL
ROBOT ARM CONTROL
ROBOT LEARNING

ROBOT NAVIGATION
SENSORIMOTOR LEARNING
As noted in the “Historical Fragment” section of Part I, the interchange between biology and technology that characterizes the
study of neural networks is an outgrowth of work in cybernetics in
the 1940s. One of the keys to cybernetics was control (the other
was communication of the kind studied in information theory). It
is thus appropriate that control theory should have become a major
application area for neural networks as well as being a key concept
of brain theory. The objective of control is to influence the behavior
of a dynamical system in some desired fashion. The latter includes

72

Part II: Road Maps

maintaining the outputs of systems at constant values (regulation)
or forcing them to follow prescribed time functions (tracking).
Maintaining the altitude of an aircraft or the glucose level in the
blood at constant values are examples of regulation; controlling a
rocket to follow a given trajectory is an example of tracking. MOTOR CONTROL, BIOLOGICAL AND THEORETICAL sets forth the basic
cybernetic concepts. A motor control system acts by sending motor
commands to a controlled object, often referred to as “the plant,”
which in turn acts on the local environment. The plant or the environment has one or more variables which the controller attempts
to regulate. If the controller bases its actions on signals which are
not affected by the plant output, it is said to be a feedforward
controller. If the controller bases its actions on a comparison between desired behavior and the controlled variables, it is a feedback
controller. “Motor Pattern Generation” provides a related perspective (see the road map Motor Pattern Generators).
The major advantage of negative feedback control, in which the
controller seeks constantly to cancel the feedback error, is that it
is a very simple, robust strategy that operates well without exact
knowledge of the controlled object, and despite internal or external
disturbances. The advantage of feedforward control is that it can,
in the ideal case, give perfect performance with no error between
the reference and the controlled variable. The main disadvantages
are the practical difficulties in developing an accurate controller,
and the lack of corrections for unexpected disturbances. IDENTIFICATION AND CONTROL explores the major strategy for developing
an accurate controller, namely to “identify” the plant as belonging
to (or more precisely, being well approximated by) a system obtained from a general family of systems by setting a key set of
parameters (e.g., the coefficients in the matrices of a linear system).
By coupling a controller to an identification procedure, one obtains
an adaptive controller that can handle an unknown plant even if its
dynamics are (slowly) changing. In both biology and many technological applications, nonlinearities and uncertainties play a major
role, and linear approximations are not satisfactory. The article
presents research using neural networks to handle these nonlinearities and examines the theoretical assumptions that have to be made
when such networks are used as identifiers and controllers.
REINFORCEMENT LEARNING IN MOTOR CONTROL recalls the
general theory introduced in “Reinforcement Learning” and proceeds to note its utility in motor control. Many motor skills are
attained in the absence of explicit feedback about muscle contractions or joint angles. In contrast to supervised learning, such learning depends on “reinforcement” (or evaluative feedback; it need
not involve pleasure or pain), which tells the learner whether or
not, and possibly by how much, its behavior has improved, or provides an indication of success or failure. Instead of trying to match
a standard of correctness, a reinforcement learning system tries to
maximize the goodness of behavior as indicated by evaluative feedback. To do this, it has to actively try alternatives, compare the
resulting evaluations, and use some kind of selection mechanism
to guide behavior toward the better alternatives. Q-LEARNING FOR
ROBOTS applies reinforcement learning techniques to robot control.
Q-learning does not require a model of the robot-world interaction,
and it uses learning examples in the form of triplets (situation,
action, Q-value), where the Q-value is the utility of executing the
action in the situation. Q-learning involves three different functions, evaluation, memorization, and updating. Heuristically
adapted Q-learning has proved successful in applications such as
obstacle avoidance, wall following, go-to-the-nest, etc., using
neural-based implementations such as multilayer perceptrons
trained with backpropagation, or self-organizing maps.
SENSORIMOTOR LEARNING explains how neural nets can acquire
“models” of some desired sensorimotor transformation. A forward
model is a representation of the transformation from motor commands to movements, in other words, a model of the controlled

object. An inverse model is a representation of the transformation
from desired movements to motor commands, and so can be used
as the controller for the controlled object. The managing of multiple
models, each with their own range of applicability in given tasks,
is given special attention. ROBOT LEARNING focuses on learning
robot control, the process of acquiring a sensorimotor control strategy for a particular movement task and movement system. The
article offers a formal framework within which to discuss robot
learning in terms of the different methods that have been suggested
for the learning of control policies, such as learning the control
policy directly, learning the control policy in a modular way, indirect learning of control policies, imitation learning, and learning
of motor control components. The article also reviews specific
function approximation problems in robot learning, including neural network approaches. ROBOT ARM CONTROL addresses related
issues concerning the availability of precise mappings from physical space or sensor space to joint space or motor space. Robot arm
controllers are usually hierarchically structured from the lowest
level of servomotors to the highest levels of trajectory generation
and task supervision. In each case an actual motion is made to
follow as closely as possible a commanded motion through the use
of feedback. The difference lies in the coordinate systems used at
each level. At least four coordinate spaces can be distinguished:
the task space (used to specify tasks, possibly in terms of sensor
readings), the workspace (6D Cartesian coordinates defining a position and orientation of the end-effector), the joint space (intrinsic
coordinates determining a robot configuration), and the actuator
space (in which actual motions are commanded). Correlational procedures carry out feature discovery or clustering and are often used
to represent a given state space in a compact and topologypreserving manner, using procedures such as those described in
“Self-Organizing Feature Maps.” Error-minimization procedures
require explicit data on input-output pairs; their goal is to build a
mapping from inputs to outputs that generalizes adequately using,
e.g., the least-mean-squares (LMS) rule and backpropagation. In
between both extremes lie procedures that use reinforcement learning to build a mapping that maximizes reward. ARM AND HAND
MOVEMENT CONTROL discusses some of the most prominent regularities of arm and hand control, and examines computational and
neural network models designed to explain them. The analysis reveals an interesting competition between explanations sought on
the neural, biomechanical, perceptual, and computational levels
that has created its share of controversy. Whereas some topics, such
as internal model control, have gained solid grounding, the importance of the dynamic properties of the musculoskeletal system in
facilitating motor control, the role of real-time perceptual modulation of motor control, and the balance between dynamical systems
models versus optimal control-based models are still seen as offering many open questions.
BIOLOGICALLY INSPIRED ROBOTICS describes how modern robotics may learn from the way organisms are constructed biologically and how this creates adaptive behaviors. (I cannot resist noting here the acronym introduced by R. I. Damper, R. L. B. French,
and T. W. Scutt, 2000, ARBIB: An Autonomous Robot Based on
Inspiration from Biology, Robotics and Autonomous Systems,
31:247–274.) Research on autonomous robots based on inspiration
from biology ranges from modeling animal sensors in hardware to
guiding robots in target environments to investigating the interaction between neural learning and evolution in a variety of robot
tasks. After reviewing the historical roots of the subject, the article
provides a general introduction to biologically inspired robotics,
with special emphasis on the ideas that the robot is situated in the
world and that many complex behaviors are emergent properties
of the collective effects of linking a variety of simple behaviors.
REACTIVE ROBOTIC SYSTEMS provides a conceptual framework for
robotics that is rooted in “Schema Theory” (q.v.) rather than sym-

II.8. Motor Systems
bolic AI. Here, robot behavior is controlled by the activation of a
collection of low-level primitive behaviors (schemas), and complex
behavior emerges through the interaction of these schemas and the
complexities of the environment in which the robot finds itself.
This work was inspired in part by studies of animal behavior (see,
e.g., “Neuroethology, Computational” and related articles discussed in the road maps on Motor Pattern Generators and Neuroethology and Evolution). However, the article not only shows
the power of reactive robots in many applications, it also notes the
utility of hybrid systems capable of using deliberative reasoning as
well as reactive execution (which fits with an evolutionary view of
the human brain in which reactive systems handle many functions
but can be overruled or orchestrated by, e.g., the deliberative activities of prefrontal cortex).
ROBOT NAVIGATION examines how to get a mobile robot to
move to its destination efficiently (e.g., along short trajectories)
and safely (i.e., without colliding). If a target location is either
visible or identified by a landmark (or sequence of landmarks), a
simple stimulus-response strategy can be adopted. However, if targets are not visible, the robot needs a model (or map) of the environment encoding the spatial relationships between its present
and desired locations. Sensor uncertainty, together with the inaccuracy of the robot’s actuators and the unpredictability of real environments, makes the design of mobile robot controllers a difficult
task. It has thus proved desirable to endow robots with learning
capabilities in order to acquire autonomously their control system
and to adapt their behavior to never experienced situations. The
article thus reviews neural approaches to localization, map building, and navigation. More specifically, POTENTIAL FIELDS AND
NEURAL NETWORKS examines biological findings on the use of
potential fields (which represent, e.g., the force field that drives the
motor output of an animal or part of an animal, such as a limb) to
characterize the control and learning of motor primitives. The notion of potential fields has also been used to model externally induced constraints as well as internally constructed sensorimotor
maps for robot motion control. A robot can reach a stable configuration in its environment by following the negative gradient of its
potential field. In this case, the configurations reached will be locally stable but may not be optimal with respect to some behavioral
criterion. This deficit can be overcome either by incorporating a
global motion planner or by using a harmonic function that does
not contain any local minima. The article further indicates how
potential field–based motion control can benefit from the use of
ANN-based learning. There are links here to the more biological
concerns of the articles “Cognitive Maps,” “Hippocampus: Spatial
Models,” and “Motor Primitives.”

Motor Pattern Generators
CHAINS OF OSCILLATORS IN MOTOR AND SENSORY SYSTEMS
COMMAND NEURONS AND COMMAND SYSTEMS
CRUSTACEAN STOMATOGASTRIC SYSTEM
GAIT TRANSITIONS
HALF-CENTER OSCILLATORS UNDERLYING RHYTHMIC
MOVEMENTS
LOCOMOTION, INVERTEBRATE
LOCOMOTION, VERTEBRATE
LOCUST FLIGHT: COMPONENTS AND MECHANISMS IN THE MOTOR
MOTOR PATTERN GENERATION
MOTOR PRIMITIVES
RESPIRATORY RHYTHM GENERATION
SCRATCH REFLEX
SENSORIMOTOR INTERACTIONS AND CENTRAL PATTERN
GENERATORS
SPINAL CORD OF LAMPREY: GENERATION OF LOCOMOTOR
PATTERNS

73

MOTOR PATTERN GENERATION provides an overview of the basic
building blocks of behavior (see “Motor Control, Biological and
Theoretical” for more general background) to be expanded upon in
many of the following articles. The emphasis is on rhythmic behaviors (such as flight or locomotion), but a variety of “one-off”
motor patterns (as typified in a frog snapping at its prey) are also
studied. The crucial notion is that a central pattern generator (CPG),
an autonomous neural circuit, can yield a good “sketch” of a movement, but that the full motor pattern generator (MPG) augments
the CPG with sensory input which can adjust the motor pattern to
changing circumstances (e.g., the pattern of locomotion varies
when going uphill rather than on level terrain, or when the animal
carries a heavy load). SENSORIMOTOR INTERACTIONS AND CENTRAL PATTERN GENERATORS discusses both the impact of sensory
information on CPGs and the influence of motor systems on sensory activity. It stresses that interaction between motor and sensory
systems is pervasive, from the first steps of sensory detection to
the highest levels of processing, emphasizing that descending motor commands are only acted upon by spinal circuits when these
circuits integrate their intrinsic activity with all incoming
information.
COMMAND NEURONS AND COMMAND SYSTEMS analyzes the extent to which an MPG may be activated alone or in concert with
others through perceptual stimuli mediated by a single “command
neuron” or by more diffuse “command systems.” Command functions provide the sensorimotor interface between sensory pattern
recognition and localization, on the one side, and motor pattern
generation on the other. For example, if a certain interneuron is
stimulated electrically in the brain of a marine slug, the animal then
displays a species-specific escape swimming behavior, although no
predator is present. If in a toad a certain brain area of the optic
tectum is stimulated in this manner, snapping behavior is triggered,
although no prey is present. In both cases, a stimulus produces a
rapid ballistic response.
MOTOR PRIMITIVES and SCRATCH REFLEX look at two behaviors
(the former studied in frogs, the latter primarily in turtles) elicited
by an irritant applied to the animal’s skin. In each case, the position
at which the limb is aimed varies with the position of the irritant;
there is somatotopic (i.e., based on place on the body) control of
the reflex. In both frog and turtle, and thus more generally, spinal
cord neural networks can by themselves generate complex sensorimotor transformations even when disconnected from supraspinal
structures. Moreover, each reflex has different “modes.” To understand this, just think of scratching your lower back. As the scratch
site moves higher, the positioning of the limb changes continuously
with the position of the irritant until the irritant moves up so much
that you make a discontinuous switch to the “over-the-shoulder”
mode of back-scratching. The mode changes in these two articles
may be compared to the GAIT TRANSITIONS (q.v.), discussed below.
In any case, we see here two important issues: how is an appropriate
pattern of action chosen, and how is the chosen pattern parameterized on the basis of sensory input? MOTOR PRIMITIVES advances
the idea that CPGs construct spinal motor acts by recruiting a few
motor primitives from a set encoded in the spinal cord. The best
evidence comes from examination of wiping movements and microstimulation of frog spinal cord, where movements are constructed as a sequencing and combination of a collection of forcefield motor primitives or fundamental elements. “Visuomotor
Coordination in Frog and Toad” discusses how the frog’s motor
acts may be assembled on the basis of visual input.
With this, we switch to articles in which the emphasis is on
rhythmic behavior, with rather little concern for the spatial structure
of the movement (for example, the discussion of locomotion will
focus on coordinating the rhythms of the legs when the animal
progresses straight ahead, rather than on how these rhythms are
modified when the animal traverses uneven terrain or turns to avoid

74

Part II: Road Maps

an obstacle). CRUSTACEAN STOMATOGASTRIC SYSTEM analyzes
specific circuits of identified neurons controlling the chewing (by
teeth inside the stomach) behavior of crustaceans. Of particular
interest is the finding that neuropeptides (see “Neuromodulation in
Invertebrate Nervous Systems”) can change the properties of cells
and the strengths of connections so that, e.g., a cell can become a
pacemaker or a previously ineffective connection can come to exert
a strong influence, and with this a network can dramatically change
its overall behavior. Thus, the change of “mode” may be under the
control of an explicit chemical “switch” of underlying cellular
properties. Of course, in some systems, different input patterns of
excitation and inhibition may enable a given circuit to act in one
of several modes; while in other cases the change of mode may
involve the transfer of control from one neural circuit to another.
LOCOMOTION, INVERTEBRATE focuses on invertebrate locomotion
systems for which quantitative modeling has been done, reviewing
computer models of swimming, flying, crawling, and walking, paying special attention to the interaction of neural networks with the
biomechanical systems they control. The article also reviews the
use of biologically inspired locomotion controllers in robotics,
stressing their distributed nature, their robustness, and their computational efficiency. Conversely, robots can serve as an important
new modeling methodology for testing biological hypotheses. LOCUST FLIGHT: COMPONENTS AND MECHANISMS IN THE MOTOR narrows the focus to one specific invertebrate motor system. The article emphasizes the interactions of the intrinsic properties of flight
neurons, the operation of complex circuits, and phase-specific proprioceptive input, all subject to the concentrations of circulating
neuromodulators. Locust flight can adapt to the demands of a constantly changing sensory environment, and the flight system is flexible and able to operate despite severe ablations and then to recover
from these lesions.
HALF-CENTER OSCILLATORS UNDERLYING RHYTHMIC MOVEMENTS looks at a set of minimal circuits for generating rhythmic
behavior, starting with the half-center oscillator model first proposed to account for the observation that spinal cats (i.e., cats in
which connections between brain and spinal cord had been severed)
could produce stepping movements even when all sensory feedback
from the animal’s motion was eliminated. The article shows the
utility of models of this type in analyzing rhythms in invertebrates
as well as vertebrates—the pelagic mollusk Clione, tadpoles, and
lampreys—in terms of the intrinsic membrane properties of the
component neurons interacting with reciprocal inhibition to initiate
and sustain oscillation in these networks. SPINAL CORD OF LAMPREY: GENERATION OF LOCOMOTOR PATTERNS marks an important
transition: from seeing how one network can oscillate to seeing
how the oscillation of a series of networks can be coordinated.
Experiments show that neural circuitry in isolated pieces of the
spinal cord of lamprey (a jawless, primitive type of fish) can exhibit
oscillations, and when these pieces constitute an intact spinal cord,
they all oscillate with the same frequency but form a “traveling
wave” with a phase relationship that in the complete fish would
yield a wave of bending progressing down the fish from head to
tail to yield the coordinated “wiggling” that yields swimming. The
article reviews the interaction between experimentation and modeling stimulated by such findings. RESPIRATORY RHYTHM GENERATION presents several alternative models of breathing and evaluates them against mammalian data. These data point to the
importance both of endogenous bursting neurons and of network
interactions in generating the basic rhythm. In most models, rhythmogenesis is either pacemaker or network driven. The article reviews the data and these models, and then points the way to future
models that clarify the integration of endogenous bursting with
network interactions. LOCOMOTION, VERTEBRATE shows how neural networks in the spinal cord generate the basic rhythmic patterns
necessary for vertebrate locomotion, while higher control centers

interact with the spinal circuits for posture control and accurate
limb movements, and by sending higher-level commands such as
stop and go signals, speed, and heading of motion. In mammals,
evolution of the CPGs has been accompanied by important modifications of the descending pathways under the requirements of
complex posture control and accurate limb movements, although
the extent of the respective changes remains unknown. Computer
models that combine neural models with biomechanical models are
seen as having an important role to play in studying these issues.
One example uses “genetic algorithms” to model the transition
from a lamprey-like spinal cord that supports traveling waves to a
salamander-like spinal cord that supports both traveling waves for
swimming and “standing waves” for terrestrial locomotion, and
shows how vision may modulate spinal activity to yield locomotion
toward a goal (see also “Visuomotor Coordination in Salamander”).
CHAINS OF OSCILLATORS IN MOTOR AND SENSORY SYSTEMS
abstracts from the specific circuitry to show how oscillators and
their coupling can be characterized in a way that allows the proof
of mathematical theorems about patterns of coordination. CPGs are
discussed not only for the spinal cord of lamprey, but also for the
crayfish swimmeret system and the leech network of swimming. In
the context of locomotion, each oscillator is likely to be a local
subnetwork of neurons that produces rhythmic patterns of membrane potentials. Since the details of the oscillators often are not
known and are difficult to obtain, the object of the mathematics is
to find the consequences of what is known, and to generate sharper
questions to motivate further experimentation. GAIT TRANSITIONS
also studies its topic (e.g., the transition from walking to running)
from the abstract perspective of dynamical systems.

Mammalian Motor Control
ACTION MONITORING AND FORWARD CONTROL OF MOVEMENTS
ARM AND HAND MOVEMENT CONTROL
BASAL GANGLIA
CEREBELLUM AND MOTOR CONTROL
COLLICULAR VISUOMOTOR TRANSFORMATIONS FOR GAZE
CONTROL
EQUILIBRIUM POINT HYPOTHESIS
EYE-HAND COORDINATION IN REACHING MOVEMENTS
GEOMETRICAL PRINCIPLES IN MOTOR CONTROL
GRASPING MOVEMENTS: VISUOMOTOR TRANSFORMATIONS
HIPPOCAMPUS: SPATIAL MODELS
IMAGING THE MOTOR BRAIN
LIMB GEOMETRY, NEURAL CONTROL
MOTOR CONTROL, BIOLOGICAL AND THEORETICAL
MOTOR CORTEX: CODING AND DECODING OF DIRECTIONAL
OPERATIONS
MOTONEURON RECRUITMENT
MUSCLE MODELS
OPTIMIZATION PRINCIPLES IN MOTOR CONTROL
PROSTHETICS, MOTOR CONTROL
PURSUIT EYE MOVEMENTS
REACHING MOVEMENTS: IMPLICATIONS FOR COMPUTATIONAL
MODELS
REINFORCEMENT LEARNING IN MOTOR CONTROL
RODENT HEAD DIRECTION SYSTEM
SENSORIMOTOR LEARNING
VESTIBULO-OCULAR REFLEX
Muscle transduces chemical energy into force and motion, thereby
providing power to move the skeleton. Because of the intricacies
of muscle microstructure and architecture, no comprehensive models are yet able to predict muscle performance completely. MUSCLE
MODELS reviews three classes of models each fulfilling a more

II.8. Motor Systems
narrowly defined objective, ranging from attempts to understand
the molecular level (cross-bridge models) through lumped parameter mechanical models to input-output models of whole muscle
behavior that can be used as part of a broader study of basic musculoskeletal biomechanics or issues of neural control. A motor neuron together with the muscle fibers that it innervates constitutes a
motor unit, and each muscle is a composite structure whose forcegenerating components, the motor units, are typically heterogeneous. Such aggregates can produce much larger forces than a single motor unit. MOTONEURON RECRUITMENT shows how the motor
units can be recruited in the service of reflexes, voluntary movement, and posture. The article considers mechanisms that compensate for muscle fatigue and yielding, models the possible role of
Renshaw cells in linearization or equalization of motor neuron pool
responses, and considers the possible role of cerebellum in control
of motor neuron gain, as well as the roles of motor cortex in motor
neuron recruitment.
PROSTHETICS, MOTOR CONTROL deals with the use of electrical
stimulation to alter the function of motor systems, either directly
or indirectly. The article presents three clinical applications. Therapeutic electrical stimulation is electrically produced exercise in
which the beneficial effect occurs primarily off-line as a result of
trophic effects on muscles and perhaps the CNS. Neuromodulatory
stimulation is preprogrammed stimulation that directly triggers or
modulates a function without ongoing control or feedback from the
patient, and functional electrical stimulation (FES) provides precisely controlled muscle contractions that produce specific movements required by the patient to perform a task. The article also
describes subsystems for muscle stimulation, sensory feedback,
sensorimotor regulation, control systems, and command signals,
most of which are under development to improve on-line control
of FES.
MOTOR CONTROL, BIOLOGICAL AND THEORETICAL sets forth the
basic cybernetic concepts. A motor control system acts by sending
motor commands to a controlled object, often referred to as “the
plant,” which in turn acts on the local environment. The plant or
the environment has one or more variables that the controller attempts to regulate. If the controller bases its actions on signals that
are not affected by the plant output, it is said to be a feedforward
controller. The full understanding of movement must rest on a full
analysis of the integration of neural networks with the biomechanics of the skeletomuscular system. Nonetheless, much has been
learned about limb control from a more abstract viewpoint, as the
next four articles show. Optimization theory has become an important aid to discovering organizing principles that guide the generation of goal-directed motor behavior, specifying the results of
the underlying neural computations without requiring specific details of the way those computations are carried out. OPTIMIZATION
PRINCIPLES IN MOTOR CONTROL concedes that not all motor behaviors are necessarily optimal but argues that attempts to identify
optimization principles can yield a useful taxonomy of motor behavior. The hypothesis is that in performing a motor task, the brain
produces coordinated actions that minimize some measure of performance (such as effort, smoothness, etc.). The article reviews
several studies in which such ideas were examined in the context
of planar upper limb movements, comparing the purely kinematic
minimum jerk model with the more dynamics-based minimum
torque change model. Bur how does one go from a kinematic description of the movement of the hand to the pattern of muscle
control that yields it? There are still many competing hypotheses.
One approach seeks to find control systems that yield optimal trajectories in the absence of disturbances. Another starts from the
observation that a muscle is like a controlled-length spring: set its
length, and it will naturally return to the equilibrium length that
was set. The EQUILIBRIUM POINT HYPOTHESIS builds on this a
systems-level description of how the nervous system controls the

75

muscles so that a stable posture is maintained or a movement is
produced. In this framework, the controller is composed of muscles
and the spinal-based reflexes, and the plant is the skeletal system.
The controller defines a force field that is meant to capture the
mechanical behavior of the muscles and the effect of spinal reflexes. The equilibrium point hypothesis views motion as a gradual
postural transition, and it is suggested that for the case of multijoint
arm movements, one can predict the hand’s motion if the supraspinal system smoothly shifts the equilibrium point from the start
point to a target location. GEOMETRICAL PRINCIPLES IN MOTOR
CONTROL considers a different transition, that from the spatial representation of a motor goal to a set of appropriate neuromuscular
commands, which is in many respects similar to a coordinate transformation. (A word of caution: The matter is subtle because the
brain rarely has neurons whose firing encodes a single coordinate.
Consider, for example, retinotopic coding as distinct from the specific use of (x, y) or (r, h) coordinates. Thus the issue is whether
the activity in certain networks is better described as encoding one
representation than another, such as those related to the eye rather
than those related to the shoulder.) The article describes three types
of coordinate system—end-point coordinates, generalized coordinates, and actuator coordinates—each representing a particular
“point of view” on motor behavior, then examines the geometrical
rules that govern the transformations between these classes of coordinates. It shows how a proper representation of dynamics may
greatly simplify the transformation of motor planning into action.
LIMB GEOMETRY, NEURAL CONTROL offers another perspective,
starting from a discussion of the role of extrinsic and intrinsic coordinates when a human makes a movement. Multijointed coordination complicates the problem of motor control. Consider the
case of arm movements. The activation of an elbow flexor will
always contribute a flexor torque at the elbow, but the resulting
elbow movement can be flexion, extension, or no motion at all,
depending on the actively produced torque at the shoulder. Although in principle a coordinated motor action could be planned
muscle by muscle, a more parsimonious solution is to plan more
global goals at higher levels of organization and let the lower-level
controllers specify the implementation details. The article reviews
issues related to the kinematic aspects of limb geometry control for
arm movements and for posture and gait.
Fast, coordinated movements depend on the nervous system being able to use copies of motor control signals (the corollary discharge) to compute expectations of how the body will move, rather
than always waiting for sensory feedback to signal the current state
of the body. ACTION MONITORING AND FORWARD CONTROL OF
MOVEMENTS spells out three functions of corollary discharge. The
stability of visual perception during eye movements was one of the
first physiological applications proposed for an internal comparison
between a movement and its sensory outcome. Second, goaldirected behavior implies that the action should continue until the
goal has been satisfied, so that motor representations must involve
not only forward mechanisms for steering the action but also mechanisms for monitoring its course and checking its completion.
Third, similar processes have been postulated for actions aimed at
complex and relatively long-term goals, for comparing the representation of the intended action to the actual action and compensating for possible mismatch between the two. Clearly, the effective
use of corollary discharge rests on the brain having learned the
relation between current state, motor command, and the movement
that ensues. SENSORIMOTOR LEARNING explains how neural nets
can acquire forward and inverse “models” of some desired sensorimotor transformation. The managing of multiple models, each
with its own range of applicability in given tasks, is given special
attention. The relevance of such models to the role of cerebellum
(CEREBELLUM AND MOTOR CONTROL) is briefly noted, as is the
idea that these models may act by controlling lower-level “Motor

76

Part II: Road Maps

Primitives” (q.v.). REINFORCEMENT LEARNING IN MOTOR CONTROL, which presents general learning strategies based on adaptive
neural networks, is treated further in the road map Robotics and
Control Theory.
With this background, we turn to articles primarily concerned
with visually controlled behaviors for which neurophysiological
data are available from the mammalian (and in many cases the
monkey) brain, as well as behavioral and, in some cases, imaging
data for humans. The road map takes us from basic unconscious
behaviors to those involving skilled action. The vestibulo-ocular
reflex (VOR) serves to stabilize the retinal image by producing eye
rotations that counterbalance head rotations. Vestibular nuclei neurons are much more than a simple relay; their functions include
multimodality integration, temporal signal processing, and adaptive plasticity. VESTIBULO-OCULAR REFLEX reviews the empirical
data, as well as control-theoretic and neural network models for
the neural circuits that mediate the VOR. These perform diverse
computations that include oculomotor command integration, temporal signal processing, temporal pattern generation, and experience-dependent plasticity.
COLLICULAR VISUOMOTOR TRANSFORMATIONS FOR GAZE CONTROL analyzes the role of superior colliculus in the control of the
rapid movement, called a saccade, of the eyes toward a target. The
article touches on afferent and efferent mapping, target selection,
visuomotor transformations in motor error maps, remapping models, and coding of dynamic motor error. The theme of remapping
is pursued in DYNAMIC REMAPPING, which distinguishes “oneshot” remapping (updating the internal representation in one operation to compensate for an entire movement) from a continuous
remapping process based on the integration of a velocity signal or
the relaxation of a recurrent network. In both cases, the problem
amounts to moving a hill of activity in neuronal maps. The article
uses data on arm movements as well as saccades. Models can be
constrained by considering deficits that accompany localized lesions in humans. These data not only provide valuable insights into
the nature of remappings but they might also help bridge the gap
between behavior and single-cell responses. PURSUIT EYE MOVEMENTS takes us from saccadic “jumps” to those smooth eye movements involved in following a moving target. Current models of
pursuit vary in their organization and in the features of pursuit that
they are designed to reproduce. Three main types of model are
“image motion” models, “target velocity” models, and models that
address the role of prediction in pursuit. However, these models
make no explicit reference to the neural structures that might be
responsible. The article thus analyzes the neural pathways for pursuit, stressing the importance of both visual areas of the cerebral
cortex and oculomotor regions of the cerebellum, to set goals for
future modeling.
IMAGING THE MOTOR BRAIN shows that the behavioral form and
context of a movement are important determinants of functional
activity within cortical motor areas and the cerebellum, stressing
that functional imaging of the human motor system requires one to
study the interaction of neurological and cognitive processes with
the biomechanical characteristics of the limb. Neuroimaging shows
that multiple neural systems and their functional interactions are
needed to successfully perform motor tasks, encode relevant information for motor learning, and update behavioral performance in
real time. The article discusses how evidence from functional imaging studies provides insight into motor automaticity as well as
the role of internal models in movement.
Two articles explore the way in which the rat charts the spatial
structure of its environment, using both “landmark cues” and a
sense of its head orientation with respect to some key aspects of
its environment. HIPPOCAMPUS: SPATIAL MODELS starts with the
finding that single-unit recordings in freely moving rats have revealed “place cells” in fields CA3 and CA1 of the hippocampus,

so called because their firing is restricted to small portions of the
rat’s environment (the corresponding place fields), but the firing
properties of place cells change when the rat is placed in a new
environment. The article focuses on data and models for the role
of place cell firing in the rat’s navigation (see “Cognitive Maps”
for a less neurophysiological approach to the same general issues).
RODENT HEAD DIRECTION SYSTEM focuses on head direction cells
in a number of brain areas that fire maximally when the rat’s head
is pointed in a specific preferred direction, with a gradual falloff in
firing as the heading departs from that direction. Head direction is
not a simple reflection of sensory stimuli since, for example, the
neural coding can be updated when the animal turns in the dark.
The authors analyze such phenomena using attractor networks.
The next six articles are concerned with reaching and grasping.
MOTOR CORTEX: CODING AND DECODING OF DIRECTIONAL
OPERATIONS spells out the relation between the direction of reaching and changes in neuronal activity that have been established for
several brain areas, including the motor cortex. The cells involved
each have a broad tuning function, the peak of which denotes the
“preferred” direction of the cell. A movement in a particular direction will engage a whole population of cells. It is found that the
weighted vector sum of these neuronal preferences is a “population
vector” that points in (close to) the direction of the movement for
discrete movements in 2D and 3D space. Further observations link
this population encoding to speed of movement as well as to preparation for movement. The present article addresses the question
of how movement variables are encoded in the motor cortex and
how this information could be used to drive a simulated actuator
that mimics the primate arm. ARM AND HAND MOVEMENT CONTROL discusses some of the most prominent regularities of arm and
hand control, and examines computational and neural network
models designed to explain them. The analysis reveals the controversies engendered by competition between explanations sought on
different levels—neural, biomechanical, perceptual, or computational. Although some topics, such as internal model control, have
gained solid grounding, the importance of the dynamic properties
of the musculoskeletal system in facilitating motor control, the role
of real-time perceptual modulation of motor control, and the balance between dynamical systems models versus optimal controlbased models are still seen as offering many open questions.
REACHING MOVEMENTS: IMPLICATIONS FOR COMPUTATIONAL
MODELS reviews a number of issues that are emerging from neurophysiological studies of motor control and stresses their implications for development of future models. Data on movement planning, trajectory generation, temporal features of cortical activity,
and overlapping polymodal gradients are used to set challenges for
computational models that will meet the demands of both functional competence and biological plausibility.
EYE-HAND COORDINATION IN REACHING MOVEMENTS focuses
on possible mechanisms responsible for visually guiding the hand
toward a point within the prehension space. Reaching at a visual
target requires transformation of visual information about target
position into a frame of reference suitable for the planning of hand
movement. Accurate encoding of target location requires concomitant foveal and extraretinal signals. The most popular hypothesis
to explain how trajectories are planned is that the trajectory is specified as a vector in the arm’s joint space, with joint angle variations
controlled in a synergic way (temporal coupling). The motor command initially sent to the arm is based on an extrafoveal visual
signal; at the end of the ocular saccade, the updated visual signal
is used to adjust the ongoing trajectory. Because of consistent delays in sensorimotor loops, the rapid path corrections observed during reaching movements cannot be attributed to sensory information only but must rely on a “forward model” of arm dynamics. In
any case, where this article focuses on how the hand is brought to
a target, GRASPING MOVEMENTS: VISUOMOTOR TRANSFORMA-

II.9. Applications, Implementations, and Analysis
TIONS emphasizes the neural mechanisms that control the shaping
of the hand itself to grasp an object, noting the crucial preshaping
of the hand during reaching prior to grasping the object. The analysis emphasizes the cooperative computation of visual mechanisms
in parietal cortex with motor mechanisms in premotor cortex to
integrate sensing and corollary discharge throughout the movement.
CEREBELLUM AND MOTOR CONTROL reviews a number of models of the role of the cerebellum in building “internal models” to
improve motor skills. The article asserts that motor control and
learning in the brain employ a modsular approach in which multiple
controllers coexist, with each controller suitable for one or a small
set of contexts. The basic idea is that, to select the appropriate
controller or controllers at each moment, each of the multiple in-

77

verse models is augmented with a forward model that determines
the responsibility each controller should assume during movement.
This view is exemplified in the MOSAIC (MOdular Selection And
Identification Control) model. Recent human brain imaging studies
have started to accumulate evidence supporting multiple internal
models of tools in the cerebellum. (One caveat: The article stresses
the idea that the cerebellum provides complete motor controllers;
other authors emphasize the idea that the cerebellum provides a
corrective side path that learns how best to augment controllers
located elsewhere in the brain.) Finally, BASAL GANGLIA reviews
the structure of this system in terms of multiple loops, with special
emphasis on those involved in skeletomotor and oculomotor functions. It also reviews the role of dopamine in motor learning and
the mechanisms underlying Parkinson’s disease.

II.9. Applications, Implementations, and Analysis
Applications
BRAIN-COMPUTER INTERFACES
DECISION SUPPORT SYSTEMS AND EXPERT SYSTEMS
FILTERING, ADAPTIVE
FORECASTING
KALMAN FILTERING: NEURAL IMPLICATIONS
PROSTHETICS, MOTOR CONTROL
PROSTHETICS, NEURAL
PROSTHETICS, SENSORY SYSTEMS
The road map Robotics and Control Theory presents a number
of applications of neural networks. Here we offer a representative
(but by no means exhaustive) set of other applications, a list that
can be augmented by the study of many other road maps. Examples
include a variety of topics in vision and speech processing (see the
road maps Vision and Linguistics and Speech Processing, respectively). As noted in the Preface, the discussion of applications
of ANNs in areas from astronomy to steel making was a feature of
the first edition of the Handbook that is not reproduced in the second edition.
Several articles review the various contributions of adaptive neural networks to signal processing. FILTERING, ADAPTIVE notes that
adaptive filtering has found widespread use in noise canceling and
noise reduction, channel equalization, cochannel signal separation,
system identification, pattern recognition, fetal heart monitoring,
and array processing. The parameters of an adaptive filter are adjusted to “learn” or track signal and system variations according to
a task-specific performance criterion. The field of adaptive filtering
was derived from work on neural networks and adaptive pattern
recognition. An adaptive filter can be viewed as a signal combiner
consisting of a set of adjustable weights (or coefficients represented
by a polynomial) and an algorithm (learning rule) that updates these
weights using the filter input and output, as well as other available
signals. The filter may include internal signal feedback, whereby
delayed versions of the output are used to generate the current
output, and it may contain some nonlinear components. The singlelayer perceptron is a well-known type of adaptive filter that has a
binary output nonlinearity (see “Perceptrons, Adalines, and Backpropagation”). The article focuses on the most widely used adaptive
filter architecture and describes in some detail two representative
adaptive algorithms: the least-mean-square algorithm and the constant modulus algorithm. KALMAN FILTERING: NEURAL IMPLICATIONS then introduces Kalman filtering, a powerful idea rooted in
modern control theory and adaptive signal processing. Under linear

and Gaussian conditions, the Kalman filter produces a recursive
estimate of the hidden state of a dynamic system, i.e., one that is
updated with each subsequent (noisy) measurement of the observed
system, with the estimate being optimum in the mean-square-error
sense. The Kalman filter provides an indispensable tool for the
design of automatic tracking and guidance systems, and an enabling technology for the design of recurrent multilayer perceptrons
that can simulate any finite-state machine. In the context of neurobiology, Kalman filtering provides insights into visual recognition and motor control. Related applications are discussed in FORECASTING. Neural nets, mostly of the standard backpropagation type,
have been used with great success in many forecasting applications.
This article looks at the use of neural nets for forecasting with
particular attention to understanding when they perform better or
worse than other technologies, showing how the success of neural
networks in forecasting depends significantly on the characteristics
of the process being forecast.
A decision support system is an information system that helps
humans make a decision on a given problem, under given circumstances and constraints. Expert systems are information systems
that contain expert knowledge for a particular problem area and
perform inferences when new data are entered that may be partial
or inexact. They provide a solution that is expected to be similar
to the solution provided by experts in the field. DECISION SUPPORT
SYSTEMS AND EXPERT SYSTEMS uses the collective term decision
system to refer to either a decision support system or an expert
system. The article discusses how neural networks can be employed
in a decision system. Such systems help humans in their decision
process and so should be comprehensible by humans. The article
reviews results of connectionist-based decision systems. In particular, trainable knowledge-based neural networks can be used to
accumulate both knowledge (rules) and data, building adaptive decision systems with incremental, on-line learning. (For further developments related to the construction of expert systems, see
“Bayesian Networks” and the three articles on “Graphical
Models.”)
BRAIN-COMPUTER INTERFACES discusses the use of on-line analysis of brainwaves to derive information about a subject’s mental
state as a basis for driving some external action, such as selecting
a letter from a virtual keyboard or moving a robotics device, providing an alternative communication and control channel that does
not depend on the brain’s normal output pathway of peripheral
nerves and muscles, which may be nonfunctional in some patients.
The brainwave signals may be evoked potentials generated in response to external stimuli or components associated with sponta-

78

Part II: Road Maps

neous mental activity. Targets for current research include the extraction of local components of brain activity with fast dynamics
that subjects can consciously control. The article reviews the challenge of developing classifiers that work while the subject operates
a brain-actuated application, with ANNs providing robust approaches to on-line learning. These studies are complemented by
a range of articles on prosthetics. PROSTHETICS, NEURAL provides
an overview of the physical components that tend to be common
to all neural prosthetic systems. It emphasizes the biophysical factors that constrain the sophistication of those interfaces. Electroneural interfaces for both stimulation of and recording from neural
tissue are analyzed in terms of biophysics and electrochemistry. It
is also shown how the design of practical neural prostheses must
address the systems hardware issues of power and data management and packaging. PROSTHETICS, SENSORY SYSTEMS focuses on
sensory prostheses, in which information is collected by electronic
sensors and delivered directly to the nervous system by electrical
stimulation of pathways in or leading to the parts of the brain that
normally process a given sensory modality. After assessing the
amenability of all sensory modalities (hearing, vision, touch, proprioception, balance, smell, and taste) the article focuses on auditory and visual prostheses. The great success story has been with
cochlear implants. Here the article reviews improved temporospatial representations of speech sounds, combined electrical and
acoustic stimulation in patients with residual hearing, and psychophysical correlates of performance variability. Visual prostheses
are still in their early days, with no general agreement on the most
promising site to apply electrical stimulation to the visual pathways. The article reviews the cortical approach and the retinal approach. Finally, it is noted that since a prosthesis does not necessarily match natural neural encoding of a stimulus, the success of
the prosthesis depends in part on the plasticity of the human brain
as it remaps to accommodate this new class of signals. PROSTHETICS, MOTOR CONTROL deals with the subset of neural prosthetic
interfaces that employ electrical stimulation to alter the function of
motor systems, either directly or indirectly. The article presents
three clinical applications. Therapeutic electrical stimulation is
electrically produced exercise in which the beneficial effect occurs
primarily off-line as a result of trophic effects on muscles and perhaps the CNS; neuromodulatory stimulation is preprogrammed
stimulation that directly triggers or modulates a function without
ongoing control or feedback from the patient; and functional electrical stimulation (FES) provides precisely controlled muscle contractions that produce specific movements required by the patient
to perform a task. The article describes subsystems for muscle
stimulation, sensory feedback, sensorimotor regulation, control
systems, and command signals, most of which are under development to improve on-line control of FES. Electrical stimulation
of the nervous system is also being used to treat other disorders,
including spinal cord stimulation to control pain and basal ganglia
stimulation to control parkinsonian dyskinesias.
To close this road map, we note the importance of using specialpurpose VLSI chips to gain the full efficiency of artificial neural
network in various applications. Such chips are among the methods
for implementation of neural networks discussed in the next road
map, Implementation and Analysis.

Implementation and Analysis
ANALOG VLSI IMPLEMENTATIONS OF NEURAL NETWORKS
BIOPHYSICAL MECHANISMS IN NEURONAL MODELING
BRAIN SIGNAL ANALYSIS
DATABASES FOR NEUROSCIENCE
DIGITAL VLSI FOR NEURAL NETWORKS
GENESIS SIMULATION SYSTEM
NEUROINFORMATICS

NEUROMORPHIC VLSI CIRCUITS AND SYSTEMS
NEURON SIMULATION ENVIRONMENT
NEUROSIMULATION: TOOLS AND RESOURCES
NSL NEURAL SIMULATION LANGUAGE
PHOTONIC IMPLEMENTATIONS OF NEUROBIOLOGICALLY INSPIRED
NETWORKS
PROGRAMMABLE NEUROCOMPUTING SYSTEMS
SILICON NEURONS
STATISTICAL PARAMETRIC MAPPING OF CORTICAL ACTIVITY
PATTERNS
Briefly, a neural network (whether an artificial neural network for
technological application or a simulation of a biological neural network in computational neuroscience) can be implemented in three
main ways: by programming a general-purpose electronic computer, by programming an electronic computer designed for neural
net implementation, or by building a special-purpose device to emulate a particular network or parametric family of networks. We
discuss these three approaches in turn, and then review a number
of articles describing tools and methods for the analysis of brain
signals and related activity.
NEUROSIMULATION: TOOLS AND RESOURCES reviews neurosimulators, i.e., programs designed to reduce the time and effort required to build models of neurons and neural networks. A neurosimulator requires, at the very least, a highly developed interface,
a scalable design (e.g., through parallel hardware), and extendibility with new neural network paradigms. The review includes programs for modeling networks of biological neurons as well as programs for kinetic modeling of intracellular signaling cascades and
regulatory genetic networks but does not cover connectionist simulators. It provides a general picture of the capabilities of several
neurosimulators, highlighting some of the best features of the various programs, and also describes ongoing efforts to increase compatibility among the various programs. Compatibility allows models built with one neurosimulator to be independently evaluated
and extended by investigators using different programs, thereby
reducing duplication of effort, and also allows models describing
different levels of complexity (molecular, cellular, network) to be
related to one another. The next three articles present some of the
methods necessary for efficient simulation of detailed models of
single neurons (see, e.g., the articles “Axonal Modeling” and “Dendritic Processing” in the Biological Neurons and Synapses road
map). BIOPHYSICAL MECHANISMS IN NEURONAL MODELING is a
primer on biophysically detailed compartmental models of single
neurons (see the road map Biological Neurons and Synapses for
a fuller précis), but contributes to the topic of neurosimulators by
illustrating examples of model definitions using the Surf-Hippo
Neuron Simulation System, providing a minimal syntax that facilitates model documentation and analysis. GENESIS SIMULATION SYSTEM describes GENESIS (GEneral NEural SImulation
System), which was developed to support “structurally realistic”
simulations, computer-based implementations of models designed
to capture the anatomical structure and physiological characteristics
of the neural system of interest. GENESIS has been widely used
for single-cell “compartmental” modeling but is also used for large
network models, using libraries of ion channels and complete cell
models, respectively. NEURON is a neurosimulator that was first
developed for simulating empirically based models of biological
neurons with extended geometry and biophysical mechanisms that
are spatially nonuniform and kinetically complex. This functionality has been enhanced to include extracellular fields, linear circuits to emulate the effects of nonideal instrumentation, models of
artificial (integrate-and-fire) neurons, and networks that can involve
any combination of artificial and biological neuron models. NEURON SIMULATION ENVIRONMENT shows how these capabilities
have been implemented so as to achieve computational efficiency

II.9. Applications, Implementations, and Analysis
while maintaining conceptual clarity, i.e., the knowledge that what
has been instantiated in the computer model is an accurate representation of the user’s conceptual model. Where NEURON has
been primarily used for detailed modeling of single neurons, NSL
NEURAL SIMULATION LANGUAGE provides methods for simulating
very large networks of relatively simple (artificial or biological
simulation) neurons. NSL (pronounced “Nissl”) models focus on
modularity, a well-known software development strategy in dealing
with large and complex systems. Full understanding of a system is
gained both by simulating modules in isolation and by designing
computer experiments that follow the dynamics of the interactions
between the various modules. An NSL model can be described
either by direct programming in NSLM, the NSL (compiled) Modeling language, or by using the Schematic Capture System (SCS),
a visual programming interface to NSLM supporting the description of module assemblages. “Phase-Plane Analysis of Neural
Nets” introduces the qualitative theory of differential equations in
the plane for analyzing neural networks. Computational methods
are a very powerful adjunct to this type of analysis. The article
concludes with comments on numerical methods and software. Between them, the articles reviewed in this paragraph make clear the
challenge of providing multilevel neurosimulation environments in
which one can move effortlessly between the levels of schemas
(functional decomposition of an overall behavior), large neural networks, detailed models of single neurons, and neurochemical models of synaptic plasticity. To be fully effective, such an environment
will also need visualization tools, and the ability to access a database to provide experimental results for comparison with modelbased predictions.
The next two articles address the digital, parallel implementation
of neural networks. DIGITAL VLSI FOR NEURAL NETWORKS starts
by looking at the differences between digital and analog design
techniques, with a focus on analyzing cost-performance trade-offs
in flexibility (Amdahl’s Law), and then considers the use of standard VLSI processors in parallel configurations for ANN emulation. The Adaptive Solutions CNAPS custom digital ANN processor is then discussed to convey a sense of some of the issues
involved in designing digital structures for ANN emulation. Although this chip is no longer produced, it is still being used and
provides a good vehicle for understanding the trade-offs inherent
in emulating neural structures digitally. Finally, the article looks at
field programmable gate array (FPGA) technology as a promising
vehicle for digital implementation of ANNs. PROGRAMMABLE
NEUROCOMPUTING SYSTEMS emphasizes that the design of specialized digital neurocomputers has exploited three items common
to many neural (ANN) algorithms to improve cost/performance:
the limited numeric precision required; the inherently high data
parallelism, where the same operation is performed across large
arrays of data; and communication patterns restricted enough to
allow broadcast buses or unidirectional rings to support parallel
execution of many common neural network algorithms. However,
in the future, the work of commercial design teams to incorporate
multimedia-style kernels into the workloads they consider during
the design of new microprocessors will have as a by-product the
ability to dramatically improve performance for ANN algorithms.
This suggests that in the future there will be greatly reduced interest
in special-purpose neurocomputers but much attention to software
strategies to optimize ANN performance on commercially available
microprocessors.
However, the above three assumptions are not so useful in the
implementation of detailed “compartmental” models of neurons.
Here, attention has been paid to the design of highly specialpurpose analog VLSI circuits. Digital VLSI assigns a different circuit to each bit of information that is to be stored and processed.
Each circuit is driven to the limit so that it settles into a 0-state or
a 1-state, passing through a linear voltage-current regime to get

79

from one saturation state to the other. Thus, if a synaptic weight is
to be stored with eight-bit precision in digital VLSI, it requires
eight such circuits. By contrast, the linear regime of a single circuit
element on a VLSI chip can store data with about three bits of
precision with far less “real estate” on the chip, and with far less
power loss. The price, of course, is that precision cannot be guaranteed on the same scale as for digital circuits, but in many neural
net applications, analog precision is more than adequate. ANALOG
VLSI IMPLEMENTATIONS OF NEURAL NETWORKS provides an overview of the implementation of circuitry in analog VLSI, and then
summarizes a number of technological implementations of such
analog chips for ANNs. The article introduces the difference between the constraints imposed by the biological and silicon media
and emphasizes that letting the silicon medium constrain the design
of a system results in more efficient methods of computation. Special emphasis is given to five properties of a silicon synapse that
are essential for building large-scale adaptive analog VLSI synaptic
arrays. This article focuses on building neural network integrated
circuits (IC), and especially on building connectionist neural network models. SILICON NEURONS takes the same implementation
methodology into the realm of computational neuroscience. Biological neural networks are difficult to model because they are composed of large numbers of nonlinear elements and have a wide
range of time constants. Simulation on a general-purpose digital
computer slows dramatically as the number and coupling of elements increase. By contrast, silicon neurons operate in real time,
and the speed of the network is independent of the number of neurons or their coupling. On the other hand, high connectivity still
poses problems in 2D chip layouts, and the design of specialpurpose hardware is a significant investment, particularly if it is
analog hardware, since analog VLSI still lacks a general set of easyto-use design tools. In any case, NEUROMORPHIC VLSI CIRCUITS
AND SYSTEMS charts the virtues of using analog VLSI to build
“neuromorphic” chips, i.e., chips whose design is based on the
structure of actual biological neural networks. Biological systems
excel at sensory perception, motor control, and sensorimotor coordination by sustaining high computational throughput with minimal energy consumption. Neuromorphic VLSI systems employ
distributed and parallel representations and computation akin to
those found in their biological counterparts. The high levels of
system integration offered in VLSI technology make it attractive
for the implementation of highly complex artificial neuronal systems, even though the physics of the liquid-crystalline state of biological structures is different from the physics of the solid-state
silicon technologies. The article provides a basic foundation in device physics and presents a set of specific circuits that implement
certain essential functions that exemplify the breadth possible
within this design paradigm. However, VLSI-based neural networks have difficulty in scaling up or interconnecting multiple neural chips to incorporate large numbers of neuron units in highly
interconnected architectures without significantly increasing the
computational time. This motivates the use of optical interconnections. The success of optic fibers as media for telecommunications
has been complemented by the use of holograms and spatial light
modulators as mechanisms for storing and processing information
via patterns of light (photonics) rather than patterns of electrons
(electronics). The current state of photonic approaches to neural
network implementation is charted in PHOTONIC IMPLEMENTATIONS OF NEUROBIOLOGICALLY INSPIRED NETWORKS, which
provides a perspective on the use of holography as a technique for
building adaptive connection matrices for ANNs, as well as earlier
discussions of holography as a metaphor for the working of
associative memory in actual brains. In photonic implementation
of neurobiologically inspired networks, optical (free-space or
through-substrate) techniques enable an increase in the number
of neuron units and the interconnection complexity by using the

80

Part II: Road Maps

off-chip (third) dimension. This merging of optical and photonic
devices with electronic circuitry provides additional features such
as parallel weight implementation, adaptation, and modular
scalability.
The remaining articles provide a number of perspectives on the
analysis of data on the brain.
BRAIN SIGNAL ANALYSIS reviews applications of ANNs to brain
signal analysis, including analysis of the EEG and MEG, the electromyogram (EMG), and computed tomographic (CT) images and
magnetic resonance (MR) brain images, and to series of functional
MR brain images (fMRI). Since most medical signals usually are
not produced by variations in a single variable or factor, many
medical problems, particularly those involving decision making,
must involve a multifactorial decision process. In these cases,
changing one variable at a time to find the best solution may never
reach the desired objective, whereas multifactorial ANN approaches may be more successful. The review is organized according to the nature of brain signals to be analyzed and the role that
ANNs play in the applications.
STATISTICAL PARAMETRIC MAPPING OF CORTICAL ACTIVITY
PATTERNS describes the construction of statistical maps to test hypotheses about regionally specific effects like “activations” during
brain imaging studies. Statistical parametric maps (SPMs) are image processes with voxel values that are, under the null hypothesis,
distributed according to a known probability density function (usually Student’s T or F distributions), analyzing each and every voxel
using any standard (univariate) statistical test. The resulting statistical parameters are assembled into an image, the SPM. SPM{T}
refers to an SPM comprising T statistics; similarly, SPM{F} denotes an SPM of F statistics. SPMs are interpreted as spatially
extended statistical processes by referring to the probabilistic behavior of stationary Gaussian fields. Unlikely excursions of the
SPM are interpreted as regionally specific effects, attributable to
the sensorimotor or cognitive process that has been manipulated
experimentally.
NEUROINFORMATICS presents an integrated view of neuroinformatics that combines tools for the storage and analysis of neuroscience data with the use of computational models in structuring

masses of such data. In Europe, neuroinformatics is a term used to
encompass the full range of computational approaches to brain theory and neural networks. In the United States, some people use the
term neuroinformatics solely to refer to databases in neuroscience.
Taking the perspective of the Handbook, this article sees the key
challenge for neuroinformatics to be to integrate insights from synthetic data obtained from running a model with data obtained empirically from studying the animal or human brain. The problem is
that the data, and thus the models, of neuroscience are so diverse.
Neuroscience integrates anatomy, behavior, physiology, and chemistry, and studies levels from molecules to compartments and neurons up to biological neural networks and on to the behavior of
organisms. The article thus presents an architecture for a federation
of databases of empirical neuroscientific data in which results from
diverse laboratories can be integrated. It further advocates a cumulative approach to modeling in neuroscience that facilitates the
reusability (with appropriate changes) of modules within current
neural models, with the pattern of re-use fully documented and
tightly constrained by the linkage with this federation of databases.
DATABASES FOR NEUROSCIENCE then focuses on the issues in constructing such databases. In order to be able to integrate such diverse sources, the various communities within the neurosciences
must begin to develop standards for their community’s data. Neuroscientists use many different and incompatible data formats that
do not allow for the free exchange of data, and the article stresses
the need for standards for the description of the actual data (i.e., a
formalized description of the metadata), possibly using extensible
markup language (XML) technologies. (On a related theme, NEUROSIMULATION: TOOLS AND RESOURCES examines two of the enabling neurosimulation technologies that will allow modelers to
compare and modify models, verify one another’s simulations, and
extend models with their own tools.) One possible solution to integrating data from sources with heterogeneous data and representation is to extend the conventional wrapper-mediator architecture
with domain-specific knowledge. The article concludes with analysis of a specific database of brain images (the fMRI Data Center)
and a comprehensive table of neuroscience databases constructed
to date.

Part III: Articles

Action Monitoring and Forward Control of Movements

83

Action Monitoring and Forward Control of Movements
Marc Jeannerod
Introduction
Monitoring its own output is thought to be a basic principle of
functioning of the nervous system. This idea, inherited from the
cybernetic era, and still operational now, is based on the notion of
a comparison of the actual output of the system with the expected,
or desired, output. In the domain of motor control, for example, it
is assumed that each time the motor centers generate an outflow
signal for producing a movement, a “copy” of this command (the
“efference copy”) is retained. The reafferent inflow signals generated by the movement (e.g., visual, proprioceptive) are compared
with the copy. If a mismatch between the two types of signals is
recorded, new commands are generated until the actual outcome of
the movement corresponds to the desired movement. This comparison cannot be made, however, until the reafferent signals and
the efference copy have been rendered compatible with one another. Proprioceptive signals, in principle, should be directly compatible with motor output (they arise from the same muscles and
joints that are activated during the movement). Visual signals, by
contrast, are generated in a set of coordinates quite different from
those of motor output. Thus, a common set of coordinates must be
computed to make the comparison useful.
The efference copy can only measure the performance error
when the action comes to execution. In order to give this mechanism a predictive role in anticipating the effects of an action, one
must assume the existence of a more complex “internal model” of
that action. Such a model should be able to simulate the action
generation process without waiting for the sensory reafference, or
even without performing it. According to Wolpert, Ghahramani,
and Jordan (1995), a combination of two processes is required:
“The first process uses the current state estimate and motor command to predict the next state by simulating the movement dynamics with a forward model. The second process uses a model of the
sensory output process to predict the sensory feedback from the
current state estimate. The sensory error—the difference between
actual and predicted sensory feedback—is used to correct the state
estimate resulting from the forward model” (p. 1881). Several possible applications for this mechanism are reviewed in the following
discussion.

Stability of Visual Perception and Target Localization
The stability of visual perception during eye movements was one
of the first physiological applications proposed for an internal comparison between a movement and its sensory outcome. When one
moves one’s eyes across the visual field, objects tend to appear
stationary in spite of their displacement on the retina; if, however,
the same displacement is produced by an external agent (e.g., by
gently pressing against the eye at the external canthus), objects no
longer appear stationary. To account for this phenomenon, it has
been conjectured that the command signals to the eye muscles are
effective in remapping the visual scene and canceling out the visual
displacement. In the absence of this signal, the visual displacement
becomes visible. Sperry (1950) coined the term of “corollary discharge” (CD) for the centrally arising discharge that reaches the
visual centers as a corollary of any command generated by the
motor centers. In this way, the visual centers can distinguish the
retinal displacement related to a self-generated movement from that
produced by a moving scene. Visual changes produced by a movement of the eye are normally “canceled” by a CD of a corresponding size and direction. If, however, the CD is absent or does not

correspond to the visual changes (e.g., when the eye is pressed),
these changes are not canceled and are read by the visual system
as having their origin in the external world. The combination of
the retinal signals and the extraretinal command signals (CD) thus
produces a perceived stability of the visual world (Jeannerod, Kennedy, and Magnin, 1979). Signals arising from eye muscle proprioceptors also contribute to visual stability during eye movements
(see Gauthier, Nommay, and Vercher, 1990). A CD type of regulation should in principle be more advantageous, however, because
of its timing: a discharge propagating directly from the motor to
the visual centers should be available to the visual system earlier
than discharges arising from the periphery.
The same logic used for perceptual visual stability can also apply
to egocentric localization of visual targets. The retinal position of
a target cannot in itself be sufficient for its localization in space
because, as the eyes move in the head, and the head moves on the
trunk, several different retinal positions correspond to the same
spatial locus. The spatial location of the target must therefore be
reconstructed by combining eye/head position signals with the position of the target on the retina. The relationships between the
retinal error signal (the position of the target on the retina) and the
eye position signal were first formalized by Robinson (1975). In
this influential model, the efference copy from eye position is derived from the output of a neural integrator that maintains the eye
at a given position during fixation. It is this signal of actual eye
position that is combined with retinal error to provide other motor
systems (e.g., the arm) with the target location information. Eye
movements are not generated on the basis of retinal error. Instead,
the driving signal for the eye to reach the desired eye position
relative to the head is the eye motor error signal. This signal is
obtained by “subtracting” the actual change in eye position in orbit
from the desired position. The movement stops when the motor
error equals zero.
Guitton (1992) was able to directly demonstrate the dynamic
nature of this process, by showing that output neurons from the
superior colliculus—the tectoreticular (TR) neurons—code the
change in eye motor error during the movement. Before the movement takes place, a TR neuron with a preferred vector corresponding to the desired eye position will be activated and will drive the
eye movement generator. As the movement progresses and motor
error is reduced, other TR neurons coding for smaller vectors will
be activated until the error is zero. At this point, a TR neuron coding
for a zero vector will be activated and fixation will be maintained.
Guitton postulates that an internal representation of change in gaze
position is generated and compared with the desired gaze position
to yield instantaneous gaze motor error. If one assumes that this
error is the parameter represented topographically on the collicular
map, one can conceive how this signal will activate the proper
sequence of TR neurons. Hence Guitton’s hypothesis of a moving
“hill” of activity shifting across the collicular map, from the caudal
part where large vectors are encoded to the rostral part where fixation vectors are encoded. There are some difficulties with this
model, however, notably with the timing of discharges in the superior colliculus which, in order to be suitable for coding motor
error, should precede those of the eye movement generator.

Representation of Goals of Movements
Goal-directed behavior implies that the action should continue until
the goal has been satisfied. Motor representations must therefore
involve not only forward mechanisms for steering and directing the

84

Part III: Articles

action, but also mechanisms for monitoring its course and for
checking its completion. This error correction mechanism implies
a short-term storage of outflow information processed at each level
of action generation. Because reafferent signals during execution
of a movement are normally delayed with respect to the command
signal, the comparison mechanism must look ahead in time and
produce an estimate of the movement velocity corresponding to the
command. The image of this estimated velocity is used for computing the actual position of the limb with respect to the target
(Hoff and Arbib, 1992). It is only because the current state of the
action is monitored on-line (rather than after the movement terminates), that corrections can be applied without delay as soon as
the deviation of the current trajectory from the desired trajectory is
detected. A subtle mechanism postulated by Miles and Evarts
(1979) for the regulation of movements could be useful here. They
pointed out that the discharge of muscle spindles in the agonist
muscle during a movement (due to the co-activation of the gamma
motoneurons) exactly fulfills the criterion for an efference copy
that propagates “upward” and is an exact copy of the motor input
sent to the alpha motoneurons. This signal could well be used for
on-line comparison with incoming signals resulting from the limb
movement.
It has been proposed that the information stored in the comparison process should encode, not joint rotations or kinematic parameters, but final configurations (of the body, of the moving segments,
etc.) as they should arise at the end of the action. In other words,
the goal of the action, rather than the action itself, would be represented in the internal model of the action. This hypothetical
mechanism is supported by experimental arguments. Desmurget et
al. (1995) recorded reach and grasp movements directed at a handle
that had to be grasped with a power grip. When the orientation of
the handle was suddenly changed at the onset of a movement, the
arm smoothly shifted from the optimal configuration initially
planned to reach the object to another optimal configuration corresponding to the object in its new orientation. The shift was
achieved by simultaneous changes at several joints (shoulder abduction, wrist rotation), so that the final grasp was effected in the
correct position. In this case the comparison between the desired
and the actual arm position could be effected dynamically through
a process similar to that which has been proposed to solve the
problem of coordinate transformation during goal-directed movements. The position of an object in space is initially coded in extrinsic (e.g., visual) coordinates. In order to be matched by the
moving limb, however, this position must be transferred into an
intrinsic coordinate frame. If the position of the object in extrinsic
coordinates and the position of the extremity of the limb in intrinsic
coordinates coincide (that is, if these positions correspond to the
same point in the two systems of coordinates), the action should
logically be considered as terminated (see Carrozzo and Lacquaniti,
1994).
In addition to matching the movement trajectory to the representation of the intended movement, this mechanism has also other
potential functions for the control of movements. The comparison
between corollary and incoming signals might be used to produce
a correspondence between the motor command and the amount of
muscular contraction, even if the muscular plant is not linear. Other
nonlinearities may also arise from interaction of the moving limb
with external forces, especially if it is loaded (for a review, see
Weiss and Jeannerod, 1998). This mapping problem, which is a
critical factor for producing accurate limb movements, is less important for eye movements, where interactions with the external
force field are minimal and where the load of the moving segment
is constant. In this case, the pattern of command issued by the eye
movement generator should unequivocally reflect the final desired
position of the eye, that is, the position where the retinal error is
zero.

Action Monitoring
At a still higher level, that of actions aimed at complex and relatively long-term goals, similar processes have been postulated for
comparing the representation of the intended action to the actual
action and compensating for possible mismatch between the two.
Several studies, using brain imaging techniques, have focused on
identifying neural structures that would fulfill the requirements for
a comparison mechanism or an error detecting device. Carter et al.
(1998) studied the activity of the anterior cingulate gyrus, a region
lying on the medial cortical surface of the frontal lobe, in a letter
detection task designed to increase error rates and manipulate response competition. Activity was found to increase during erroneous responses, but also during correct responses in conditions of
high levels of response competition. They concluded that the anterior cingular gyrus detects conditions under which errors are
likely to occur, rather than errors themselves. This result suggests
that action-monitoring mechanisms anticipate the occurrence of errors, by using internal models of the effects of the action on the
world. In other words, the sensory consequences of an action are
evaluated before they occur, even in conditions in which the action
may not be executed. This mechanism can also become a powerful
means of determining whether a sensory event is produced by our
own action or by an external agent (and ultimately, if an action is
self-produced or not). Blakemore, Rees, and Frith (1998) compared
brain activity during the processing of externally produced tones
and the processing of tones resulting from self-produced movements. They found an increase in the right inferior temporal lobe
activity when the tones were externally produced, suggesting that
this area would be inhibited by the volitional system in the selfproduced condition. This result raises interesting questions about
the possible consequences of a dysfunction of such a system. Increased activity in the primary auditory areas in the temporal lobe
has been observed during auditory hallucinations in psychotic patients (Dierks et al., 1999). Hence, it is possibility that a defective
self-monitoring system would produce false attribution of one’s
own speech to an external source.
Road Map: Mammalian Motor Control
Related Reading: Collicular Visuomotor Transformations for Gaze Control; Consciousness, Neural Models of; Eye-Hand Coordination in
Reaching Movements; Schema Theory; Sensorimotor Learning

References
Blakemore, S. J., Rees, G., and Frith, C. D., 1998, How do we predict the
consequences of our actions? A functional imaging study, Neuropsychologia, 36:521–529. ⽧
Carrozzo, M., and Lacquaniti, F., 1994, A hybrid frame of reference for
visuomanual coordination, Neuroreport, 5:453–456.
Carter, C. S., Braver, T. S., Barch, D. M., Botwinick, M. M., Noll, D., and
Cohen, J. D., 1998, Anterior cingulate cortex, error detection and the
online monitoring of performance, Science, 280:747–749. ⽧
Desmurget, M., Prablanc, C., Rossetti, Y., Arzi, M., Paulignan, Y., Urquizar, C., and Mignot, J. C., 1995, Postural and synergic control for threedimensional movements of reaching and grasping, J. Neurophysiol.,
74:905–910.
Dierks, T., Linden, D. E. J., Jandl, M., Formisano, E., Goebel, R., Lanferman, H., and Singer, W., 1999, Activation of Heschl’s gyrus during
auditory hallucinations, Neuron, 22:615–621.
Gauthier, G. M., Nommay, D., and Vercher, J. L., 1990, The role of ocular
muscle proprioception in visual localization of targets, Science, 249:58–
61.
Guitton, D., 1992, Control of eye-head coordination during orienting gaze
shifts, Trends Neurosci., 15:174–179.
Hoff, B., and Arbib, M. A., 1992, A model of the effects of speed, accuracy
and perturbation on visually guided reaching, in Control of Arm Move-

Activity-Dependent Regulation of Neuronal Conductances
ment in Space (R. Caminiti, P. B. Johnson, and Y. Burnod, Eds.), Experimental Brain Research, Series 22, pp. 285–306. ⽧
Jeannerod, M., Kennedy, H., and Magnin, M., 1979, Corollary discharge:
Its possible implications in visual and oculomotor interactions, Neuropsychologia, 17:241–258.
Miles, F., and Evarts, E. V., 1979, Concepts of motor organization, Annu.
Rev. Psychol., 30:327–362.
Robinson, D. A., 1975, Oculomotor control signals, in Basic Mechanisms

85

of Ocular Motility and Their Clinical Implications (G. Lennerstrand and
P. Bach-y-Rita, Eds.), Oxford, UK: Pergamon, pp. 337–374.
Sperry, R. W., 1950, Neural basis of the spontaneous optokinetic response
produced by visual inversion, J. Comp. Physiol. Psychol., 43:482–489.
Weiss, P., and Jeannerod, M., 1998, Getting a grasp on coordination, News
in Physiologoical Science, 13:70–75.
Wolpert, D. M., Ghahramani, Z., and Jordan, M. I., 1995, An internal model
for sensorimotor integration, Science, 269:1880–1882. ⽧

Activity-Dependent Regulation of Neuronal
Conductances
Larry F. Abbott and Eve Marder
Introduction
An enormous amount of both theoretical and experimental work
has focused on the implications of activity-dependent synaptic
plasticity for development, learning, and memory. Less attention
has been paid to the fact that the intrinsic characteristics of individual neurons change during development (Spitzer and Ribera,
1998) and can be modified by activity (Franklin, Fickbohm, and
Willard, 1992; Desai, Rutherford, and Turrigiano, 1999), yet these
too play a vital role in shaping network function.
The electrical characteristics of a neuron depend on the numbers
of channels of various types that are active within the cell membrane and on how these channels are distributed over the surface
of the cell. The conventional approach to developing a conductance-based model is to attempt to measure all the ionic currents
expressed by a neuron, describe them with Hodgkin-Huxley equations, and finally assemble the neuron model (Koch and Segev,
1998). This approach is based on the assumptions that individual
neurons of a given class have the same ionic currents expressed at
the same levels and that a neuron expresses the same currents
whenever it is sampled under a specified set of experimental conditions. However, these assumptions appear to be contradicted by
experimental evidence (see, e.g., Liu et al., 1998). In addition, it is
rarely possible to make all of the measurements needed to construct
a conductance-based model in this manner. Most neurons have
many types of ion channels with complex spatial distributions. It
is unlikely that all these currents and their spatial distributions can
be measured. As a result, conventional conductance-based models
depend on considerable hand-tuning of parameters. Each attempt
to make neurons more biologically realistic is accompanied by a
worsening of this problem. Hand-tuning a detailed, multicompartment, conductance-based model can be extremely time consuming
and frustrating.
Neurons accomplish the feat of expressing appropriate numbers
of ion channels in the relevant locations without running months
of computer simulations. This suggests that a set of parameter adjustment mechanisms may allow neurons to self-tune their conductance densities to produce specific electrophysiological properties. A number of attempts have been made at incorporating such
mechanisms into conductance-based neuron models (Bell, 1992;
LeMasson, Marder, and Abbott, 1993; Siegel, Marder, and Abbott,
1994; Liu et al., 1998; Golowasch et al., 1999; Stemmler and Koch,
1999). Self-tuning activity-dependent models provide an alternative approach to modeling neurons and networks. This class of
models does not assume that neurons necessarily have the same
conductance densities over time, nor that individual neurons of a
well-defined class are identical. Rather, they depend on simple negative feedback mechanisms to develop and maintain sets of con-

ductances that produce particular firing patterns and response characteristics. In these models, a second-messenger system, which
may involve Ca2Ⳮ influx and a variety of Ca2Ⳮ sensors, guides the
expression of the membrane conductances in a self-regulating
manner.

Results
A Model Neuron with Self-Regulating Conductances
Self-regulating models are constructed by specifying a set of activity sensors and the rules by which they modify conductance densities. Experimental work indicates that the intracellular Ca2Ⳮ concentration is a good indicator of neuronal activity. Intracellular
Ca2Ⳮ concentrations become elevated in response to activity and
fall during inactive periods (Ross, 1989). Many Ca2Ⳮ-dependent
cellular processes, controlled by a variety of Ca2Ⳮ sensors that
monitor Ca2Ⳮ entry through different ion channels, can affect channel densities (Bito, Deisseroth, and Tsien, 1997; Barish, 1998;
Finkbeiner and Greenberg, 1998). The intracellular Ca2Ⳮ concentration itself, or sensors of inward Ca2Ⳮ currents, can thus be used
as feedback elements that monitor activity and change conductances. In general, when the neuron’s activity is high, the activitydependent rules decrease excitability, and when activity is low,
they increase excitability by modifying appropriate conductances.
A model neuron constructed on these principles can start with
almost any initial conductance densities and self-assemble a set of
maximal conductances that produce particular intrinsic characteristics and patterns of activity. An example of a model spontaneously developing a set of maximal conductances that produces
bursting behavior, starting from an initially silent state, is shown
in Figure 1 (Liu et al., 1998). This illustrates but one example of
the many ways that the model can generate bursting activity. Virtually any initial state leads ultimately to bursting, but, interestingly, the sets of conductances constructed by the model differ from
trial to trial, although the final pattern of bursting is always similar
to that shown in Figure 1. Thus, there is a non-unique map between
maximal conductances and activity. The final set of conductances
depends on initial conditions and is variable, even though the pattern of activity produced by the model is not.

A Model Network with Self-Regulating Conductances
Circuits of self-regulating neurons can self-assemble into functional circuits. This can be illustrated using a simplified model of
the pyloric circuit of the crab stomatogastric ganglion (STG). The
pyloric rhythm of the STG consists of alternating bursts of activity
in several neurons, including the lateral pyloric (LP) and pyloric

86

Part III: Articles
entire network self-assembles to generate a pattern of activity similar to the triphasic rhythm recorded in the intact STG, and it can
do so from any initial configuration of the maximal conductances
of the three neurons (Golowasch et al., 1999). Interestingly, the
intrinsic maximal conductances and responses properties of the individual model neurons are different if they self-assemble as a coupled circuit rather than in isolation. When assembled in a circuit,
each of the model neurons ends up similar to its biological counterpart when acutely isolated, and the entire network generates realistic rhythmic activity. Thus, a cell-autonomous, activity-dependent regulatory rule is sufficient to self-assemble an entire circuit,
at least in this example. It is not necessary to use any sensor that
monitors the output of the whole circuit. Rather, each neuron takes
care of its own activity, and the resultant circuit is tuned as a consequence of each cell’s independent self-adjustment.

Figure 1. Self-assembly of a bursting model neuron (B) starting from different initial conditions (A). A and B represent the voltage traces at the
beginning and end of the self-tuning process. (Adapted from Liu et al.,
1998).

(PY) neurons, and the anterior burst (AB)/pyloric dilator (PD)
pacemaker unit. The model shown in Figure 2 is a three-neuron
circuit with individual neurons and synaptic connections similar to
those of the LP and PY neurons and the AB/PD unit of the STG.
Each model neuron consists of two compartments and has maximal
conductances that are regulated by activity as described in the previous section.
When isolated from each other, the individual AB/PD, LP, and
PY neurons of the model, like the neuron model shown in Figure
1, self-assemble their conductances. A novel feature of the circuit
model is apparent when a realistic pattern of fixed synaptic connections is established between the model cells. In this case, the

Figure 2. Comparison of a network model and experimental data. A, Control conditions in the model (left) and experiment (right). There is a triphasic
motor pattern, revealed in the extracellular lvn recording. Intracellular PD
recordings are also shown. In the model, the AB/PD and LP neurons contain
a proctolin current. In the experiment, the modulatory inputs in the stn were
left intact. B, Immediately after the modulatory inputs are removed, rhythmic activity is lost. Modulatory inputs are removed in the model by setting
the proctolin current to zero. In the experiment, impulse activity in the stn
was blocked to prevent the release of the neuromodulators. C, Activity
eventually resumes. In the model, the activity-dependent sensors in each
cell respond to the change in activity seen in B, and slowly modify the
conductances of each of the model neurons, resulting in the recovery of
rhythmic network activity, as occurred in the experimental case. (Adapted
from Golowasch et al., 1999.)

Comparison with Data from STG Organ Culture
Generation of the pyloric rhythm normally requires the presence
of neuromodulatory substances released from axon terminals of the
stomatogastric nerve (stn). If the stn is cut or blocked, rhythmic
activity slows considerably or ceases. However, if the preparation
is maintained over a period of days without stn modulatory input,
rhythmic activity eventually resumes (Golowasch et al., 1999).
Thus, it appears that prolonged removal of modulatory input alters
the configuration of the pyloric circuit, allowing it to operate independently of the modulators that it normally requires.
The right side of Figure 2 shows the basic experimental result.
Before blockade of the stn, the preparation shown on the right side
of Figure 2A displayed a robust pyloric rhythm. Immediately following blockade of action potential conduction along the stn, the
rhythm completely terminated (Figure 2B, right). However, when
the block was maintained for approximately 24 hours, rhythmic
pyloric activity resumed (Figure 2C, right). This recovery may be
due, at least in part, to changes in the intrinsic properties of the
neurons of the STG induced by the shift in activity following stn
blockade, which allows the pyloric network to operate in the absence of neuromodulatory input.
This hypothesis can be studied, using the model discussed in the
previous section, by including a proctolin conductance to simulate
the effects of neuromodulators released by stn axons. The peptide
proctolin is only one of many substances released from axon terminals of the stn, but it is a particularly potent modulator of the
pyloric network. Figure 2 compares the behavior of the model with
experimental results. Initially, the activity of the model network
with the proctolin current included (Figure 2A, left) was similar to
the pyloric activity of the experimental preparation with the stn
intact (Figure 2A, right). To simulate the effects of blocking the
stn, the proctolin conductance in the LP and AB/PD neurons was
set to zero (Figure 2B, left). This immediately terminated the rhythmic activity of the model network, duplicating the effect of blocking the stn in the real preparation (Figure 2B, right). The suppression of the rhythm following the elimination of the proctolin
conductance caused the activity-dependent conductance regulation
mechanisms to modify the maximal conductances of the model
neurons. This resulted in restoration of the pyloric rhythm in the
model network after elimination of the proctolin conductance (Figure 2C, left), matching the natural resumption of the rhythm (Figure
2C, right).
It is important to stress that although the pyloric rhythms in
Figures 2A and 2C look similar, they are produced by quite different cellular mechanisms. In Figure 2A, the existence of the rhythm
depends on the presence of the modulatory proctolin current, while
in Figure 2C the rhythms are produced in the absence of the
modulator.

Adaptive Resonance Theory

87

Discussion

References

The homeostatic regulation of neuronal circuits is an essential element in their development and maintenance as functioning systems.
This is often ignored in the construction of neural networks, because fixed parameters are adjusted and used to control network
function. Biological systems do not have the luxury of using fixed
constants, because of the continual recycling of the proteins from
which they are built. As a consequence, biological networks are
typically more robust than artificial networks, and they are selfassembling. The work described here is an attempt to incorporate
these features into neural network models.
The model studies described have revealed several interesting
consequences of activity-dependent regulation of conductances: (1)
Conductance regulation stabilizes a model neuron against activity
shifts caused by extracellular perturbations. (2) Intrinsic properties
of model neurons are modified by sustained shifts in activity. (3)
The regulation scheme described here, applied as a local regulator
of channel density in a multicompartment model, can produce a
realistic spatial distribution of conductances. (4) Regulation of the
activity of individual neurons in a network may, in some case, be
sufficient for the development and maintenance of a network pattern requiring coordination across neurons.
One of the most significant messages provided by models of
conductance regulation is that the same mechanisms that develop
and maintain membrane conductances are likely to modify these
conductances in response to long-lasting changes in the activity of
the neuron. Furthermore, different neurons, or the same neuron at
different times, may exhibit similar characteristics and activity
while expressing membrane conductances at quite different levels.
These observations make it apparent that neuron models must be
much more flexible and dynamic than has conventionally been the
case.

Barish, M. E., 1998, Intracellular calcium regulation of channel and receptor expression in the plasmalemma: Potential sites of sensitivity along
the pathways linking transcription, translation, and insertion, J. Neurobiol., 37:146–157.
Bell, A. J., 1992, Self-organization in real neurons: Anti-Hebb in “channel
space,” in Advances in Neural Information Processing Systems (J.
Moody, S. Hanson, and R. Lippmann), Eds., San Mateo, Morgan Kaufmann, pp. 59–66.
Bito, H., Deisseroth, K., and Tsien, R. W., 1997, Ca2Ⳮ-dependent regulation in neuronal gene expression, Curr. Opin. Neurobiol., 7:419–429.
Desai, N. S., Rutherford, L. C., and Turrigiano, G. G., 1999, Plasticity in
the intrinsic excitability of cortical pyramidal neurons, Nature Neurosci.,
2:515–520.
Finkbeiner, S., and Greenberg, M. E., 1998, Ca2Ⳮ channel-regulated neuronal gene expression, J. Neurobiol., 37:171–189. ⽧
Franklin, J. L., Fickbohm, D. J., and Willard, A. L., 1992, Long-term regulation of neuronal calcium currents by prolonged changes of membrane
potential, J. Neurosci., 12:1726–1735.
Golowasch, J., Casey, M., Abbott, L. F., and Marder, E., 1999, Network
stability from activity-dependent regulation of neuronal conductances,
Neural Computat., 11:1079–1096.
Koch, C., and Segev, I., 1998, Methods in Neuronal Modeling, Cambridge,
MA: MIT Press. ⽧
LeMasson, G., Marder, E., and Abbott, L. F., 1993, Activity-dependent
regulation of conductances in model neurons, Science, 259:1915–1917.
Liu, Z., Golowasch, J., Marder, E., and Abbott, L. F., 1998, A model neuron
with activity-dependent conductances regulated by multiple calcium sensors, J. Neurosci., 18:2309–2320.
Ross, W. N., 1989, Changes in intracellular calcium during neuron activity,
Annu. Rev. Physiol., 51:491–506. ⽧
Siegel, M., Marder, E., and Abbott, L. F., 1994, Activity-dependent current
distributions in model neurons, Proc. Natl. Acad. Sci. USA, 91:11308–
11312.
Spitzer, N. C., and Ribera, A. B., 1998, Development of electrical excitability in embryonic neurons: Mechanisms and roles, J. Neurobiol.,
37:190–197. ⽧
Stemmler, M., and Koch, C., 1999, How voltage-dependent conductances
can adapt to maximize the information encoded by neuronal firing rate,
Nature Neurosci., 2:521–527.

Road Map: Biological Neurons and Synapses
Background: Ion Channels: Keys to Neuronal Specialization
Related Reading: Biophysical Mosaic of the Neuron

Adaptive Resonance Theory
Gail A. Carpenter and Stephen Grossberg
Introduction
Principles derived from an analysis of experimental literatures in
vision, speech, cortical development, and reinforcement learning,
including attentional blocking and cognitive-emotional interactions, led to the introduction of adaptive resonance as a theory of
human cognitive information processing (Grossberg, 1976a,
1976b). The theory has evolved as a series of real-time neural network models that perform unsupervised and supervised learning,
pattern recognition, and prediction (Levine, 2000; Duda, Hart, and
Stork, 2001). Models of unsupervised learning include ART 1 (Carpenter and Grossberg, 1987) for binary input patterns and fuzzy
ART (Carpenter, Grossberg, and Rosen, 1991) for analog input
patterns. ARTMAP models (Carpenter et al., 1992) combine two
unsupervised modules to carry out supervised learning. Many variations of the basic supervised and unsupervised networks have
since been adapted for technological applications and biological
analyses.

Match-Based Learning, Error-Based Learning,
and Stable Fast Learning
A central feature of all ART systems is a pattern matching process that compares an external input with the internal memory
of an active code. ART matching leads either to a resonant state,
which persists long enough to permit learning, or to a parallel
memory search. If the search ends at an established code, the
memory representation may either remain the same or incorporate new information from matched portions of the current input.
If the search ends at a new code, the memory representation
learns the current input. This match-based learning process is
the foundation of ART code stability. Match-based learning allows memories to change only when input from the external
world is close enough to internal expectations, or when something completely new occurs. This feature makes ART systems
well suited to problems that require on-line learning of large and
evolving databases.

88

Part III: Articles

Match-based learning is complementary to error-based learning,
which responds to a mismatch by changing memories so as to reduce the difference between a target output and an actual output,
rather than by searching for a better match. Error-based learning is
naturally suited to problems such as adaptive control and the learning of sensorimotor maps, which require ongoing adaptation to
present statistics. Neural networks that employ error-based learning
include backpropagation and other multilayer perceptrons (MLPs)
(Duda et al., 2001; see BACKPROPAGATION: GENERAL PRINCIPLES).
Many ART applications use fast learning, whereby adaptive
weights converge to equilibrium in response to each input pattern.
Fast learning enables a system to adapt quickly to inputs that occur
rarely but that may require immediate accurate recall. Remembering details of an exciting movie is a typical example of learning on
one trial. Fast learning creates memories that depend on the order
of input presentation. Many ART applications exploit this feature
to improve accuracy by voting across several trained networks,
with voters providing a measure of confidence in each prediction.

strong competition, activation is concentrated at the F2 node that
receives the maximal F1 r F2 signal; in this winner-take-all mode,
only one code component remains positive (see WINNER-TAKEALL NETWORKS).
Before learning can change memories, ART treats the chosen
code as a hypothesis, which it tests by matching the top-down expectation of y against the input that selected it (Figure 1B). Parallel
specific and nonspecific feedback from F2 implements matching as
a real-time locally defined network computation. Nodes at F1 receive both learned excitatory signals and unlearned inhibitory signals from F2. These complementary signals act to suppress those
portions of the pattern I of bottom-up inputs that are not matched
by the pattern V of top-down expectations. The residual activity
x* represents a pattern of critical features in the current input with
respect to the chosen code y. If y has never been active before, x*
⳱ x ⳱ I, and F1 registers a perfect match.

Coding, Matching, and Expectation

If the matched pattern x* is close enough to the input I, then the
memory trace of the active F2 code converges toward x*. The property of encoding an attentional focus of critical features is key to
code stability. This learning strategy differentiates ART networks
from MLPs, which typically encode the current input rather than a
matched pattern, and hence employ slow learning across many input trials to avoid catastrophic forgetting.
ART memory search begins when the network determines that
the bottom-up input I is too novel or unexpected with respect to
the active code to satisfy a matching criterion. The search process
resets the F2 code y before an erroneous association to x* can form
(Figure 1C). After reset, medium-term memory within the F1 r F2
pathways (Carpenter and Grossberg, 1990) biases the network
against the previously chosen node, so that a new code y* may be
chosen and tested (Figure 1D).
The ART matching criterion is determined by a parameter q
called vigilance, which specifies the minimum fraction of the input
that must remain in the matched pattern in order for resonance to
occur. Low vigilance allows broad generalization, coarse categories, and abstract memories. High vigilance leads to narrow generalization, fine categories, and detailed memories. At maximal
vigilance, category learning reduces to exemplar learning. While
vigilance is a free parameter in unsupervised ART networks, in
supervised networks vigilance becomes an internally controlled
variable that triggers a search after rising in response to a predictive
error. Because vigilance then varies across learning trials, the memories of a single ARTMAP system typically exhibit a range of
degrees of refinement. By varying vigilance, a single system can
recognize both abstract categories, such as faces and dogs, and
individual examples of these categories.

Figure 1 illustrates a typical ART search cycle. To begin, an input
pattern I registers itself as a short-term memory activity pattern x
across a field of nodes F1 (Figure 1A). Converging and diverging
pathways from F1 to a coding field F2, each weighted by an adaptive long-term memory trace, transform x into a net signal vector
T. Internal competitive dynamics at F2 further transform T, generating a compressed code y, or content-addressable memory. With

Attention, Search, Resonance, and Learning

Supervised Learning and Prediction

Figure 1. An ART search cycle imposes a matching criterion, defined by
a dimensionless vigilance parameter q, on the degree of match between a
bottom-up input I and the top-down expectation V previously learned by
the F2 code y chosen by I. See text for discussion of A through D sequence.

An ARTMAP system includes a pair of ART modules, ARTa and
ARTb (Figure 2). During supervised learning, ARTa receives a
stream of patterns {a(n)} and ARTb receives a stream of patterns
{b(n)}, where b(n) is the correct prediction given a(n). An associative
learning network and a vigilance controller link these modules to
make the ARTMAP system operate in real time, creating the minimal number of ARTa recognition categories, or hidden units,
needed to meet accuracy criteria. A minimax learning rule enables
ARTMAP to learn quickly, efficiently, and accurately as it conjointly minimizes predictive error and maximizes code compression in an on-line setting. A baseline vigilance parameter q̄a sets
the minimum matching criterion, with smaller q̄a allowing broader
categories to form. At the start of a training trial, qa ⳱ q̄a. A predictive failure at ARTb increases qa just enough to trigger a search,
through a feedback control mechanism called match tracking. A

Adaptive Resonance Theory

89

Figure 3. A distributed ART (dART) architecture retains the stability of
WTA ART networks but allows the F2 code to be distributed across arbitrarily many nodes.
Figure 2. The general ARTMAP network for supervised learning includes
two ART modules. For classification tasks, the ARTb module may be
simplified.

newly active code focuses attention on a different cluster of input
features, and checks whether these features are better able to predict
the correct outcome. Match tracking allows ARTMAP to learn a
prediction for a rare event embedded in a cloud of similar frequent
events that make a different prediction.
ARTMAP employs a preprocessing step, called complement
coding, which, by normalizing input patterns, solves a potential
category proliferation problem (Carpenter et al., 1991). Complement coding doubles the number of input components, presenting
to the network both the original feature vector and its complement.
In neurobiological terms, complement coding uses both on-cells
and off-cells to represent an input pattern. The corresponding oncell portion of a weight vector encodes features that are consistently
present in category exemplars, while the off-cell portion encodes
features that are consistently absent. Small weights in complementary portions of a category representation encode as uninformative
those features that are sometimes present and sometimes absent.

Distributed Coding
Winner-take-all activation in ART networks supports stable coding
but causes category proliferation when noisy inputs are trained with
fast learning. In contrast, distributed McCulloch-Pitts activation in
MLPs promotes noise tolerance but causes catastrophic forgetting
with fast learning (see LOCALIZED VERSUS DISTRIBUTED REPRESENTATIONS). Distributed ART (dART) models are designed to
bridge these two worlds: distributed activation enhances noise tolerance, while new system dynamics retain the stable learning capabilities of winner-take-all ART systems (Carpenter, 1997). These
networks automatically apportion learned changes according to the
degree of activation of each coding node, which permits fast as
well as slow distributed learning without catastrophic forgetting.
New learning laws and rules of synaptic transmission in the reconfigured dART network (Figure 3) sidestep computational problems that occur when distributed coding is imposed on the architecture of a traditional ART network (Figure 1). The critical design
element that allows dART to solve the catastrophic forgetting problem of fast distributed learning is the dynamic weight. This quantity
equals the rectified difference between coding node activation and
an adaptive threshold, thereby combining short-term and long-term
memory in the network’s fundamental computational unit.
Thresholds sij in paths projecting directly from an input field F0
to a coding field F2 obey a distributed instar (dInstar) learning law,

which reduces to an instar law when coding is winner-take-all.
Rather than adaptive gain, learning in the F0 r F2 paths resembles
the redistribution of synaptic efficacy (RSE) observed by Markram
and Tsodyks (1996) at neocortical synapses. In these experiments,
pairing enhances the strength, or efficacy, of synaptic transmission
for low-frequency test inputs, but fails to enhance, and can even
depress, synaptic efficacy for high-frequency test inputs. In the
dART learning system, RSE is precisely the computational dynamic needed to support real-time stable distributed coding.
Thresholds sji in paths projecting from the coding field F2 to a
matching field F1 obey a distributed outstar (dOutstar) law, which
realizes a principle of atrophy due to disuse to learn the network’s
expectations with respect to the distributed coding field activation
pattern. As in winner-take-all ART systems, dART compares topdown expectation with the bottom-up input at the matching field,
and quickly searches for a new code if the match fails to meet the
vigilance criterion.

Discussion: Applications, Rules,
and Biological Substrates
ART and dART systems are part of a growing family of selforganizing network models that feature attentional feedback and
stable code learning. Areas of technological application include
industrial design and manufacturing, the control of mobile robots,
face recognition, remote sensing land cover classification, target
recognition, medical diagnosis, electrocardiogram analysis, signature verification, tool failure monitoring, chemical analysis, circuit
design, protein/DNA analysis, three-dimensional visual object recognition, musical analysis, and seismic, sonar, and radar recognition (e.g., Caudell et al., 1994; Griffith and Todd, 1999; Fay et al.,
2001). A book by Serrano-Gotarredona, Linares-Barranco, and Andreou (1998) discusses the implementation of ART systems as
VLSI microchips. Applications exploit the ability of ART systems
to learn to classify large databases in a stable fashion, to calibrate
confidence in a classification, and to focus attention on those featural groupings that the system deems to be important based on
experience. ART memories also translate to a transparent set of IFTHEN rules that characterize the decision-making process and may
be used for feature selection.
ART principles have further helped explain parametric behavioral and brain data in the areas of visual perception, object recognition, auditory source identification, variable-rate speech and
word recognition, and adaptive sensorimotor control (e.g., Levine,
2000; Page, 2000). One area of recent progress concerns how the
neocortex is organized into layers, clarifying how ART design prin-

90

Part III: Articles

ciples are found in neocortical circuits (see LAMINAR CORTICAL
ARCHITECTURE IN VISUAL PERCEPTION).
Pollen (1999) resolves various past and current views of cortical
function by placing them in a framework he calls adaptive resonance theories. This unifying perspective postulates resonant feedback loops as the substrate of phenomenal experience. Adaptive
resonance offers a core module for the representation of hypothesized processes underlying learning, attention, search, recognition,
and prediction. At the model’s field of coding neurons, the continuous stream of information pauses for a moment, holding a fixed
activation pattern long enough for memories to change. Intrafield
competitive loops fixing the moment are broken by active reset,
which flexibly segments the flow of experience according to the
demands of perception and environmental feedback. As Pollen
(1999, pp. 15–16) suggests, “[I]t may be the consensus of neuronal
activity across ascending and descending pathways linking multiple cortical areas that in anatomical sequence subserves phenomenal visual experience and object recognition and that may underlie
the normal unity of conscious experience.”
Road Maps: Learning in Artificial Networks; Vision
Related Reading: Competitive Learning; Helmholtz Machines and SleepWake Learning; Laminar Cortical Architecture in Visual Perception

References
Carpenter, G. A., 1997, Distributed learning, recognition, and prediction by
ART and ARTMAP neural networks, Neural Netw., 10:1473–1494.
Carpenter, G. A., and Grossberg, S., 1987, A massively parallel architecture
for a self-organizing neural pattern recognition machine, Computer Vision Graphics Image Process., 37:54–115.

Carpenter, G. A., and Grossberg, S., 1990, ART 3: Hierarchical search
using chemical transmitters in self-organizing pattern recognition architectures, Neural Networks, 3:129–152.
Carpenter, G. A., Grossberg, S., Markuzon, N., Reynolds, J. H., and Rosen,
D. B., 1992, Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps, IEEE
Trans. Neural Netw., 3:698–713.
Carpenter, G. A., Grossberg, S., and Rosen, D. B., 1991, Fuzzy ART: Fast
stable learning and categorization of analog patterns by an adaptive resonance system, Neural Netw., 4:759–771.
Caudell, T. P., Smith, S. D. G., Escobedo, R., and Anderson, M., 1994,
NIRS: Large scale ART-1 neural architectures for engineering design
retrieval, Neural Netw., 7:1339–1350.
Duda, R. O., Hart, P. E., and Stork, D. G., 2001, Pattern Classification,
2nd ed., New York: Wiley, section 10.11.2. ⽧
Fay, D. A., Verly, J. G., Braun, M. I., Frost, C., Racamato, J. P., and
Waxman, A. M., 2001, Fusion of multi-sensor passive and active 3D
imagery, in Proc. SPIE Enhanced Synthet. Vision, vol. 4363.
Griffith, N., and Todd, P. M., Ed., 1999, Musical Networks: Parallel Distributed Perception and Performance, Cambridge, MA: MIT Press.
Grossberg, S., 1976a, Adaptive pattern classification and universal recoding: I. Parallel development and coding of neural feature detectors, Biol.
Cybern., 23:121–134.
Grossberg, S., 1976b, Adaptive pattern classification and universal recoding: II. Feedback, expectation, olfaction, and illusions, Biol. Cybern.,
23:187–202.
Levine, D. S., 2000, Introduction to Neural and Cognitive Modeling, Mahwah, New Jersey: Erlbaum, chap 6. ⽧
Markram, H., and Tsodyks, M., 1996, Redistribution of synaptic efficacy
between neocortical pyramidal neurons, Nature, 382:807–810.
Page, M., 2000, Connectionist modelling in psychology: A localist manifesto, Behav. Brain Sci., 23:443–512.
Pollen, D. A., 1999, On the neural correlates of visual perception, Cereb.
Cortex, 9:4–19.
Serrano-Gotarredona, T., Linares-Barranco, B., and Andreou, A. G., 1998,
Adaptive Resonance Theory Microchips: Circuit Design Techniques,
Boston: Kluwer Academic.

Adaptive Spike Coding
Adrienne Fairhall and William Bialek
Introduction
The meaning of any signal that we receive from our environment
is modulated by the context within which it appears. Our interpretation of color, a spoken phoneme, or a patch of luminance depends
critically on its context. Although “context” may be a rather abstract notion, it is often reasonable to understand the term as meaning the statistical ensemble in which the signal is embedded. Interpreting a message requires both registering the signal itself and
knowing something about this statistical ensemble. The relevant
temporal or spatial ensemble depends on the task. The context may
be highly local; we interpret appropriately gradations of light and
dark in a scene where local brightness typically varies over orders
of magnitude (see FEATURE ANALYSIS). For tasks such as decision
making, the relevant statistics may reflect complex descriptions of
the world accumulated over long periods.
Neural representations at every level of information processing
should be similarly modulated by context. Information theoretically, this has measurable advantages: representations that appropriately take into account the statistical properties of the incoming
signal are more efficient. Since the 1950s it has been suggested that
efficiency is a design principle of the nervous system, allowing
neurons to transmit more useful information with their limited dynamic range (see OPTIMAL SENSORY ENCODING). Thus, one expects that learning the context and implementing this knowledge

through coding strategy is inherent in the formation of representations.
Such adjustments occur over a wide range of time scales.
Through the genetic code, species adapt to environmental changes
over many generations. In a single individual, learning, implemented through neural plasticity, continues throughout life in response to experience of the world; perceptual learning is stored
even at low levels of neural information processing (see SOMATOTOPY: PLASTICITY OF SENSORY MAPS). In the article, we discuss
even more rapid changes: neural adaptation, which we take to mean
reversible change in the response properties of neurons on short
time scales.
Since Adrian’s first observations of adaptation in spiking neurons, it has been suggested that adaptation serves a useful function
for information processing, preventing a neuron from continuing
to transmit redundant information and increasing its responsiveness
to new stimuli. Within the simplified picture of a neuron as a combination of linear filtering followed by a threshold, or a decision
rule for spiking, either or both of the two components—the filter
and the threshold function—may be adaptive functions of the input,
and both may implement the goal of increasing information transmission. We will discuss both of these possibilities.
Neurons in every sensory modality have been shown to have
adaptive properties, and the mechanisms governing various types
of adaptation have been at least partially explored (Torre et al.,

Adaptive Spike Coding

91

1995). Here we will discuss adaptation as the simplest form of
learning and memory. We describe recent experiments that explicitly aim to link the phenomenology of adaptive spike coding to its
functional relevance, in particular to improved information transmission. A common feature of adaptation is the existence of multiple time scales. In examining mechanisms, we concentrate on
recent work suggesting that the long time scales retaining shortterm memory can be generated through single-cell properties.

Adaptive Coding
Adaptation of neural firing rate to stationary stimuli has been seen
in all modalities of the primary sensory system. In the visual system, photoreceptors adapt to light level, and retinal ganglion cells
show rapid contrast gain control. The trade-offs and information
processing gains due to adaptation in insect eyes, relevant also for
the vertebrate retina, are discussed in Laughlin (1989). Mechanoreceptors in the somatosensory system have been classified into
four main types of cells, three of which are distinguished by the
time scales of their adaptation (rapidly and slowly adapting), and
these time scales in part determine the cells’ function: slowly adapting cells are implicated in the perception of spatial form and texture, while the experience of flutter and of motion is mediated by
rapidly adapting cells (Johnson, 2001). Thus, the dynamics of adaptation can determine a neuron’s functional role.
Adaptation is not limited to primary receptors. In visual cortex,
V1 neurons show contrast adaptation, which is thought to occur
entirely at the level of cortex. The motion aftereffect, a familiar
phenomenon whereby following exposure to motion in one direction, the visual field appears to move in the opposite direction, is
thought to be due to adaptation of direction-sensitive neurons in
visual cortex.

Figure 1. Firing rate of rabbit retinal ganglion cells in response to a flicker
stimulus where the variance of the light intensity I switches periodically in
time. (From Smirnakis S. M., et al., 1997, Adaptation of retinal processing
to image contrast and spatial scale, Nature, 386: 69–73. Copyright 1997,
Macmillan Publishers Ltd.; reprinted with permission.)

Adaptation to a Distribution
Understanding the significance of adaptation for information processing requires going beyond fixed stimuli. Recently, studies have
focused on adaptation to the stimulus distribution. This approach
is necessary to characterize coding information theoretically: the
evaluation of a coding strategy requires considering the entire ensemble of inputs and outputs. In Smirnakis et al. (1997), retinal
ganglion cells were stimulated with dynamic movies of flickering
light intensity where the mean light level was fixed but the variance
was switched periodically from one value to another. The spike
rate of the neurons showed typical adaptive behavior (Figure 1):
following an increase in variance, the firing rate increased initially,
but gradually returned to a considerably lower level; a decrease in
variance led to a sudden dip in firing rate, with eventual recovery.
The experiments of Smirnakis et al. (1997) consider only firing
rate. However, the timing of single spikes can convey a great deal
of information about the stimulus. In the visual system of the fly,
in particular the motion-sensitive identified neuron H1 in the fly’s
lobula plate, much is understood about single-spike coding, providing an excellent opportunity to study the effects of adaptation
in detail.
H1 responds to a simple stimulus, wide-field horizontal motion.
The neuron is characterized by its input/output relation P(spike|s),
or the probability of a spike given the projection s of the dynamic
stimulus onto a relevant feature, determined by reverse correlation.
When the system has reached steady state through exposure to
a zero-mean, white noise velocity stimulus with a given variance
r 2, its input/output relation is measured. The resulting curves, measured for a range of values of the variance, are shown in Figure 2.
Clearly, the input/output relation is not a fixed property of the system but adapts to the distribution of inputs. Indeed, it does so in
such a way that the stimulus appears to be measured in units of its

Figure 2. (a) A set of input/output relations relating the probability of
spiking to the velocity stimulus, measured for stationary white noise stimuli
with different variances. (b) The curves differ only by a scale factor as is
shown by normalizing the stimulus by its standard deviation. In this case
the curves coincide. (From Brenner, N., Bialek, W., and de Ruyter van
Steveninck, R. R., 2001, Adaptive rescaling maximizes information transmission, Neuron, 26:695–702. Copyright 2000, Elsevier Science, reprinted
with permission.)

standard deviation; when the curves are replotted with the stimulus
normalized by its RMS value, they superimpose. Thus, a scale factor k multiplying the stimulus, and thereby matching the dynamic
range of the response to the distribution of the inputs, is a degree
of freedom for the system. The value of k chosen by the system
achieves a maximum of information transmission (Brenner, Bialek,
and de Ruyter van Steveninck, 2000).
This is a simple form of learning: the system gauges the standard
deviation of the signal and modifies its response properties to adjust
its dynamic range to the range of inputs. The adjustment must take
some time, as the new distribution must be sampled from examples.
This sets fundamental physical and statistical limits for the system’s estimate of the current variance. We can examine the time
scale for learning (Fairhall et al., 2001) by, as in the retina experiments described earlier, switching periodically between two distributions. The firing rate shows the same pattern of adaptation as
was seen in the experiments of Smirnakis et al. (1997), but this
pattern need not correspond to the time scale for adjustment of the
input/output relation. Indeed, it was found that the scale factor of
the input/output relations, measured dynamically, adjusts much

92

Part III: Articles

more rapidly than the relaxation time of the rate—on the order of
100 ms, compared with several seconds. This short time scale is
consistent with the limits imposed by estimates of noise from the
photoreceptors. One can verify that the dynamic adaptation of the
input/output relation maintains information transmission through
the system by computing how much information one can extract
from the spikes about the stimulus (see SENSORY CODING AND
INFORMATION TRANSMISSION and Fairhall et al., 2001). The information rate recovers on comparably short time scales.
For the decoder, a potential drawback of adaptive coding is ambiguity: it is necessary to know the context in order to interpret the
signal correctly. Thus, information about the context must be conveyed independently. Although this information might be carried
by other neurons in the network, here the information about the
ensemble is carried simultaneously by the same spike train: it can
be read off, either through the rate (taking into account the delays
due to the slow relaxation) or, more accurately, through the variance dependence of the statistics of spike time differences (Fairhall
et al., 2001). Thus, for the code of H1, spikes carry multiple meanings: in absolute timing, as precise markers of single stimulus
events, and in relative timing, as indicators of the stimulus
ensemble.

Multiple Time Scales
The slow relaxation of the rate appears to be related to a commonly
observed property of adapting primary sensory neurons: a power
law decay of the firing rate r, r  tⳮ␣. More generally, in the case
just presented, the rate is close to the fractional derivative of the
logarithm of the stimulus variance. For each frequency x, fractional
differentiation shifts the frequency component by a constant phase,
and scales each component by x␣, where ␣ is a power less than 1.
Some of the properties of a fractional differentiator are illustrated
in Figure 3. Several examples of a power law decay of the rate
following a step change in stimulus amplitude were collected by
Thorson and Biederman-Thorson (1974; Figure 4) and more have
since been observed; examples include various invertebrate mechanoreceptors and photoreceptors, mammalian carotid sinus baroreceptors, and cat retinal ganglion cells.
We have noted a separation of time scales in the adaptation of
the input/output relation compared with the rate. This type of adaptation on its own signals the existence of many time scales.
Power-law scaling implies the lack of a typical time scale or the
presence of multiple time scales. Fractional differentiation is nonlocal; the response at time t0 is affected by times t K t0. This is a
linear “memory” mechanism.

Figure 4. Four curves showing power law adaptation in response to a step
increase in stimulus in four different receptors: cockroach leg mechanoreceptor, in response to distortion of the tactile spine on the femur (curve
A); slit sensillum on the leg of the hunting spider in response to 1,200 Hz
sound (curve B); slowly adapting stretch receptor of the crayfish (curve C);
and increase of response over light-adapted level of Limulus lateral-eye
eccentric cell to an increase in light intensity (curve D). (Examples from
Thorson, J., and Biedermann-Thorson, M., 1974, Distributed relaxation
processes in a sensory adaptation, Science, 183:161–172. Copyright 1974,
American Association for the Advancement of Science; reprinted with
permission.)

Such adaptation is particularly interesting both because it is so
prevalent and because it may have an important role in optimizing
information transmission. Fractional-differentiation-like behavior
is observed in fly photoreceptors, and in that case, the exponent of
the fractional differentiator appears to be matched to the spectrum
of natural stimuli (van Hateren and Snippe, 2001). Thus the effect
of the transformation is to whiten the spectrum of natural signals.
Because many natural stimuli have power-law characteristics, it is
intriguing to speculate that fractional differentiation at the sensory
periphery may be a general neural mechanism for whitening input
statistics.

Mechanisms
Adaptation requires retaining memory of activity over extended
time scales. These long time scales can arise from a number of
sources. Intracellular calcium concentration has been identified as
playing an important role in information processing, acting as a
slowly changing “integrator” of activity. Other forms of adaptation,
particularly the power-law-like behavior discussed in the previous
section, are also likely to be a property of single cells rather than
of the network. Recent biophysical studies show that membrane
dynamics can have long time scales that retain memory of the history of stimulation/activity over hundreds of seconds (Marom,
1998). This could be brought about either by the modification of
intrinsic properties or by intrinsic properties that have built-in long
time scales through state-dependent inactivation (Turrigiano, Marder, and Abbott, 1996; Marom, 1998).

Calcium as an Integrator of Activity
Figure 3. Illustration of some properties of a fractional differentiator with
exponent ␣ ⳱ 0.3. (a) A step function stimulus leads to a power law decaying rate. In a log-log plot the curve would appear as a straight line with
slope ⳮ␣. (b) A square wave leads to a similar adaptation curve as shown
in Figure 1.

Each spike introduces a roughly constant amount of calcium into
the cell through voltage-dependent Ca2Ⳮ channels. The Ca2Ⳮ concentration then decays slowly. Thus, [Ca2Ⳮ] can be modeled as a
leaky integrator of activity, with a decay time scale of ca. 100 ms.
This calcium signal can allow activity-dependent regulation of sub-

Adaptive Spike Coding
sequent neural activity through the modification of conductances
(see ACTIVITY-DEPENDENT REGULATION OF NEURONAL CONDUCTANCES).
Recent evidence indicates that single-cell properties may contribute to contrast adaptation in cortex (Sanchez-Vives, Nowak, and
McCormick, 2000). Previous work has shown that contrast adaptation is associated with hyperpolarization of the membrane potential in cat area 17 neurons. By stimulating the neurons directly with
injected current, effects similar to contrast adaptation are seen
(though less dramatically than to real visual input). This suggests
that these effects can be induced through the modulation of intrinsic
cell properties; the activation of Ca2Ⳮ- and NaⳭ-dependent potassium conductances is indicated.

State-Dependent Channel Dynamics
In some cases the relevant dynamics may be due to the complex
behavior of the channels themselves. Recently it has become clear
that the dynamics of inactivation provide the membrane with the
possibility for extended history dependence (Marom, 1998).
A simplified picture of the gating of voltage-gated ion channels
is a three-state scheme:
C⇔O⇔I

as approximately constant activity (see ACTIVITY-DEPENDENT
REGULATION OF NEURONAL CONDUCTANCES). Closer to our earlier
discussion, Stemmler and Koch (1999) derive a learning rule for
conductances that maximizes the mutual information between input
and output, where the output is taken to be the neuron’s firing rate.
The learning rule adjusts conductances at every new presentation
of the stimulus, subject to biologically plausible constraints. Under
this learning rule, a realistic conductance-based model neuron was
indeed able to learn a changing distribution and adjust its firing
statistics accordingly. The time scales treated were orders of magnitude longer than those observed experimentally in Fairhall et al.
(2001) and predicted theoretically from statistical considerations.
Experimental evidence is still required to determine whether such
a model is realistic.
As noted, many adaptation processes in sensory receptors follow
a power-law relaxation. Assuming that most elementary processes
involve a single time scale, with exponential dynamics, Thorson
and Biederman-Thorson (1974) proposed that power laws may
arise from a superposition of many elementary processes with a
wide range of time scales. From the definition of the gamma
function,
tⳮ␣ ⳱

(1)

where channels can be either closed (C), open (O), or inactivated
(I). Generally, the transition between closed and open is voltagedependent and rapid, on the order of the duration of an action potential. The transition between open and inactivated, on the other
hand, is voltage-independent and can have very long time scale
dynamics. Intriguingly, studies in vitro show that some sodium
channel types have inactivation rates that scale with the duration
of the input (Marom, 1998), providing time scales of up to several
minutes. The precise mechanism underlying this large variety of
time scales is not yet well understood; it is hypothesized that the
system cascades through a multiplicity of inactivation states. Earlier theoretical work has shown that the coupling of many states
leads to a scaling relation between the duration of activity and the
rate of recovery from inactivation.
In a step closer to a realistic preparation, the dynamic clamp was
applied to cultured stomatogastric ganglion neurons to add an effective slowly inactivating potassium current (Turrigiano et al.,
1996). As had been observed previously, this produced long delays
to firing during depolarization, and an increase in excitability with
a time scale much longer than the duration of the input. Further,
the slow channel dynamics produced a long-lasting effect on the
firing properties of the neuron.
In vivo, the contribution of slowly inactivating sodium channels
to power-law-like adaptation has been suggested. Mechanosensory
neurons in the cockroach femoral tactile spine have been shown to
display power-law adaptation. From intracellular measurements,
Basarsky and French (1991) found that the spike rate adaptation is
due to cumulative slowing of the recovery of the membrane potential between spikes. Previous work had demonstrated that calcium
channel blockers or blockers of Ca2Ⳮ-activated KⳭ channels did
not reduce adaptation, while modifying sodium channel inactivation did.
These mechanisms might be seen as primitives for short-term
“learning and memory.”

93

1
C(␣)



冮

dr r ␣ⳮ1eⳮrt

(2)

0

a power law may be generated by a weighted sum of exponentials
with a range of time scales. This distribution was considered to be
generated through geometric factors, such as the inhomogeneous
distribution of elements within the receptor.
This model has met with some skepticism because of the requirements both for a continuous distribution of time scales and
for these to be present in the appropriate proportions. It has been
noted that power-law-like behavior results from much less stringent
conditions: the superposition of only a few exponentials can produce a power law over the decade or two normally available to
experiment. However, recent experimental advances, outlined in
the previous section, may provide a better underpinning for the
derivation of power-law adaptation from membrane mechanisms.

Adaptation of Receptive Fields
As noted in the Introduction, a neuron can be modeled as a combination of feature extraction (linear filtering) and a nonlinear decision function (or threshold). Although we have discussed the effects of adaptation on the nonlinear decision function, adaptation
can also affect the feature that causes the neuron to spike: the receptive field can depend on the ensemble of inputs. Although this
result had been frequently observed in work on invertebrate vision,
recent experiments demonstrate analogous results for cortical receptive fields. Sceniak et al. (1999) show that the extent of spatial
summation implemented by neurons in V1 depends adaptively on
contrast; this has parallels in the adaptation of filters in retina
(Laughlin, 1989). Theunissen, Sen, and Doupe (2000) found that
the spatiotemporal receptive fields of neurons in auditory cortex
showed a strong dependence on the stimulus ensemble. This is a
natural consequence of neural nonlinearity, but such a dependence
is also necessary for optimal information processing.

Discussion
Modeling
Historically, attempts to model adaptation have considered the process to involve a dynamic threshold. More recently, modeling approaches have taken a functional perspective on the outcome of
adaptation and have proposed algorithms whereby the conductances may adjust to provide the cell with desirable properties, such

The ubiquity of adaptation throughout the nervous system should
be proof of its fundamental importance. Although the phenomenology of adaptation, particularly to constant stimuli, has been extensively explored, recent experimental and theoretical approaches
have made contact with the principles of information theory in
order to evaluate adaptive coding. For fly motion-sensitive neurons,

94

Part III: Articles

it was found that the coding strategy of the system adapts rapidly
and continuously to track dynamic changes in the statistics of the
stimulus.
We have discussed a variety of mechanisms that may implement
adaptive coding at the level of single cells. Although it is likely
that systems will implement such important behavior at many levels, it is appealing that the simplest elements of neural computation
have the power to carry out dynamic aspects of information
processing.
Road Map: Neural Coding
Related Reading: Population Codes; Sensory Coding and Information
Transmission

References
Basarsky, T., and French, A., 1991, Intracellular measurements from a rapidly adapting sensory neuron, J. Neurophysiol., 65:49–56.
Brenner, N., Bialek, W., and de Ruyter van Steveninck, R., 2000, Adaptive
rescaling maximizes information transmission, Neuron, 26:695–702.
Fairhall, A. L., Lewen, G., Bialek, W., and de Ruyter van Steveninck, R. R.,
2001, Efficiency and ambiguity in an adaptive neural code, Nature,
412:787–792.
Johnson, K. O., 2001, The roles and functions of cutaneous mechanoreceptors, Curr. Opin. Neurobiol., 11:455–61. ⽧

Laughlin, S. B., 1989, The role of sensory adaptation in the retina, J. Exp.
Biol., 146:39–62. ⽧
Marom, S., 1998, Slow changes in the availability of voltage-gated ion
channels: Effects on the dynamics of excitable membranes, J. Membr.
Biol., 161:105–113. ⽧
Sanchez-Vives, M., Nowak, L., and McCormick, D., 2000, Membrane
mechanisms underlying contrast adaptation in cat area 17 in vivo, J.
Neurosci., 20:4267–4285.
Sceniak, M. P., Ringach, D. L., Hawken, M. J., and Shapley, R., 1999,
Contrast’s effect on spatial summation by macaque V1 neurons, Nature
Neurosci., 2:733–739.
Smirnakis, S. M., Berry, M. J., Warland, D. K., Bialek, W., and Meister,
M., 1997, Adaptation of retinal processing to image contrast and spatial
scale, Nature, 386:69–73.
Stemmler, M., and Koch, C., 1999, How voltage-dependent conductances
can adapt to maximize the information encoded by neuronal firing rate,
Nature Neurosci., 2:521–527.
Theunissen, F., Sen, K., and Doupe, A., 2000, Spectral-temporal receptive
fields of nonlinear auditory neurons obtained using natural sounds, J.
Neurosci., 20:2315–2331.
Thorson, J., and Biederman-Thorson, M., 1974, Distributed relaxation processes in a sensory adaptation, Science, 183: 161–172. ⽧
Torre, V., Ashmore, J. F., Lamb, T. D., and Menini, A., 1995, Transduction
and adaptation in sensory receptor cells, J. Neurosci., 15:7757–7763. ⽧
Turrigiano, G., Marder, E., and Abbott, L., 1996, Cellular short-term memory from a slow potassium conductance, J. Neurophysiol., 75:963–968.
van Hateren, J. H., and Snippe, H. P., 2001, Information theoretical evaluation of parametric models of gain control in blowfly photoreceptor
cells, Vision Res., 41:1851–1865.

Amplification, Attenuation, and Integration
H. Sebastian Seung
Introduction
Differential equations such as
sẋi Ⳮ xi ⳱ f

冢兺 W x Ⳮ b 冣
ij j

i

(1)

j

have long been used to model networks of interacting neurons (Ermentrout, 1998; PHASE-PLANE ANALYSIS OF NEURAL NETS). The
activity of neuron i is represented by a single dynamical variable
xi, and its input-output characteristics by a single transfer function
f. There are more biophysically realistic descriptions of neural networks that include many dynamical variables per neuron, in order
to explicitly model dendritic integration, action potential generation, and synaptic transmission. Nevertheless, simplified models
like that in Equation 1 have been useful for understanding how the
computational properties of neural networks are related to their
synaptic organization.
The parameter Wij in Equation 1 represents the strength of the
synapse from neuron j to i. These synapses are termed recurrent,
as they connect to other neurons in the same network. Feedforward
synaptic input from outside the network is implicit in the bias bi.
The feedforward synapses could be made explicit by writing
bi ⳱ b0i Ⳮ aVia za, where za are input neuron activities, Via the
strengths of the feedforward synapses, and b0i any intrinsic tendency of neuron i to be active. But the feedforward connections
will be left implicit in the following, so as to focus on the computational role of the recurrent connections.
Accordingly, the biases bi in Equation 1 will be regarded as the
inputs to the network, while the activities xi are the outputs. If there
were no recurrent synapses (Wij ⳱ 0 for all i and j), then each
neuron i would respond by low-pass filtering the signal f (bi) with
time constant s. When there are recurrent synapses, a general char-

acterization of the response properties of a network is difficult, but
the situation is greatly simplified when nonlinearity is neglected.
Putting the transfer function f (u) ⳱ u in Equation 1 yields the linear
network
sẋi Ⳮ xi ⳱

兺j Wij xj Ⳮ bi

(2)

which can be completely analyzed using the tools of linear systems
theory. The modest goal of this article is to describe some properties of linear networks and give examples of their application to
neural modeling.
In particular, the focus is on the role of recurrent synaptic connectivity. Provided that they do not lead to instability, the recurrent
connections alter both the gain and speed of response to feedforward input. Either they amplify and slow down responses to feedforward input, or they attenuate and speed up responses. Both effects can occur simultaneously in the same network, as can be seen
by mathematically transforming the network of interacting neurons
into a set of noninteracting eigenmodes. The effect of the recurrent
synapses generally varies from mode to mode.
Besides amplification and attenuation, a linear network can also
carry out the operation of temporal integration, in the sense of
Newtonian calculus. This happens when the strength of feedback
is precisely tuned for an eigenmode, so that its gain and time constant diverge to infinity.
Admittedly, the neglect of nonlinearity is a step away from biological realism. Nevertheless, linear models are important because
they give insight into the local behavior of nonlinear networks,
which can often be linearly approximated in the vicinity of fixed
points. And the linear computations of amplification, attenuation,
and integration have been ascribed to a number of brain areas.

Amplification, Attenuation, and Integration

Autapse
The simplest example of a recurrent synapse is a single neuron
with a synapse onto itself, or autapse, in the terminology of neurophysiology. For this case, the dynamics (Equation 2) takes the
form
sẋ Ⳮ x ⳱ Wx Ⳮ b

(3)

The autapse has strength W and is said to be excitatory if W  0
and inhibitory if W  0. The example is not meant to be a realistic
model of a biological autapse; it is only a simple illustration of
some of the effects of recurrent synaptic connections. The parameter W will also be called the strength of feedback, in the terminology of engineering. Without feedback (W ⳱ 0), the neuron acts
as a low-pass filter of input b with time constant s. When the effect
of feedback is considered, the first distinction that has to be made
is between the unstable W  1 and the stable W  1 cases. (Discussion of the borderline W ⳱ 1 case is postponed until later.)
If W  1, the autapse is unstable, as can be seen by solving
Equation 3 for input b that is constant in time. The solution diverges
exponentially to infinity, because the feedback is so strong that it
leads to runaway instability. Note that in a more realistic model,
the growth of this runaway instability would eventually be limited
by nonlinearity, but in the idealized linear model (Equation 3),
divergence to infinity is possible.
If W  1, the autapse is stable, and the dynamics (Equation 3)
can be rewritten in the form
s
b
ẋ Ⳮ x ⳱
1ⳮ W
1ⳮW

(4)

From this formula can be read two numbers that characterize the
autapse: the steady-state gain, and the time constant of response.
The gain is operationally defined by holding the input constant and
allowing the output to relax to the steady-state value x ⳱ b/(1 ⳮ
W). Then the steady-state gain, defined as the ratio of output x to
input b, is 1/(1 ⳮ W). By this definition, the gain is exactly unity
in the case of no feedback (W ⳱ 0). Positive (W  0) and negative
(W  0) feedback have different effects. Positive feedback amplifies, boosting the gain to a value greater than 1. Negative feedback
attenuates, making the gain less than 1.
Positive and negative feedback also have opposite effects on the
speed of response. The time constant of the exponential relaxation
to the steady state is s/(1 ⳮ W). In the case of no feedback, this is
equal to the fundamental time constant s. But positive feedback
lengthens the time constant, while negative feedback shortens it.
This means that there is a trade-off between amplification and
speed, sometimes known as the gain-bandwidth trade-off. Intuitively speaking, the trade-off arises because feedback amplification
requires that the signal circulate in the feedback loop, so that more
amplification requires more time.
In summary, a feedback loop containing a perfectly linear element behaves in a simple way. Positive feedback (W  0) amplifies
and slows down response, assuming that it doesn’t lead to instability. Negative feedback (W  0) attenuates and speeds up
response.
The idea of amplification by positive feedback has been prominent in a number of models of primary visual cortex (Douglas et
al., 1995). Neurons in layer 4 receive both feedforward drive from
the thalamus and recurrent input from other cortical neurons. It has
been proposed that the recurrent interactions amplify the responses
to feedforward input. To test this idea, Ferster and colleagues recorded from layer 4 neurons. They inactivated corticocortical inputs both by cooling (Ferster, Chung, and Wheat, 1996) and electrical stimulation (Chung and Ferster, 1998). In both cases, they
measured a two- or threefold reduction in the amplitude of cortical

95

responses to visual stimulation, which was interpreted as a loss of
amplification by positive feedback.
The above discussion omitted the special case of W ⳱ 1, which
is the borderline between stability and instability. For W ⬆ 1, there
was exactly one steady state, which was either stable or unstable,
depending on whether W was less than or greater than 1. In contrast,
if W ⳱ 1, there is not a unique steady state. The number of steady
states depends on b. There are infinitely many if b ⳱ 0, and none
at all if b ⬆ 0. To understand the case of non-zero b, it is helpful
to return to Equation 3, which reduces to sẋ ⳱ b. In other words,
the response x is the time integral of b. Therefore, a linear autapse
can act as an integrator, if the strength of feedback is precisely
tuned (Seung et al., 2000). Variants of this idea have been used to
model neural integrators, brain areas that integrate their inputs in
the sense of Newtonian calculus (Robinson, 1989).

Mutually Inhibitory Pair
While the autapse illustrates the gain-bandwidth trade-off in feedback amplification, it involves only a single neuron, and cannot
capture genuine population behaviors. A more interesting example
consists of two linear neurons with mutual inhibition:
sẋ1 Ⳮ x1 ⳱ ⳮbx 2 Ⳮ b1

(5)

sẋ 2 Ⳮ x 2 ⳱ ⳮbx1 Ⳮ b2

(6)

The parameter b is assumed to be positive, so that the interaction
is inhibitory. This dynamics is more complex than Equation 3 because it involves two differential equations that are coupled to each
other. Luckily, it turns out that the equations can be decoupled by
adding and subtracting them.
Adding the two equations yields an equation for the common
mode xc ⳱ x1 Ⳮ x2,
s

d
(x Ⳮ x 2) Ⳮ (x1 Ⳮ x 2) ⳱ ⳮb(x1 Ⳮ x 2) Ⳮ (b1 Ⳮ b2)
dt 1

(7)

Comparison with Equation 3 reveals that the common mode behaves like an autapse with negative feedback. Therefore the common mode attenuates its input b1 Ⳮ b2 with steady-state gain 1/(1
Ⳮ b) and time constant s/(1 Ⳮ b).
Similarly, subtracting the two equations yields an equation for
the differential mode xd ⳱ x1 ⳮ x 2,
s

d
(x ⳮ x 2) Ⳮ (x1 ⳮ x 2) ⳱ b(x1 ⳮ x 2) Ⳮ (b1 ⳮ b2)
dt 1

(8)

The differential mode behaves like an autapse with positive feedback. If b  1, the differential mode is unstable. If b  1, then the
differential mode amplifies its input b1 ⳮ b2 with steady-state gain
1/(1 ⳮ b) and time constant s/(1 ⳮ b).
To recapitulate, transforming from (x1, x2) to (xc, xd) formally
decoupled the mutually inhibitory pair of neurons into two “virtual”
autapses. Note that the transformation is reversible, as x1 and x2
can be reconstructed from the common and differential modes, e.g.,
x1 ⳱ (xc Ⳮ xd)/2.
A striking aspect of this example is that mutual inhibition has
completely opposite effects on the common and differential modes.
For the common mode, inhibition mediates negative feedback,
which leads to attenuation. But inhibition mediates positive feedback for the differential mode, which leads to amplification.
The general lesson to be drawn is that no direct correspondence
exists between the sign of synaptic connections and the sign of
feedback. This is because a synapse is local, belonging to just two
neurons. In contrast, feedback strength is global, belonging to a
distributed mode of the network. As will be described below, the
feedback strength is given in general by the eigenvalues of the
synaptic weight matrix W. The autapse is a special exception for

96

Part III: Articles

which the sign of the synaptic connection directly corresponds to
the sign of feedback, but this does not hold true in general.
The idea that inhibition between neurons can amplify differences
has been used to explain the fact that visual systems are more
sensitive to relative luminance, or contrast, than to absolute luminance. For example, the Limulus retina consists of visual receptors
that are topographically organized in a two-dimensional network
and interact via lateral inhibition. Measurements of retinal output
reveal enhancement of luminance differences, a fact that has been
successfully explained using network models that are generalizations of the mutually inhibitory pair considered here (Hartline and
Ratliff, 1972).
The special case b ⳱ 1 is also of interest. It is the borderline of
stability for the differential mode. If b1 ⳮ b2 is zero, then the
differential mode x1 ⳮ x 2 is constant in time, according to Equation
8, while the common mode x1 Ⳮ x 2 converges exponentially to the
value (b1 Ⳮ b2)/2. This is a simple example of a line attractor, a
line of fixed points to which all trajectories are attracted (Seung,
1996). More complex nonlinear network models with approximate
line attractors have been used to model the phenomenon of persistent neural activity (Seung, 1996; Zhang, 1996).
Note that having a continuous set of fixed points is an unusual
situation, requiring the precise tuning of the inhibitory strength b
and the differential input b1 ⳮ b2. When b1 ⳮ b2 is non-zero, then
it is integrated by the differential mode. In this case, inhibitory
interactions yield an integrator, in contrast to the autapse, which
requires excitatory feedback to integrate. Robinson et al. proposed
that lateral inhibition is the mechanism of the oculomotor neural
integrator, which converts vestibular and other velocity-coded inputs into eye position outputs (Cannon, Robinson, and Shamma,
1983).

General Network
For a general network of N neurons, the effects of feedback can be
understood via eigensystem analysis. It is convenient to rewrite the
dynamics in Equation 2 in matrix-vector form as
s

d
x Ⳮ x ⳱ Wx Ⳮ b
dt

(9)

where x and b are vectors and W is the synaptic weight matrix.
Suppose that the weight matrix can be factorized as W ⳱ SKSⳮ1,
where K is a real diagonal matrix and S is a real invertible matrix.
A sufficient condition for a real diagonalization is that the weight
matrix W be symmetric, but this is not a necessary condition. The
diagonal entries of K are the eigenvalues of W. The columns of S
are the right eigenvectors of W, and the rows of Sⳮ1 are the left
eigenvectors.
Recall that transforming to the common and differential modes
simplified the dynamics of the mutually inhibitory pair. The analogue here is to change from x and b to
x̃ ⳱ Sⳮ1x,

b˜ ⳱ Sⳮ1b

These vectors can be used to express x and b as linear combinations
of the right eigenvectors, x ⳱ Sx̃ and b ⳱ Sb̃.
The transformation of Equation 9 is effected by multiplying with
Sⳮ1,
d
s
x̃ Ⳮ x̃ ⳱ Sⳮ1Wx Ⳮ b˜
dt

(10)

⳱ Sⳮ1WSx̃ Ⳮ b˜

(11)

⳱ Kx̃ Ⳮ b˜

(12)

Writing out the last expression component by component yields

s

d
x̃a Ⳮ x̃a ⳱ ka x̃a Ⳮ b˜ a
dt

where ka is the ath diagonal element of K, or equivalently the ath
eigenvalue of W. This is a great simplification: the network (Equation 9) of N interacting neurons has been transformed into N noninteracting “virtual” autapses. Each autapse has feedback with
strength given by the eigenvalues ka. Assuming that the eigenvalues are less than or equal to 1, each autapse can perform the operations of amplification, attenuation, or integration.

Discussion
In this article, some effects of recurrent synaptic connectivity on
linear networks were characterized. The autapse example demonstrated that there is a gain-bandwidth trade-off in amplification and
attenuation by feedback, and the possibility of integration when
feedback is precisely tuned. The mutually inhibitory pair illustrated
the decoupling of an interacting network into “virtual” autapses,
and also illustrated that the sign of feedback is not directly related
to the sign of synaptic connections. Such a decoupling is generally
possible for any synaptic weight matrix W that is diagonalizable
with all real eigenvalues.
More generally, the eigenvalues (and eigenvectors) are complex
numbers. When an eigenvalue of W has a non-zero imaginary part,
the corresponding eigenmode exhibits oscillatory behavior. Accordingly, linear analyses have been used to explain the existence
of oscillations in some neural network models (Li and Hopfield,
1989).
It is natural to ask whether the concepts introduced above have
any relevance for nonlinear neural networks. A simple way of modeling nonlinearity is to introduce a threshold for activation by
choosing f (x) ⳱ max{x, 0} for the transfer function in Equation 1.
Because the resulting dynamics are piecewise linear, eigenvalues
and eigenvectors are still essential for mathematical analysis (Hadeler and Kuhn, 1987; Hahnloser et al., 2000), but the threshold
nonlinearity leads to a richer variety of dynamical behaviors. A full
discussion of threshold linear networks is beyond the scope of this
article, but let us briefly reconsider the example of a mutually inhibitory pair of neurons presented with inputs that are constant in
time. For linear neurons, the mutual inhibition caused differences
in input to be amplified in the steady-state response. If the neurons
are instead threshold linear, winner-take-all behavior can result for
some choices of model parameters. Then only a single neuron is
active at steady state, no matter how small the difference in inputs
may be (Amari and Arbib, 1977). As in the purely linear case, the
difference in steady-state outputs is greater than the difference in
inputs. However, this behavior cannot be explained in terms of a
simple linear amplification. For a more detailed explanation, see
WINNER-TAKE-ALL NETWORKS.
Road Map: Dynamic Systems
Background: I.3. Dynamics and Adaptation in Neural Networks
Related Reading: Pattern Formation, Neural; Winner-Take-All Networks

References
Amari, S., and Arbib, M. A., 1977, Competition and cooperation in neural
nets, in Systems Neuroscience (J. Metzler, Ed.), New York: Academic
Press, pp. 119–165. ⽧
Cannon, S. C., Robinson, D. A., and Shamma, S. 1983, A proposed neural
network for the integrator of the oculomotor system, Biol. Cybern.,
49:127–136.
Chung, S., and Ferster, D., 1998, Strength and orientation tuning of the
thalamic input to simple cells revealed by electrically evoked cortical
suppression, Neuron, 20:1177–1189.

Analog Neural Networks, Computational Power
Douglas, R. J., Koch, C., Mahowald, M., Martin, K. A. C., and Suarez,
H. H., 1995, Recurrent excitation in neocortical circuits, Science,
269:981–985.
Ermentrout, B., 1998, Neural networks as spatio-temporal pattern-forming
systems, Rep. Prog. Phys., 61:353–430. ⽧
Ferster, D., Chung, S., and Wheat, H., 1996, Orientation selectivity of thalamic input to simple cells of cat visual cortex, Nature, 380(6571):249–
252.
Hadeler, K. P., and Kuhn, D., 1987, Stationary states of the Hartline-Ratliff
model, Biol. Cybern., 56:411–417.
Hahnloser, R. H., Sarpeshkar, R., Mahowald, M. A., Douglas, R. J., and
Seung, H. S., 2000, Digital selection and analogue amplification coexist
in a cortex-inspired silicon circuit, Nature, 405(6789):947–951.
Hartline, H. K., and Ratliff, F., 1972, Inhibitory interaction in the retina of
Limulus, in Handbook of Sensory Physiology: Physiology of Photore-

97

ceptor Organs (M. G. F. Fuortes, Ed.), Berlin: Springer-Verlag, pp. 382–
447.
Li, Z., and Hopfield, J. J., 1989, Modeling the olfactory bulb and its neural
oscillatory processings, Biol. Cybern., 61:379–392.
Robinson, D. A., 1989, Integrating with neurons, Annu. Rev. Neurosci.,
12:33–45.
Seung, H. S., 1996, How the brain keeps the eyes still, Proc. Natl. Acad.
Sci. USA, 93:13339–13344.
Seung, H. S., Lee, D. D., Reis, B. Y., and Tank, D. W., 2000, The autapse:
A simple illustration of short-term analog memory storage by tuned synaptic feedback, J. Comput. Neurosci., 9:171–185. ⽧
Zhang, K., 1996, Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: A theory, J. Neurosci.,
16:2112–2126.

Analog Neural Networks, Computational Power
Bhaskar DasGupta and Georg Schnitger
Introduction
The last two decades have seen a surge in theoretical techniques to
design and analyze the performance of neural nets as well as novel
applications of neural nets to various applied areas. Theoretical
studies on the computational capabilities of neural nets have provided valuable insights into the mechanisms of these models.
In subsequent discussion, we distinguish between feedforward
neural nets and recurrent neural nets. The architecture of a feedforward net ᏺ is described by an interconnection graph and the
activation functions of ᏺ. A node (processor or neuron) v of ᏺ
computes a function
k

cv

冢兺 a
i⳱1

冣

Ⳮ bv

vi ,v u vi

(1)

of its inputs u v1, . . . , u vk. These inputs are either external (i.e.,
representing the input data) or internal (i.e., representing the outputs of the immediate predecessors of v). The coefficients avi ,v (respectively bv) in Equation 1 are the weights (respectively threshold)
of node v, and the function cv is the activation function of v. No
cycles are allowed in the interconnection graph, and the output of
designated nodes provides the output of the network. A recurrent
neural net, on the other hand, allows cycles, thereby providing potentially higher computational capabilities. The state uv of node v
in a recurrent net is updated over time according to
k

冢兺 a

u v(t Ⳮ 1) ⳱ cv

i⳱1

vi ,v uvi(t)

冣

Ⳮ bv

(2)

In this article, we emphasize the exact and approximate representational power of feedforward and recurrent neural nets. This
line of research can be traced back to Kolmogorov (1957), who
essentially proved the first existential result on the (exact) representation capabilities of neural nets (cf. UNIVERSAL APPROXIMATORS). The need to work with “well-behaved” activation functions,
however, enforces approximative representations of target functions, and the question of the approximation power (with limited
resources) becomes fundamental.
The representation power of neural nets has immediate consequences for learning, since we cannot learn (approximately) what
we cannot represent (approximately). On the other hand, the complexity of learning increases with increasing representational power
of the underlying neural model, and care must be exercised to strike
a balance between representational power, on the one hand, and

learning complexity, on the other. The emphasis of this article is
on representational power, i.e., what can be represented with networks using a given set of activation functions, rather than on learning complexity.
In this article, we discuss only a small subset of the literature on
this topic. After introducing the basic notation, we discuss the representational power of feedforward and recurrent neural nets. There
follows a brief discussion of networks of spiking neurons and their
relation to sigmoidal nets, with a summary statement.

Models and Basic Definitions
In this section we present the notation and basic definitions used
in subsequent sections. For real-valued functions we measure the
approximation quality of function f by function g (over a domain
D 債 ⺢ n) by the Chebychev norm,
㛳 f ⳮ g㛳 D ⳱ sup{| f (x) ⳮ g(x)| : x 僆 D}
(the subscript D will be omitted when clear from the context). To
emphasize the selection of activation functions, we introduce the
concept of C-nets for a class C of real-valued activation functions.
A C-net ᏺ assigns only functions in C to nodes. We assume that
each function in C is defined on some subset of ⺢, and require that
C contain the identity function by default (thus allowing weighted
additions as node outputs). Finally, we restrict our attention to Cnets with a single output node.
The depth of a feedforward net ᏺ is the length of the longest
path of the (acyclic) interconnection graph of ᏺ, and the size of ᏺ
is the number of nodes. The hidden nodes are all nodes excluding
all input nodes and the output node.
The class of important activation functions is rather large and
includes, among others, the binary threshold function
Ᏼ(x) ⳱

冦01

if x ⱕ 0,
if x  0,

the cosine squasher, the Gaussian, the standard sigmoid r(x) ⳱ 1/
(1 Ⳮ eⳮx), the hyperbolic tangent, (generalized) radial basis functions, polynomials and trigonometric polynomials, splines, and rational functions.
Care must be exercised when using a neural net with continuous
activation functions to compute a Boolean-valued function, since

98

Part III: Articles

in general, the output node computes a real number. A standard
output convention in this case is as follows (see Maass, 1994):
Definition 1. A C-net ᏺ computes a Boolean function F: ⺢ n r
{0, 1} with separation e  0 if there is some t 僆 ⺢ such that for
any input x 僆 ⺢ n, the output node of ᏺ computes a value that is
at least t Ⳮ e if F(x) ⳱ 1, and at most t ⳮ e otherwise.
Recurrent neural nets, unlike their feedforward counterparts, allow loops in their interconnection graph. Certainly asynchronous
recurrent nets are an important neural model, but we assume in
Equation 2 that all nodes update synchronously at each time step.
Typically, besides internal and external data lines, some of the inputs and outputs are validation lines, indicating if there is any input
or output present at the time.

Computational Power of Feedforward Nets
The simple perceptron as a feedforward neural net with one layer
has only limited computational abilities. For instance, if we restrict
ourselves to one-node simple perceptrons and assume monotone,
but otherwise arbitrary, activation functions, then the XOR function
XOR(x1, x 2) ⳱ x1 丣 x 2 cannot be computed.
On the other hand, if we choose the binary threshold function Ᏼ
as activation function, then the learning problem for simple perceptrons is efficiently solvable by linear programming. This positive result is also extendable to a large class of activation functions,
including the standard sigmoid. But simple perceptrons should not
be underestimated, since the problem of approximately minimizing
the missclassification ratio (when the target function is not representable as a simple perceptron) has been shown to be (probably)
intractable (Arora et al., 1997).
However, the power of feedforward nets increases significantly
when networks of more layers are considered. In fact, a result of
Kolmogorov (refuting Hilbert’s 13th problem for continuous functions), when translated into neural net terminology, shows that any
continuous function can be computed exactly by a feedforward net
of depth 3.
Theorem 1 (Kolmogorov, 1957). Let n be a natural number. Then
there are continuous functions h1, . . . , h 2nⳭ1 : [0, 1] r ⺢ such
that any continuous function f : [0, 1]n r ⺢ can be represented as
2nⳭ1

f (x) ⳱

兺

j⳱1

n

冢 兺 ␣ h (x )冣

g

i

j

i

i⳱1

where the function g as well as the weights ␣1, . . . , ␣n depend
on f.
But, unfortunately, the function g depends on the function to be
represented. Moreover, the functions h j are nondifferentiable and
hence cannot be used by current learning algorithms. For further
discussion, we refer the reader to Poggio and Girosi (1989).
However, if we only allow everywhere differentiable activation
functions (such as the standard sigmoid), then we can only represent everywhere differentiable target functions. Thus, one has to
relax the requirement of exact representation, and demand only that
the approximation error (in an appropriate norm) is small. Applying
the Stone-Weierstrass theorem one obtains, for instance, (trigonometric) polynomials as universal approximators, and hence we get
neural nets with one hidden layer as universal approximators.
Cybenko (1989) considers activation functions from the class of
continuous discriminatory functions. This class contains, for instance, sigmoidal functions, i.e., continuous functions r satisfying
r(t) r

冦10

as t r Ⳮ
as t r ⳮ

Theorem 2. Let r be a continuous discriminatory function and let
f : [0, 1]n r ⺢ be a continuous target function. Then, for every e
 0 and for sufficiently large N (where N depends on the target
function f and e), there exist weights ␣ij , wj and thresholds bj , such
that 㛳 f ⳮ g㛳  e, where g ⳱ Nj⳱1wj • r(ni⳱1 ␣ij • xi Ⳮ bj).
In particular, one hidden layer suffices to approximate any continuous function by sigmoidal nets within arbitrarily small error.
Further results along this line are shown by Hornik; Stinchcombe
and White; Funahashi, Moore, and Poggio; and Poggio and Girosi,
to mention just a few. Whereas most arguments in the abovementioned results are nonconstructive, Carroll and Dickinson describe a method using Radon transforms to approximate a given L 2
function to within a given mean square error.
Barron (1993) discusses the approximation quality achievable
by sigmoidal nets of small size. In particular, let B nr (0) denote the
n-dimensional ball with radius r around 0 and let f : B nr (0) r ⺢ be
the target function. Assume that F is the magnitude distribution of
the Fourier transform of f.
Theorem 3 (Barron, 1993). Let r be any sigmoidal function. Then
for every probability measure l and for every N there exist weights
␣ij , wj and thresholds bj , such that

冮 冢
B nr (0)

N

f (x) ⳮ

兺
j⳱1

n

wj • r

冢兺 ␣

ij

i⳱1

2

冣冣 l(dx)

• xi Ⳮ bj

ⱕ

(2r • |w| • F(dw))2
N

Set Cf ⳱ r • |w| • F(dw), and the approximation error achievable
by sigmoidal nets of size N is bounded by (2 • Cf)2/N. However,
Cf may depend superpolynomially on n, and the curse of dimensionality may strike. As an aside, the best achievable squared error
for linear combinations of N basis functions will be at least X(Cf /
n • N 1/n) for certain functions f (Barron, 1993), and hence neural
networks are superior to conventional approximation methods from
this point of view.
The results just enumerated show that depth-2 feedforward nets
are universal approximators. This dramatically increased computing power, however, has a rather negative consequence. Kharitonov
(1993) showed that under certain cryptographic assumptions, no
efficient learning algorithm will be able to predict the input-output
behavior of binary threshold nets with a fixed number of layers.
This result holds even when experimentation is allowed, that is,
when the learning algorithm is allowed to submit inputs for
classification.
In the next section, we compare important activation functions
in terms of their approximation power, when resources such as
depth and size are limited. The following section discusses networks of spiking neurons. Lower size bounds for sigmoidal nets
are mentioned when we compare networks of spiking neurons and
sigmoidal nets.

Efficient Approximation by Feedforward Nets
Our discussion will be informal, and we refer the reader to
DasGupta and Schnitger (1993) for details. Our goal is to compare
activation functions in terms of the size and depth required to obtain
tight approximations. Another resource of interest is the Lipschitz
bound of the net, which is a measure of the numerical stability of
the circuit. Informally speaking, for a net ᏺ to have Lipschitzbound L, we first demand that all weights and thresholds of ᏺ be
bounded in absolute value by L. Moreover, we require that each
activation function of ᏺ have (the conventional) Lipschitz-bound
L on the inputs it receives. Finally, the actually received inputs

Analog Neural Networks, Computational Power
have to be bounded away from regions with higher Lipschitz
bounds.
We formalize the notion of having essentially the same approximation power.
Definition 2. Let C1 and C2 be classes of activation functions.
(a) We say that C2 simulates C1 (denoted by C1 ⱕ C2) if and only
if there is a constant k ⱖ 1 such that for all C1-nets C2 of size
at most s, depth at most d, and Lipschitz bound 2s, there is a
C2-circuit C1 of size at most (sk Ⳮ 1)k, depth at most k • (d Ⳮ
1), and Lipschitz bound 2(sⳭ1) , such that
㛳C1 ⳮ C2㛳 [ⳮ1,1] n ⱕ 2ⳮs
(b) We say that C1 and C2 are equivalent if and only if C1 ⱕ C2
and C2 ⱕ C1.
In other words, when simulating classes of gate functions, we
allow depth to increase by a constant factor size and the logarithm
of the Lipschitz bound to increase polynomially. The relatively
large Lipschitz bounds should not come as a surprise, since the
negative exponential error 2ⳮs requires correspondingly large
weights in the simulating circuit.
Splines (i.e., piecewise polynomial functions) have turned out to
be powerful approximators, and they are our benchmark class of
activation functions; in particular, we assume that a spline net of
size s has as its activation functions splines of degree at most s
with at most one knot. Which properties does a class C of activation
functions need to reach the approximation power of splines? The
activation functions should be able to approximate polynomials as
well as the binary threshold Ᏼ with few layers and relatively few
nodes.
Tightly approximating polynomials is not difficult as long as
there is at least one “sufficiently smooth” nontrivial function c 僆
C. The crucial problem is to obtain a tight approximation of Ᏼ. It
turns out that c-nets achieve tight approximations of Ᏼ whenever
|c(x) ⳮ c(x Ⳮ e)|⳱ O(e/x 2)
for x ⱖ 1,

eⱖ0



and

冷冮

1

冷

c(u2)du ⬆ 0

Let us call a function with these two properties strongly sigmoidal.
(We are actually demanding too much, since it suffices to tightly
approximate a strongly sigmoidal function c by small C-nets with
few layers.) Let us call a class C powerful if there is at least one
“sufficiently smooth” nontrivial function in C and if a strongly
sigmoidal function can be approximated as demanded above.
Examples of powerful singleton classes include, for instance,
1/x as a prime example, and more generally any rational function
that is not a polynomial, exp(x) (since exp(ⳮx) is strongly sigmoidal) and ln(x) (since ln(x Ⳮ 1) ⳮ ln(x) is strongly sigmoidal),
any power x ␣ provided ␣ is not a natural number, and the standard
sigmoid as well as the Gaussian exp(ⳮx 2).
Theorem 4.
(a) Assume that C is powerful. Then splines ⱕ C.
(b) The following classes of activation function have equivalent
approximation power: splines (of degree s for nets of size s),
any rational function that is not a polynomial, any power x ␣
(provided ␣ is not a natural number), the logarithm (for any
base), exp(x), and the Gaussian exp(ⳮx 2).
Notably missing from the list of equivalent activation functions
are polynomials, trigonometric polynomials, and the binary threshold function Ᏼ (or, more generally, low-degree splines). Low-

99

degree splines turn out to be properly weaker. The same applies to
polynomials, even if we allow any polynomial of degree s an activation function for nets of size s. Finally, sine nets cannot be
simulated (as defined in Definition 2), for instance by nets of standard sigmoids, and we conjecture that the reverse is also true,
namely, that nets of standard sigmoids cannot be simulated efficiently by sine nets.
What happens if we relax the required approximation quality
from 2ⳮs to sⳮd, when simulating nets of depth d and size s? Linear
splines and the standard sigmoid are still not equivalent, but the
situation changes completely if we count the number of inputs
when determining size and if we restrict the Lipschitz bound of the
target function to be at most sⳮd. With this modification an even
larger class of important functions, including linear splines, polynomials, and the sine function, turn out to be equivalent with the
standard sigmoid.
The situation for Boolean input and output is somewhat comparable. Maass, Schnitger, and Sontag, and subsequently DasGupta
and Schnitger constructed Boolean functions that are computed by
sigmoidal nets of constant size (i.e., independent of the number of
input bits), whereas Ᏼ-nets of constant size do not suffice. (See
Maass, 1994, for a more detailed discussion.) However, Maass
(1993) showed that spline nets of constant degree, constant depth,
and polynomial size (in the number of input bits) can be simulated
by Ᏼ-nets of constant depth and polynomial size. This simulation
holds without any restriction on the weights used by the spline net.
Thus, analog computation does help for discrete problems, but
apparently by not too much. For a thorough discussion of discrete
neural computation, see Siu, Roychowdhury, and Kailath (1994).

Sigmoidal Nets and Nets of Spiking Neurons
A formal model of networks of spiking neurons is defined in Maass
(1997); see SPIKING NEURONS, COMPUTATION WITH. The architecture is described by a directed graph G ⳱ (V, E), with V as the set
of nodes and E as the set of edges. We interpret nodes as neurons
and edges as synapses, and assign to each neuron v a threshold
function Hv : ⺢Ⳮ r ⺢Ⳮ. (⺢Ⳮ denotes the set of nonnegative reals.)
The value of Hv(t ⳮ t⬘) measures the “reluctance” (or the threshold
to be exceeded) of neuron v to fire at time t (t  t⬘), assuming that
v has fired at time t⬘. This reluctance can be overcome only if the
potential Pv(t) of neuron v at time t is at least correspondingly as
large.
The potential of v at time t depends on the recent firing history
of the presynaptic neurons (or the immediate predecessors) u of v.
In particular, if the synapse between neurons u and v has the efficacy (or weight) wuv , if euv(t ⳮ s) is the response to the firing of
neuron u at time s (s  t) and if the presynaptic neuron u has fired
previously for the times in the set Firetu, then the potential at time
t is defined as
Pv (t) ⳱

兺 兺

(u,v)僆E s僆Firetu

wuv • euv (t ⳮ s)

(3)

Two models, namely deterministic (respectively noisy) nets of
spiking neurons, are distinguished. The deterministic version assumes that neuron v fires whenever its potentials Pv (t) reach Hv (t
ⳮ t⬘) (with most recent firing t⬘), whereas the more realistic noisy
version assumes that the firing probability increases with increasing
difference Pv (t) ⳮ Hv (t ⳮ t).
Thus we can complete the definition of the formal model, assuming that a response function euv : ⺢Ⳮ r ⺢ as well as the weight
wuv is assigned to the synapse between u and v. The model computes by transforming a spike train of inputs into a spike train of
outputs. For instance assuming temporal coding with constants T
and c, the output of a designated neuron with firing times T Ⳮ c •
t1, . . . , T Ⳮ c • ki⳱1 ti, . . . is defined as t1, . . . , ki⳱1 ti, . . . .

100

Part III: Articles

The power of spiking neurons shows for the example of the
element distinctness function EDn with real inputs x1, . . . , xn, where

冦

1
if xi ⳱ xj for some i ⬆ j,
EDn(x) ⳱ 0
if | xi ⳮ xj | ⱖ 1 for all i ⬆ j ,
arbitrary otherwise.
We assume that the inputs x1, . . . , xn are represented by n input
trains of single spikes. Now it is easy to choose a simple threshold
function as well as simple (and indeed identical) response functions
such that even a single spiking neuron with unit weights is capable
of computing EDn. On the other hand, any sigmoidal net computing
EDn requires at least (n ⳮ 4)/2 ⳮ 1 hidden units (Maass, 1997).
This result is also the strongest lower size bound for sigmoidal nets
computing a specific function; the argument builds on techniques
from Sontag (1997).
Certainly this one-neuron computation requires time, because of
the temporal input coding, but the same applies to sigmoidal networks, since, from the point of neurobiology, the xi’s will be obtained after sampling the firing rate of their input neurons.
Nets of spiking neurons are capable of simulating Ᏼ-nets with
at most the same size, and hence are properly stronger than Ᏼ-nets
and at least in some cases stronger than sigmoidal nets. Thus, careful timing is an advantage that synchronized models cannot
overcome.

lowed, then nets have less power and recognize exactly the languages computable by polynomial-size Boolean circuits.

Discussion
We have discussed the computing power of neural nets, including
universal approximation results for feedforward and recurrent neural networks as well as efficient approximation by feedforward nets
with various activation functions. We emphasize that this survey
is far from complete. For instance, we omitted important topics
such as the VAPNIK-CHERVONENKIS DIMENSION OF NEURAL NETWORKS (q.v.) and the complexity of discrete neural computation.
Important open questions relate to proving better upper and
lower bounds for sigmoidal nets computing (or approximating)
specific functions, and achieving a better understanding of size and
depth trade-offs for important function classes. Other neural models, such as networks of spiking neurons, significantly change the
computing power, and the questions we have identified apply to
these models as well.
Road Map: Computability and Complexity
Background: I.3. Dynamics and Learning in Neural Networks
Related Reading: Neural Automata and Analog Computational Complexity; PAC Learning and Neural Networks; Universal Approximators

References
Computational Power of Recurrent Nets
The computational power of recurrent nets is investigated in Siegelmann and Sontag (1994, 1995). (See also Siegelmann, 1998, for
a thorough discussion of recurrent nets and analog computation in
general.) Recurrent nets include feedforward nets, and thus the results for feedforward nets apply to recurrent nets as well. But recurrent nets gain considerably more computational power with increasing computation time. In the following discussion, for the sake
of concreteness, we assume that the piecewise linear function

冦

0 if x ⱕ 0
p(x) ⳱ x if 0 ⱕ x ⱕ 1
1 if x ⱖ 1
is chosen as the activation function. We concentrate on binary input
and assume that the input is provided one bit at a time.
First of all, if weights and thresholds are integers, then each node
computes a bit. Recurrent nets with integer weights thus turn out
to be equivalent to finite automata, and they recognize exactly the
class of regular language over the binary alphabet {0, 1}.
The computational power increases considerably for rational
weights and thresholds. For instance, a “rational” recurrent net is,
up to a polynomial time computation, equivalent to a Turing machine. In particular, a network that simulates a universal Turing
machine does exist, and one could refer to such a network as “universal” in the Turing sense. It is important to note that the number
of nodes in the simulating recurrent net is fixed (i.e., does not grow
with increasing input length).
Irrational weights provide a further boost in computation power.
If the net is allowed exponential computation time, then arbitrary
Boolean functions (including noncomputable functions) are recognizable. However, if only polynomial computation time is al-

Arora, S., Babai, L., Stern, J., and Sweedyk, Z., 1997, The hardness of
approximate optima in lattices, codes and systems of linear equations,
J. Comput. Syst. Sci., 54:317–331.
Barron, A. R., 1993, Universal approximation bounds for superpositions of
a sigmoidal function, IEEE Trans. Inform. Theory, 39:930–945.
Cybenko, G., 1989, Approximation by superposition of a sigmoidal function, Math. Control Signals Syst., 2:303–314.
DasGupta B., and Schnitger, G., 1993, The Power of Approximating: A
Comparison of Activation Functions, NIPS 5, 615–622. Available:
http://www.cs.uic.edu/dasgupta/resume/publ/papers/approx.ps.Z
Kharitonov, M., 1993, Cryptographic hardness of distribution specific
learning, in Proceedings of the 25th ACM Symposium on the Theory of
Computing, pp. 372–381.
Kolmogorov, A. N., 1957, On the representation of continuous functions
of several variables by superposition of continuous functions of one
variable and addition, Dokl. Akad. Nauk USSR, 114:953–956.
Maass, W., 1993, Bounds for the computational power and learning complexity of analog neural nets, in Proceedings of the 25th Annual ACM
Symposium on the Theory of Computing, pp. 335–344.
Maass, W., 1994, Sigmoids and Boolean threshold circuits, in Theoretical
Advances in Neural Computation and Learning (V. P. Roychowdhury,
K. Y. Siu, and A. Orlitsky, Eds.), Boston: Kluwer, pp. 127–151.
Maass, W., 1997, Networks of spiking neurons: The third generation of
neural network models, Neural Netw., 10:1659–1671.
Poggio, T., and Girosi, F., 1989, A theory of networks for approximation
and learning, Artif. Intell. Memorandum, No. 1140.
Siegelmann, H. T., 1998, Neural Networks and Analog Computation: Beyond the Turing Limit, Boston: Birkhäuser. ⽧
Siegelmann, H. T., and Sontag, E. D., 1994, Analog computation, neural
networks, and circuits, Theoret. Comput. Sci., 131:331–360.
Siegelmann, H. T., and Sontag, E. D., 1995, On the computational power
of neural nets, J. Comput., 50:132–150.
Siu, K.-Y., Roychowdhury, V. P., and Kailath, T., 1994, Discrete Neural
Computation: A Theoretical Foundation, Englewood Cliffs, NJ: Prentice
Hall. ⽧
Sontag, E. D., 1997, Shattering all sets of k points in general position requires (k ⳮ 1)/2 parameters, Neural Computat., 9:337–348.

Analog VLSI Implementations of Neural Networks

101

Analog VLSI Implementations of Neural Networks
Paul Hasler and Jeff Dugger
Introduction
The primary goal of analog implementations of neural networks is
to incorporate some level of realistic biological modeling of adaptive systems into engineering systems built in silicon. We cannot
simply duplicate biological models in silicon media because the
constraints imposed by the biological media and the silicon media
are not identical. Approaches that have been successful begin with
the constraints that the silicon medium imposes on the learning
system. Therefore, letting the silicon medium constrain the design
of a system results in more efficient methods of computation.
We will focus our attention on issues concerning building neural
network integrated circuits (ICs), and in particular on building connectionist neural network models. Connectionist neural networks,
loosely based on biological computation and learning, can be useful
for biological modeling if the limitations are understood. These
neural systems are typically built as mappings of mathematical
models into analog silicon hardware either by using standard building blocks (i.e., Gilbert multipliers: Mead, 1989) or by taking advantage of device physics (Hasler et al., 1995). This approach, related to investigations of adaptation and learning in neurobiological
systems, provides the minimum necessary model of synaptic interaction between neurons, even for biological models. Neuromorphic (Mead, 1989, see also NEUROMORPHIC VLSI CIRCUITS
AND SYSTEMS) and connectionist approaches develop adaptive systems from different levels of neural inspiration, and therefore lead
to different levels of model complexity. Adding dendritic interactions and precise models of biological learning (e.g., LTP) to the
connectionist model yields more biological realism. Implementations of fuzzy systems typically follow a similar approach to implementations of neural networks. The related field of cellular neural networks (CNNs), started by Chua, is particularly concerned
with the circuit techniques used to build locally connected twodimensional (2D) meshes of neuron processors (Chua, 1998), but
the architecture design is fundamentally different and imposes different constraints on implementation.

Figure 1. Classic picture of a two-layer neural
network from the perspective of implementating
these networks in hardware. The neural networks
are layers of simple processors, called neurons,
that are interconnected through weighting elements, called synapses. The neurons aggregate
the incoming inputs (including a threshold or offset) and are applied through a tanh(•) nonlinearity. The synapse elements, which in general are
far more numerous than the neuron elements,
must multiply the incoming signal by an internally stored value, called the weight, and must
adapt this weight according to a particular learning rule. Learning rules implemented in silicon
are typically functions of correlations of signals
passing through each synapse processor.

Neural Network Basics Focused
on Implementation Issues
Figure 1 shows the basic feedforward structure typically used in
neural network implementations. Most approaches focus on feedforward structures, since feedback systems and networks with time
dynamics (e.g., time delays) are straightforward extensions for silicon implementation, although the algorithm design is considerably
more difficult. In this model, we encode a neuron’s activity as an
analog quantity based on the mean spiking rate in a given time
window. One can build linear or nonlinear filters at the input to the
sigmoid function. Typically, a low-pass filter is built or modeled,
since that will naturally occur for a given implementation or will
set a desired convergence to an attractor (i.e., recurrent networks).
This model is excellent for describing neurobiology if only meanfiring-rate behavior with minimal dendritic interactions is considered.
A basic model synapse (either digital or analog) must be able to
store a weight, multiply its input with the stored weight, and adapt
that weight based on a function of the input and a fed-back error
signal. We model feedforward computation mathematically as
yi ⳱ wijxj r y ⳱ Wx

(1)

where xj is the jth input (x is a vector of inputs), yi is the ith output
(y is a vector of outputs), and wij is the stored weight at position
(i,j) (W is a matrix of weights). The result of this output is passed
through a nonlinear function
zi ⳱ tanh(a( yi ⳮ hi))

(2)

where we designate zi as the result of the computation, a is a gain
factor, and hj is a variable threshold value. Other nonlinear functions, like radial basis functions (see RADIAL BASIS FUNCTION
NETWORKS), are also often used, which would typically modify the

102

Part III: Articles

Wx computation. We model the weight adaptation mathematically
as
dW
s
⳱ f (W, xeT)
(3)
dt
where e is a vector of error signals that is fed back along various
rows. We call this an outer-product learning rule, or a local learning
rule, because of the xeT computation. The outer-product learning
rule is dependent on the choice of f (W, xeT) and the choice of the
error signal.
Several learning algorithms have been proposed that conform to
this functional form; representative examples can be found elsewhere in the Handbook. Learning algorithms usually divide into
two categories, supervised and unsupervised. Supervised algorithms adapt the weights based on the input signals and a supervisory signal to train the network to produce an appropriate response. In many supervised algorithms (see PERCEPTRONS,
ADALINES, AND BACKPROPAGATION) this weight change is a time
average of the product of the input and some fed-back error signal
(e ⳱ y ⳮ ŷ, where ŷ is the target signal). Unsupervised algorithms
adapt the weights based only on the input and output signals, and
in general the weights are a function of the input statistics. Although these learning algorithms result in very different results,
both weight-update rules are similar from an implementation viewpoint. Most unsupervised algorithms are based on Hebbian learning
algorithms, which correspond to neurobiological evidence of learning (see HEBBIAN SYNAPTIC PLASTICITY). For a Hebbian synapse,
the weight change is a time average of the product of the input and
output activity (e ⳱ y).

Neural Network Implementations: Architecture Issues
Before considering circuit implementations of neurons and synapses, we first frame the overall architecture issues involved in
implementing neural networks. In most implementations, a single
layer of synapses is built as mesh architectures connected to a column of neuron processors (Figure 2). Because silicon ICs are 2D,
mesh architectures work optimally with 2D routing constraints.

Feedforward Computation
Figure 2A shows the typical mesh implementation of feedforward
computation for a single-layer architecture. A mesh of processors
is an optimal communication architecture for interconnect limited
systems, which is the case for small synapse elements. Currents are
preferred for outputs, because the summation typically required for
most connectionist models is easily performed on a single wire,
and voltages are preferred for inputs because they are easy to broadcast. Local processing is defined as interaction between physically
close elements, voltage broadcast along global lines (inputs), or
current/charge summation along a wire (outputs). As a result, each
synapse has only to compute the local computation: Wijxj. Because
the synapses store a weight value, the picture in Figure 2A resembles an analog memory that allows a full matrix-vector multiplication in the equivalent of one memory column access. This approach, called analog computing arrays, is defined and its
implication for signal processing is described elsewhere (Kucic et
al., 2001). Figure 2B shows how to modify a mesh architecture
when considering m-nearest-neighbor connections. Other sparse

Figure 2. Typical architectures for neural network implementations. Although the routing looks complicated in Figure 1, it
can easily be implemented in a mesh architecture. A, Diagram
of the classic mesh architecture, typically used for fully connected architectures. B, Diagram of a mesh processor architecture optimized for nearest-neighbor computations.

Analog VLSI Implementations of Neural Networks
encodings require digital communication and processing to handle
the addressing schemes (i.e., address translation tables) and additional complexity (i.e., multiplexing scheme to access the inputs of
each synapse).
To implement a neuron, we need a function that can compute a
tanh(•) function. Fortunately, this function occurs in many IC circuits using either BJT or MOSFET (subthreshold or abovethreshold) devices, such as the differential transistor pair (Mead,
1989). Since we only need a column of neuron circuits, they do
not have the same area constraints that are imposed on synapse
elements. Dynamics (e.g., low-pass filtering) are usually achieved
by adding additional capacitance. Often one needs a current to perform voltage conversion between the summed synapse outputs and
tanh(•) output, as well as at the output of a differential transistor
pair. This conversion often can be nonlinear, or it may have to be
nonlinear to interface with later processing stages.

Adaptive Neural Network Architectures
Synapses require both feedforward and adaptation computations;
therefore, architectural constraints imposed by the learning algorithm are an essential consideration for any neural network. Only
learning algorithms that scale to large numbers of inputs and outputs are practical. A single-layer architecture with a local supervised or unsupervised rule of the form of Equation 3 only requires
communicating the error signal along each row (Figure 3). The
complexity of the synapse computation will depend on the particular learning rule. Many complicated algorithms, such as the generalized Hebbian algorithm (GHA) (Hasler and Akers, 1992) and
INDEPENDENT COMPONENT ANALYSIS (ICA) (q.v.), require additional matrix-vector multiplications, but can be developed into a
mesh architecture. Algorithms requiring matrix-matrix multiplications are difficult in standard IC technologies.
For multilayer algorithms, the architecture gets more complicated, particularly for supervised algorithms such as multilayer
backpropagation. To extend the basic silicon synapse to a backpropagating synapse, we need an additional function: we need an
output current that is the product of the fed-back error signal (drain
voltage) and stored weight. We show this architecture in Figure
4A. This additional function results in two issues, one concerning
the signal-to-noise ratio of the resulting error signal and the other
concerning the overall synapse size. The effect of these small error
signals, even without the resolution issues, is a slow learning rate.
The neural network literature is replete with possible alternative
approaches, but we will base our proposed research on the Helm-

Figure 3. Learning in a single layer. We
can build either supervised algorithms
(LMS is explicitly shown) or unsupervised one-layer networks in this architecture. For a one-layer supervised case, ẑ is
the desired or target output signal vector,
where ej ⳱ zj ⳮ ẑj. Further, one might
apply a nonlinear function to the resulting
error signal; in LMS, one applies a nonlinear function to counteract the effect of
the sigmoid in the feedforward path.
Many unsupervised rules, like Hebbian or
Oja rules, can be formulated as ẑ ⳱ f(z).
One can schematically represent this network from its terminals, x, z, and ẑ, as
shown from its block diagram. Finally,
the nonlinear (sigmoid) elements typically convert current to voltage.

103

holtz machine concept (see HELMHOLTZ MACHINES AND SLEEPWAKE LEARNING). Our primary reason for using this approach rests
on our desire to use single-layer networks as primitives for building
larger networks, as well as the fact that this reciprocal adaptive
single-layer network architecture is seen in various models of sensory neurosystems, such as the pathways from retina to LGN to
V1 or some of the pathways between the cochlea and auditory
cortex (A1). Figure 4B considers a two-layer network implementation of a backpropagation-like learning rule using this Helmholtz
block. In this case, we double the number of layers, and therefore
double the effective synapse size; for a backpropagation algorithm,
we require the same number of floating-gate multipliers, but with
significant additional implementation costs that greatly increase the
synapse complexity. This approach seems more IC-friendly for the
development of adaptive multilayer algorithms than backpropagation approaches, although its digital implementation is nominally
equivalent to backpropagation approaches. This approach directly
expands to multiple layers and could be used in limited reconfigurable networks because we are building networks with single
adaptive layers. Starting with the single-layer network as the basic
building block simplifies the abstraction toward system development.

Resulting Synapse Design Criteria
Because the synapse is the critical element of any neural network
implementation, we state five properties of a silicon synapse that
are essential for building large-scale adaptive analog VLSI synaptic
arrays (Hasler et al., 1995):
1. The synapse must store a weight permanently in the absence of
learning.
2. The synapse must compute an output current as the product of
its input signal and its synaptic weight.
3. The synapse must modify its weight at least using outer-product
learning rules.
4. The synapse must consume minimal silicon area, thereby maximizing the number of synapses in a given area.
5. The synapse must dissipate a minimal amount of power; therefore, the synaptic array is not power constrained.
Achieving all five requirements requires a detailed discussion of
the circuits used to implement a synapse, which is the subject of
the next section.

104

Part III: Articles

Figure 4. Possible architectures for adaptive multilayer neural networks.
A, Implementation for backpropagation networks. There are many forms
and modifications, but from an implementation viewpoint, these approaches
can be modified toward this architecture. This approach significantly increases synapse size, because one typically requires the complexity of two
synapses for weight feedback. Further, this approach limits some circuit
approaches to building dense synapses. The output from the hidden layer,
or layer 1, is yh and the error signal given to the hidden layer is eh. The
synapses in the second layer must also output a current proportional to the
product of error * stored weight; the sum of these currents along a column
is the error for the next layer. As a result, the synapses on the second layer
must be more complex. B, Implementation using Helmholtz machine concepts. This approach requires twice as many synapses for all but the first
layer, which yields the same complexity as the backpropagation approaches. This approach will converge to the same steady states and requires only a modular tiling of single-layer networks; its reciprocal feedback has a similar feel to communication between layers of cortical
neurons.

Neural Network Implementation: Synapse Circuits
Early Research in Synapse Design
Several neural networks have been built in analog silicon hardware.
Several good recent implementation techniques can be found in

Cauwenberghs and Bayoumi (1999); here we present an overview.
From the architecture discussions, we require a synapse block
where an input voltage should modulate an output current, which
is summed along a line; therefore, most implementations employ
a variable resistance or transconductance element. As a result, a
primary issue in synapse circuit designs is developing dense multiplier circuits, because multiplication of an input by a weight is
fundamental to every synapse. Earlier approaches for implementating the feedforward synapse computation included fixed resistances (which were among the earliest implementations), switchedcapacitor implementations (Tsividis and Satyanarayana, 1987),
Gilbert multiplier cells (Mead, 1989), and linearized conductance
elements (Dupuie and Ismail, 1990; Cauwenberghs, Neugebaur,
and Yariv, 1991; Hasler and Akers, 1992). Intel’s ETANN chip
was the first commercially available neural network IC that used
floating gates for weight storage (Holler et al., 1989). One of the
most successful implementations of a large-scale adaptive neural
system was the Heuralt-Juetten algorithm, but it required a great
deal of circuit complexity (Cohen and Andreou, 1992). Other researchers have implemented unsupervised learning and backpropagation algorithms, with mixed success (Furman, White, and Abidi,
1988; Hasler and Akers, 1992). Successful analog implementations
of connectionist networks have included algorithmic modifications
that facilitate implementation in silicon. The history of this field
has shown that the success of an implementation is strongly correlated with the degree to which the algorithm is adapted to the
silicon medium.
Synapses in previous silicon implementations have required
large circuit complexity because they have typically been constructed using traditional circuit building blocks to realize memory,
computation, and adaptation functions separately, rather than taking advantage of device physics to combine these functions in a
compact circuit element. Not only does large circuit complexity
consume tremendous circuit area and power, but the chance of a
network operating correctly decreases exponentially with cell size.
The most difficult problem to overcome when building efficient
adaptive circuits is the effect of p-n junction leakage currents (Hasler et al., 1995; Hasler and Minch, 2002). First, since many implementations dynamically store their weight parameters on a capacitor, these junction leakage currents typically limit the hold time,
on the order of seconds; therefore, weight storage often becomes a
critical concern in many of these applications. Several on-chip refreshing schemes have been proposed and built (Hasler and Akers,
1992) and are currently finding applications in various ICs (Cauwenberghs and Bayoumi, 1999). Second, since real-time learning
often requires time constants from 10 ms to days, junction leakage
currents limit the use of capacitor storage techniques, unless prohibitively large capacitor areas are used. Weight update schemes
based on weight perturbation methods, i.e., where the error signal
is based on random known changes in the weights, can often work
in these constraints if some form of dynamic refreshing scheme is
used (Cauwenberghs and Bayoumi, 1999). Often, junction leakage
is too large for many adaptive system problems.

Single-Transistor Learning Synapses
Current research into analog neural network ICs pursues two directions. The first direction is based on refreshable DRAM elements with adaptation using weight perturbation techniques (Cauwenberghs and Bayoumi, 1999). The second direction is based on
a wide range of techniques using floating-gate synapses. Floating
gates have seen use in neural networks as storage elements (Holler
et al., 1989), which eliminates the long-term weight storage issues
but still results in fairly complex synapse circuits. Here, we briefly
describe the potential of using floating-gate synapses.
The single-transistor learning synapse (STLS), or transistor synapse, makes use of device physics and constraints inherent to the

Analog VLSI Implementations of Neural Networks

105

Figure 5. Layout, cross-section, and circuit diagram of
the floating-gate pFET in a standard double-poly n-well
MOSIS process. The cross-section corresponds to the
horizonatal line slicing through the layout view. The
pFET transistor is the standard pFET transistor in the nwell process. The gate input capacitively couples to the
floating gate by either a poly-poly capacitor, a diffused
linear capacitor, or a MOS capacitor, as seen in the circuit diagram (not explicitly shown in the other figures).
We add floating-gate charge by electron tunneling, and
we remove floating-gate charge by hot-electron injection. The tunneling junctions used by the single-transistor synapses is a region of gate oxide between the polysilicon floating gate and n-well (a MOS capacitor).
Between Vtun and the floating gate is our symbol for a
tunneling junction, a capacitor with an added arrow designating the charge flow.

silicon medium to realize learning and adaptation functions, rather
than direct implementation of learning rules using traditional circuit
building blocks (Hasler et al., 1995). This technology is rooted in
floating-gate circuits (Hasler and Lande, 2001; Hasler and Minch,
2002) in which multiple features of a floating-gate transistor are
used, not just the nonvolatile storage (Figure 5). These elements
utilize physical characteristics of the silicon medium, such as electron tunneling and hot-electron injection, which traditionally have
posed problems for engineers. The starting point for this technology is a floating-gate transistor (Hasler et al., 1995; Kucic et al.,
2001) operating with subthreshold currents and configured to simultaneously store permanently the weight charge, compute an
output current that is the product of the input signal and the synaptic
weight, and modify its weight charge based on many outer-product
learning rules. This approach meets all five requirements for a silicon synapse. These weights can be automatically programmed,
which enables setting fixed weights, setting initial bias conditions,
and employing weight perturbation learning rules (Kucic et al.,
2001). Further, by setting the appropriate boundary circuits for the
synapse array, we can get a wide range of learning rules by continuously enabling our programming mechanisms during computation (Kucic et al., 2001). One form of the learning rules looks
like
s

dwi,j
⳱ gE[xiej] ⳮ wi,j
dt

(4)

where s is the adaptation time constant and g is the strength of the
correlating term.
Road Map: Implementation and Analysis
Related Reading: Digital VLSI for Neural Networks; Photonic Implementations of Neurobiologically Inspired Networks; Silicon Neurons

References
Chua, L. O., 1998, A Paradigm for Complexity, vol. 31, in World Scientific
Series on Nonlinear Science, Series A, Singapore: World Scientific
Publishing.

Cauwenberghs, G., and Bayoumi, M. A., Eds., 1999, Learning in Silicon,
Boston: Kluwer Academic.
Cauwenberghs, G., Neugebaur, C., and Yariv, A., 1991, An adaptive
CMOS matrix vector multiplier for large scale analog hardware neural
network applications, in Proceedings of the International Joint Conference on Neural Networks, vol. 1, pp. 507–512.
Cohen, M., and Andreou, A. G., 1992, Current-mode subthreshold MOS
implementation of the Herault-Jutten autoadaptive network, IEEE
Trans. Solid State Circuits, 27:714–727.
Dupuie, S. T., and Ismail, M., 1990, High frequency CMOS transconductors, in Analogue IC Design: The Current-Mode Approach (C. Toumazou, F. J. Lidgey, and D. G. Haigh, Eds.), London: Peter Peregrinus,
pp. 181–238.
Furman, B., White, J., and Abidi, A. A., 1988, CMOS analog IC implementing the backpropagation algorithm, in Abstracts of the First Annual
INNS Meeting, vol. 1, p. 381.
Hasler, P., and Akers, L., 1992, Circuit implementation of a trainable
neural network using the generalized Hebbian algorithm with supervised techniques, in Proceedings of the International Joint Conference
on Neural Networks, vol. 1, Baltimore, pp. 1565–1568. ⽧
Hasler, P., Diorio, C., Minch, B. A., and Mead, C., 1995, Single transistor
learning synapses, in Advances in Neural Information Processing Systems 7, Cambridge, MA: MIT Press, pp. 817–824.
Hasler, P., and Lande, T. S., Eds., 2001, Floating-Gate Circuits (special
issue), IEEE Trans. Circuits Syst II, 48(1).
Hasler, P., and Minch, B. A., 2002, Floating-Gate Devices, Circuits, and
Systems, New York: IEEE Press.
Holler, M., Tam., S., Castro, H., and Benson, R., 1989, An electrically
trainable artificial neural network with 1024 “floating gate” synapses,
in Proceedings of the International Joint Conference on Neural Networks, vol. 2, Washington, D.C., pp. 191–196. ⽧
Kucic, M., Low, A.-C., Hasler, P., and Neff, J., 2001, A programmable
continuous-time floating-gate Fourier process, IEEE Trans. Circuits and
Systems II, 48:90–99.
Mead, C., 1989, Analog VSLI and Neural Systems, Reading, MA: Addison-Wesley. ⽧
Tsividis, Y., and Satyanarayana, S., 1987, Analogue circuits for variable
synapse electronic neural networks, Electron. Lett., 24(2):1313–1314.

106

Part III: Articles

Analogy-Based Reasoning and Metaphor
Dedre Gentner and Arthur B. Markman
Introduction
Analogy and metaphor have been characterized as comparison processes that permit one domain to be seen in terms of another. They
are important to connectionism for two reasons. First, there is an
affinity at the descriptive level: many of the advantages suggested
for connectionist models—representation completion, similaritybased generalization, graceful degradation, and learning—also apply to analogy (Barnden, 1994). Second, analogical processing
poses significant challenges for connectionist models. Analogy involves the comparison of systems of relations between items in a
domain. To model analogy requires representations that include
internal relations. Many connectionist models have concentrated
instead on statistical learning of correlational patterns over featural
or dimensional representations.

Tenets of Analogy and Metaphor
Analogy derives from the perception of relational commonalities
between domains that are dissimilar on the surface. These correspondences often suggest new inferences about the target domain.
Analogy has been widely studied in humans. In the past decade,
psychological research on analogy has converged on a set of benchmark phenomena against which models of analogy can be measured. These eight benchmarks, shown in Table 1, can be organized
according to four processing principles. Analogy and metaphor involve (1) structured pattern matching; (2) structured pattern completion, (3) a focus on common relational structure rather than on
common object descriptions, and (4) flexibility in that (a) the same
domain may yield different interpretations in different comparisons, and (b) a single comparison may yield multiple distinct interpretations. Any model of analogy must account for these
phenomena.
We begin by reviewing the principles and benchmarks, and then
discuss current connectionist models of analogy and metaphor. Our
discussion takes place at Marr’s computational and algorithmic

Table 1. Eight Benchmark Phenomena of Analogy
1. Relational Similarity
2. Structured Pattern
Matching
3. Systematicity
4. Candidate Inferences
5. Alignable Differences
6. Flexibility (1): Interactive
Interpretation
7. Flexibility (2): Multiple
Interpretation
8. Cross-mapping

Analogies involve relational
commonalities; object commonalities
are optional.
Analogical mapping involves one-to-one
correspondence and parallel
connectivity.
In interpreting analogy, connected
systems of relations are preferred over
sets of isolated relations.
Analogical inferences are generated via
structural completion.
Differences that are connected to the
commonalities of a pair are rendered
more salient by a comparison.
Analogy interpretation depends on both
terms. The same term yields different
interpretations in different
comparisons.
Analogy allows multiple interpretations
of a single comparison.
People typically perceive both
interpretations of a cross-mapping and
prefer the relational interpretation.

levels, at which cognition is explained in terms of representations
and associated processes. We will not evaluate the models in terms
of brain function, partly because the neural basis is not yet understood, but also because we believe a computational model must
first justify itself as a cognitive account. We will focus mainly on
analogy, which has been well studied at the processing level. Much
of what we know about analogy can be applied to metaphor as
well. Later, we will explore ways in which analogy and metaphor
may differ.

Structured Pattern Matching
The defining characteristic of analogy and many metaphors is the
alignment of relational structure. Alignment involves finding structurally consistent matches (those observing parallel connectivity
and one-to-one correspondence). Parallel connectivity requires that
matching relations have matching arguments; one-to-one correspondence limits any element in one representation to at most one
matching element in the other representation (Gentner and Markman, 1997; Holyoak and Thagard, 1995). For example, in the analogy “The atom is like the solar system,” the nucleus in the atom
(the target) corresponds to the sun in the solar system (the base)
and the electrons to the planets, because they play similar roles in
a common relational structure: e.g., revolve (sun, planets) and revolve (nucleus, electron). The sun is not matched to both the nucleus and the electron, as that violates one-to-one correspondence.
Another characteristic of analogy is relational focus: objects correspond by virtue of playing like roles and need not be similar (e.g.,
the nucleus need not be hot).
There is considerable evidence that people can align two situations, preserving connected systems of commonalities and making
the appropriate lower-order substitutions. For example, Clement
and Gentner (1991) showed people analogous stories and asked
them to state which of two assertions shared by base and target
was most important to the match. Subjects chose the assertion connected to matching causal antecedents. More generally, people’s
correspondences are based both on the goodness of the local match
and on its connection to a larger matching system (Markman and
Gentner, 1993). This finding demonstrates the systematicity principle: Analogies seek connected systems of matching relations
rather than isolated relational matches.
When making comparisons, it often occurs that nonidentical
items are matched by virtue of playing a common role in the matching system. These corresponding but nonidentical elements give
rise to alignable differences, and have been shown to be salient
outputs of the comparison process (Gentner and Markman, 1997).
In contrast, aspects of one situation that have no correspondence
in the other, called nonalignable differences, are not salient outputs
of comparison. For example, when comparing the atom to the solar
system, the fact that atoms have electrons and solar systems have
planets is an alignable difference. The fact that solar systems have
asteroids, while atoms have nothing that corresponds to asteroids,
is a nonalignable difference.

Structured Pattern Completion
Analogical reasoning also involves the mapping of inferences from
one domain to another. Thus, a partial representation of the target
is completed based on its structural similarity to the base. For example, Clement and Gentner (1991) extended the findings described earlier by deleting some key matching facts from the target

Analogy-Based Reasoning and Metaphor

107

A more striking kind of flexibility is that a single base-target comparison can give rise to multiple distinct interpretations. For a comparison like “Cameras are like tape recorders,” people can readily

provide an object-level interpretation (“Both are small mechanical
devices”) or a relational interpretation (“Both record events for later
display”). Interestingly, children tend to prefer the former and
adults the latter.
Despite this flexibility, people generally maintain structural consistency within an interpretation. In one study, Spellman and Holyoak (1992) asked subjects to map the Gulf War onto World War
II (WWII). They asked “If Saddam Hussein corresponds to Hitler,
who does George Bush correspond to?” Some subjects chose
Franklin Delano Roosevelt, whereas others chose Winston Churchill. The key finding was that, when asked to make a further mapping for the United States in 1991, subjects chose structurally consistent correspondences. Those who mapped Bush to Roosevelt
usually mapped the US-1991 to the US-during-WWII, and those
who mapped Bush to Churchill mapped the US-1991 to Britainduring-WWII.
An extreme case of conflicting interpretations is cross-mapping,
in which the object similarities suggest different correspondences
than do the relational similarities. For example, in the comparison
between “Spot bit Fido” and “Fido bit Rover,” Fido is crossmapped. When presented with cross-mapped comparisons, people
can compute both alignments. Research suggests that adding
higher-order relational commonalities increases people’s preference for the relational alignment, whereas increasing the richness
of the local object match increases people’s preference for the object match. For example, people are more likely to select the relational correspondence in Figure 1B than in Figure 1A. This example also illustrates that the analogical processes we describe can
apply to perceptual as well as conceptual materials. The ability to
compute relational interpretations (even for the cross-mappings) is
central to human analogizing across a wide range of domains.
This flexibility and the ability to process cross-mappings have
significant implications for the comparison process, because they

Figure 1. Sets of object triads containing a cross-mapping. A crossmapping occurs when two similar objects play different roles in a matching
relational system. In this case, the similar objects have different relative

sizes. (A) shows a sparse pair of objects that are likely to have few distinguishing attributes, whereas (B) shows a rich pair of objects that are likely
to have many distinguishing attributes.

story and asking subjects to make a new prediction about the target
based on the analogy with the base story. Consistent with the previous result, subjects mapped just those predicates that were causally connected to other matching predicates.

Flexibility: Interactive Interpretation
Analogy and metaphor are flexible in important ways. Indeed,
Barnden (1994) suggests that analogy and metaphor may reconcile
connectionism’s flexibility with symbolic AI’s structure-sensitivity. One way that analogy and metaphor are flexible is that the
interpretations are interactions between the two terms. The same
item can take part in many comparisons, with different aspects of
the representation participating in each comparison.
For example, Spellman and Holyoak (1992) compared politicians’ analogies for the Gulf War. Some likened it to World War
II, implying that the United States was acting to stop a tyrant,
whereas others likened it to Vietnam, implying that the United
States had embroiled itself in a potentially endless conflict between
two other opponents. Comparisons with different bases highlighted
different features of the target. Flexibility is also evident when the
same base term is combined with different targets. For example,
the metaphor “A lake is a mirror” highlights that a lake has a flat
reflective surface, whereas “Meditation is a mirror” highlights the
self-examination aspect of meditation.

Flexibility: Multiple Interpretations
of the Same Comparison

108

Part III: Articles

mean that simulations cannot simply be trained to generate a particular kind of interpretation. Rather, the comparison process must
be able to determine both object matches and structural matches
and to attend selectively to one or the other.

Connectionism and Analogical Mapping
As the preceding review makes clear, a central aspect of analogical
reasoning and metaphor is alignment and mapping between structured representations. Symbolic models—e.g., Falkenhainer, Forbus, and Gentner’s (1989) structure-mapping engine (SME)—have
been able to pass the eight benchmarks in Table 1. Advances in
connectionist models of analogy and metaphor have come with the
development of techniques for representing structure (e.g., Hinton,
1991; STRUCTURED CONNECTIONIST MODELS). The bestdeveloped models to date have been models of analogy rather than
models of metaphor, and so we will focus on the analogy models
here. We will discuss differences between analogy and metaphor
in the following section.
An early connectionist model of analogy was ACME (Holyoak
and Thagard, 1989). This model was a localist constraint-satisfaction network in which the nodes represented possible correspondences between elements in the base and target. Nodes were created
using the constraint of semantic matching via a table that determined which predicates were seen as semantically similar. Nodes
were connected in accordance with structural consistency, with
nodes for consistent matches getting excitatory links and nodes for
inconsistent matches getting inhibitory links. Finally, a pragmatic
constraint was added by activating nodes related to goals and correspondences known in advance. After this activation was set up,
the network was allowed to settle, and the most active nodes (above
some threshold) determined the correspondences between base and
target found by the system. The interpretation found by ACME
need not maintain structural consistency, which can lead to problems in making inferences. Hummel, Burns, and Holyoak (1994)
point out that the implementation of the pragmatic constraint often
causes the important node(s) to map to everything in the other
analog. Finally, because ACME settles on a single interpretation
of an analogy, its solution to cross-mappings merges the object and
relational interpretations.
A model of analogy has also been developed using tensor product representations (Smolensky, 1990). In a tensor product, two
vectors X and Y are bound by taking the outer product of these
vectors, YXT. The outer product normally forms a matrix, but a
vector can be constructed from this matrix by concatenating its
columns. Given X and YXT, the vector Y can be obtained as YXTX
if X is a unit vector. Variable bindings can thus be captured by
using one vector to represent a predicate and the other to represent
its argument.
Tensor products have been used in a distributed connectionist
model—STAR—that performs a:b::c:d analogies (Halford et al.,
1994). STAR represents binary relations (R(a, b)) using tensor
products of rank 3 (which are like the binary tensor products just
described except that three vectors are bound together). In this
model, long-term memory consists of a matrix of tensor products
corresponding to various relations the system knows about. To process an analogy, the model takes the a and b terms and probes longterm memory to find a relation between them. It then takes this
relation and the c term of the analogy and finds a fourth term that
shares that relation with the c term. This model uses a distributed
connectionist representation to perform a one-relation analogical
reasoning task. Thus, STAR performs analogy through retrieval of
known relations. STAR cannot generate multiple distinct interpretations of a comparison. If the system knows many different items
that could be the answer to the analogy, the output vector is a

combination of them all. Finally, this model does not make use of
higher-order relational structure to constrain its matches.
Perhaps the most complete connectionist model of analogy is
Hummel and Holyoak’s (1997) LISA, which operates over structured representations by using temporal synchrony in unit firing to
encode relations. The connections between relations and their arguments are maintained by having individual units, which represent
concepts, fire in phase with units that represent particular relational
bindings (STRUCTURED CONNECTIONIST MODELS). For example,
to represent kiss (John, Mary) nodes for kiss, John and agent fire
in phase. Nodes for kiss, Mary and patient also fire in phase (but
out of phase with those for John and agent). Furthermore, each
node representing a concept is connected to a distributed representation designed to capture the meaning of that concept. The semantic similarity of any two concepts is just the dot product of the
vectors in the distributed representations of those concepts. Finally,
higher-order relations are represented in LISA by chunking relations that are arguments to higher-order relations into a single node.
Mapping takes place in LISA by selecting one domain (either
the base or the target) as a driver. A role-argument binding is activated in the driver, and activation flows from the active nodes to
the distributed semantic representation, and from the semantic
nodes to localist concept nodes in the other domain. LISA has a
limited-capacity working memory of 4–6 nodes. This working
memory holds onto the correspondences from a small number of
previous firings, thus allowing some influence of higher-order relational structure. If the role bindings for a higher-order relation
are fired followed by the role bindings for the relational arguments
of that higher order relation, then the correspondences suggested
by the higher order relation can influence the mapping given to its
argument. Trainable connections between nodes in the base and
target are updated only after a certain number of firing cycles (depending on the size of working memory). LISA has been tested on
a number of analogy problems. It tends to make relational mappings for analogies, and generally finds structurally consistent correspondences. The model selects either the relational mapping or
the object mapping for a cross-mapping. On any given run, LISA
arrives at only one interpretation; however if the order in which
nodes in the driver are activated is varied, the system can find
different interpretations on different runs. Finally, because the
model can use complex representations, it can use different aspects
of the representation of a domain in different comparisons involving that domain.
LISA is the only extant model of analogical mapping to include
an explicit working memory constraint. At present, two major questions remain. First, the order in which statements are activated in
the driver—a crucial determinant of the outcome of a match—is
currently decided by the modeler. Second, the model has not been
tested on large representations of the base and target. Thus, it is
not clear how it will perform on these representations.

How Metaphor Differs from Analogy
The previous section focused on connectionist models of analogy.
There has been little work on connectionist models of metaphor.
To some degree, models of analogy could be extended to metaphor.
In this section, we discuss some differences between analogy and
metaphor that are relevant for developing a connectionist model of
metaphor.
Metaphors are nonliteral assertions of likeness. They may be
phrased as comparisons, in simile form (“A cloud is like a sponge”)
or as class inclusions, in metaphor form (“A cloud is a sponge”).
When a novel metaphor is being processed, the two domains in the
metaphor are compared using the same process that is applied to
analogy. Unlike analogy, however, metaphors need not focus ex-

Analogy-Based Reasoning and Metaphor
clusively on relations. For example, the example above could be
interpreted as a cloud that is fluffy, which would focus on an attribute of sponges that clouds also possess. This metaphor can also
be given a relational interpretation. For example, it might be interpreted to mean that both clouds and sponges soak up water and
give it back later. Typically, adults (but not children) prefer relational interpretations of metaphors to attribute interpretations.
There are three key ways in which metaphors differ from analogies. First, whereas analogies tend to have explanatory-predictive
functions, metaphors may have expressive purposes and may affect
the mood of the piece in which they are embedded. Thus, the primary impact of a metaphor might come in the emotions that it
brings out rather than on the information in the comparison that is
promoted. Second, not all metaphors are necessarily processed as
comparisons. Glucksberg and his colleagues (e.g., Glucksberg and
Keysar, 1990) suggest that metaphors might be processed as class
inclusion statements rather than as comparisons. While there is
debate as to exactly when metaphors are processed as comparisons
or as class inclusion statements, some evidence suggests that novel
metaphors and similes (e.g., “Some cults are termites”) are processed by alignment and mapping, whereas conventional metaphors (e.g., “Some people are sheep”) may be processed by accessing a stored metaphorical word sense. Finally, there are often
systems of related metaphors in a language (Lakoff and Johnson,
1980). For example, English has a system of metaphors in which
anger is described as heated fluid in a container (e.g., “Mary was
boiling mad. The pressure built up in her until she finally exploded
with rage.”). These metaphorical systems might reflect a large-scale
mapping between a base and target domain.
One model of system metaphors has been developed by Narayanan (1999). This model uses a localist connectionist system to
handle extended metaphors such as the anger as heated fluid example above. In this system, the connection between a base and
target domain is assumed to be established by convention, so there
is no mapping mechanism for constructing new correspondences.
Instead, the model focuses on how understanding a physical base
domain can aid the comprehension of an abstract target. The model
has a detailed localist network representation of the base domain
in which actions can be simulated as transitions through the network. After simulating a possible outcome in the physical domain,
the established mapping to the target domain is used by passing
activation from the base to corresponding nodes in a belief network
representing the target. In this way, metaphorical inferences can be
drawn from base to target. These inferences are confined to existing
correspondences between the domains; there is no mechanism for
establishing new correspondences. A variety of constraints on
metaphor interpretation such as the intent of the speaker can be
incorporated into the model by treating them as additional sources
of activation.
It may be possible to extend connectionist models of analogy to
metaphor. Connectionist models may be well suited to capturing
emotional aspects of metaphor. Associations between emotions and
words (and word sounds) are unlikely to be mediated by strictly
symbolic and rule-based processes. Thus, the kinds of soft constraints that are easily implemented in connectionist models might
be particularly well suited to understanding this aspect of metaphor
comprehension.

Discussion
Analogical and metaphor processing rely heavily on structurally
governed correspondences between the two domains. This leads to

109

the eight benchmarks summarized in Table 1 that pose a challenge
for any model of analogy. Connectionist models that address these
phenomena have focused on techniques for representing and processing structured representations. LISA, which uses structured
representations and structure-sensitive processing, accounts for
many of the phenomena in Table 1, although some additional specification and testing of the model is still required.
Some challenges for future research include (1) building analogical models that can preserve structural relations over incrementally
extended analogies such as are used in reasoning, (2) developing
models that can be used as components of a broader cognitive
system such as one that would perform problem solving, and (3)
developing models that can handle novel and conventional
metaphors.
Road Map: Psychology
Related Reading: Associative Networks; Compositionality in Neural Systems; Concept Learning; Systematicity of Generalizations in Connectionist Networks

References
Barnden, J. A., 1994, On the connectionist implementation of analogy and
working memory matching, in Advances in Connectionist and Neural
Computation Theory, Vol 3: Analogy, Metaphor, and Reminding (K. J.
Holyoak and J. A. Barnden, Eds.), Norwood, NJ: Ablex Publishing Company, pp. 327–374. ⽧
Clement, C. A., and Gentner, D., 1991, Systematicity as a selection constraint in analogical mapping, Cognitive Sci., 15:89–132.
Falkenhainer, B., Forbus, K. D., and Gentner, D., 1989, The structuremapping engine: An algorithm and examples, Artificial Intelligence,
41:1–63.
Gentner, D., and Markman, A. B., 1997, Structural alignment in analogy
and similarity, Am. Psychol., 52(1):45–56. ⽧
Glucksberg, S., and Keysar, B., 1990, Understanding metaphorical comparisons: Beyond similarity, Psychol. Rev., 97(1):3–18.
Halford, G. S., Wilson, W. H., Guo, J., Wiles, J., and Stewart, J. E. M.,
1994, Connectionist implications for processing capacity limitations in
analogies, in Advances in Connectionist and Neural Computation Theory, Vol. 2: Analogical Connections (K. J. Holyoak and J. Barnden,
Eds.), Norwood, NJ: Ablex, pp. 363–415.
Hinton, G. E., Ed., 1991, Connectionist Symbol Processing, Cambridge,
MA: MIT Press.
Holyoak, K. J., and Thagard, P., 1989, Analogical mapping by constraint
satisfaction, Cognit. Sci., 13(3):295–355.
Holyoak, K. J., and Thagard, P., 1995, Mental Leaps: Analogy in Creative
Thought, Cambridge, MA: MIT Press. ⽧
Hummel, J. E., Burns, B., and Holyoak, K. J., 1994, Analogical mapping
by dynamic binding: Preliminary investigations, in Advances in Connectionist and Neural Computation Theory: Vol. 2: Analogical Connections (K. J. Holyoak and J. A. Barnden, Eds.), Norwood, NJ: Ablex.
Hummel, J. E., and Holyoak, K. J., 1997, Distributed representations of
structure: A theory of analogical access and mapping, Psychol. Rev.,
104(3):427–466.
Lakoff, G., and Johnson, M., 1980, Metaphors We Live By, Chicago, IL:
The University of Chicago Press.
Markman, A. B., and Gentner, D., 1993, Structural alignment during similarity comparisons, Cognitive Psychology, 25(4):431–467.
Narayanan, S., 1999, Moving right along: A computational model of metaphoric reasoning about events, in The Proceedings of AAAI-99, Orlando, FL: AAAI.
Smolensky, P., 1990, Tensor product variable binding and the representation of symbolic structures in connectionist systems, Artificial Intelligence, 48:159–216.
Spellman, B. A., and Holyoak, K. J., 1992, If Saddam is Hitler then who
is George Bush? Analogical mapping between systems of social roles,
J. Personality Soc. Psychol., 62(6):913–933.

110

Part III: Articles

Arm and Hand Movement Control
Stefan Schaal
Introduction
The control of arm and hand movements in human and nonhuman
primates has fascinated researchers in psychology, neuroscience,
robotics, and numerous related areas. To the uninitiated observer,
movement appears effortless. It is only when trying to duplicate
such skills with artificial systems or when examining the underlying neural substrate that one discovers a surprising complexity
that, so far, has prevented us from understanding the biological
implementation, how to repair neural damage, and how to create
human-like robots with a human level of movement skills.
Research directed toward understanding motor control can be
approached on different levels of abstraction. For example, such
research may entail examining the biochemical mechanisms of neuronal firing, the representational power of single neurons and populations of neurons, neuroanatomical pathways, the biomechanics
of the musculoskeletal system, the computational principles of biological feedback control and learning, or the interaction of perception and action. No matter which level of inquiry is chosen, however, ultimately we need to solve the “reverse engineering”
problem of how the properties of each level correlate with the characteristics of skillful behavior. Motor control of the arm and hand
is an excellent example of the difficulties that arise in the reverse
engineering problem. Behavioral research has discovered a variety
of regularities in this movement domain, but it is hard to determine
on which level they arise. Moreover, most of these regularities were
examined in isolated arm or hand movement studies, whereas coordination of the arm and hand is a coupled process in which hand
and arm movement influence each other. In this article, we discuss
some of the most prominent regularities of arm and hand control
and consider where these regularities come from, with a particular
focus on computational and neural network models. It will become
apparent that an interesting competition exists among explanations
sought on the neural, biomechanical, perceptual, or computational
level. These competing explanations have created a large amount
of controversy in the research community over the years.

Behavioral Phenomena of Arm and Hand Control
Most movement skills can be achieved in an infinite number of
ways. For instance, during reaching for an object, an arbitrary hand
path can be taken between starting point and end point, and the
path can be traversed at arbitrary speed profiles. Moreover, because
of the excess of the number of degrees of freedom in primate movement systems, there is an infinite number of ways for realizing a
chosen hand path through postural configurations (see ROBOT ARM
CONTROL). On the biomechanical level there is even more redundancy, because there are many more muscles than degrees of freedom in the human body, and the redundancy increases on the neuronal level. Thus, it is extremely unlikely that two different
individuals would use similar movement strategies to accomplish
the same movement goal. Surprisingly, however, behavioral research did find a large number of regularities, not just across individuals of a given species but also across different species (see,
e.g., Flash and Sejnowski, 2001). These regularities or invariants
have become central to understanding perceptuomotor control, as
they seem to indicate some fundamental organizational principles
in the central nervous system (CNS).

Bell-Shaped Velocity Profiles and Curvature
in Reaching Movements
About 20 years ago, Morasso (see OPTIMIZATION PRINCIPLES IN
MOTOR CONTROL) discovered that in point-to-point reaching

movements in humans, the hand path in Cartesian (external) space
was approximately straight and the tangential velocity trajectory
along the path could be characterized by a symmetric bell shape,
a result that was duplicated in monkeys. In contrast, velocity profiles in joint space and muscle space were much more complex.
These findings gave rise to the hypothesis that point-to-point reaching movements are planned in external coordinates and not in internal ones. Later, more detailed examinations of reaching movements revealed that, although approximately straight, reaching
movement showed a characteristic amount of curvature as a function of where in the workspace the starting point and end point of
the movement were chosen. Also, the symmetry of the velocity
profile varies systematically as a function of movement speed (e.g.,
Bullock and Grossberg, 1988). These behavioral phenomena gave
rise to a variety of models to explain them.
Initial computational models of reaching focused on accounting
for the bell-shaped velocity profile of hand movement, employing
principles of optimal control based on a kinematic optimization
criterion for movement planning that favors smooth acceleration
profiles of the hand (see OPTIMIZATION PRINCIPLES IN MOTOR
CONTROL). As this theory would produce perfectly straight-line
movements in Cartesian space and perfectly symmetric bell-shaped
velocity profiles, the observed violation of these features in behavioral expression was explained by assuming that these movement
plans were executed imperfectly by an equilibrium point controller
(see EQUILIBRIUM POINT HYPOTHESIS). Thus, the behavioral features of point-to-point movements were attributed to perfect motor
planning and imperfect motor execution.
An alternative viewpoint was suggested by Kawato and coworkers (see OPTIMIZATION PRINCIPLES IN MOTOR CONTROL and
EQUILIBRIUM POINT HYPOTHESIS). Their line of research emphasizes that the CNS takes the dynamical properties of the musculoskeletal system into account and plans trajectories that minimize
“wear and tear” in the actuators, expressed as a minimum torquechange or minimum motor-command-change optimization criterion. According to this overall view, the behavioral features of arm
and hand control are an intentional outcome of an underlying computational principle that employs models of the entire movement
system and its environment.
Recently, Harris and Wolpert (see OPTIMIZATION PRINCIPLES IN
MOTOR CONTROL) suggested that the features of arm and hand
movement could also be due to the noise characteristics of neural
firing, i.e., the decreasing signal-to-noise ratio of motor neurons
with increasing firing frequency. Thus, the neuronal level together
with the behavioral goal of accurate reaching was held responsible
for behavioral characteristics.
Several other suggestions were made to account for features of
arm and hand control. Perceptual distortion could potentially contribute to the curvature features in reaching, and dynamical properties of feedback loops in motor planning could generate asymmetries of bell-shaped velocity profiles (Bullock and Grossberg,
1988). Moreover, imperfection of motor learning (see SENSORIMOTOR LEARNING) and delays in the control system could equally
play into explaining behavior.

Movement Segmentation
For efficient motor learning, it seems mandatory that movement
systems plan on a higher level of abstraction than individual motor
commands, as otherwise the search space for exploration during
learning would become too large to find appropriate actions for a

Arm and Hand Movement Control
new movement task (see ROBOT LEARNING). Movement primitives
(see MOTOR PRIMITIVES), also called units of action, basis behaviors, or gestures (see SPEECH PRODUCTION), could offer such an
abstraction. Pattern generators in invertebrates and vertebrates (see
MOTOR PATTERN GENERATION) and the few different behavioral
modes of oculomotor control (e.g., VOR, OKR, smooth pursuit,
saccades, vergence) can be seen as examples of such movement
primitives. For arm and hand control, however, whether some form
of units of actions exist is a topic of ongoing research (Sternad and
Schaal, 1999). Finding behavioral evidence for movement segmentation could thus provide some insight into the existence of movement primitives.
Since the 1980s, kinematic features of hand trajectories have
been used as one major indicator to investigate movement segmentation. From the number of modes of the tangential velocity
profile of the hand in linear and curvilinear drawing movements,
it was concluded that arm movements may generally be created
based on discrete strokes between start points, via points, and end
points, and that these strokes are piecewise planar in threedimensional movement (for a review, see Sternad and Schaal,
1999). From these and subsequent studies, stroke-based movement
generation and piecewise planarity of the hand movement in Cartesian space became one of the main hypotheses for movement
segmentation (Flash and Sejnowski, 2001).
Recent work (Sternad and Schaal, 1999), however, reinterpreted
these indicators of movement segmentation partially as an artifact,
in particular for rhythmic movement, that, surprisingly, was also
assumed to be segmented into planar stokes. Human and robot
experiments demonstrated that features of apparent movement segmentation could also arise from principles of trajectory formation
that use oscillatory movement primitives in joint space. When such
oscillations are transformed by the nonlinear direct kinematics of
an arm (see ROBOT ARM CONTROL) into hand movement, complex
kinematic features of hand trajectories can arise that are not due to
movement segmentation. Sternad and Schaal (1999) therefore suggested that movement primitives may be better sought in terms of
dynamic systems theory, looking for dynamical regimes like point
and limit cycle attractors and using perturbation experiments to find
principles of segmenting movements into these basic regimes.

The 2/3 Power Law
Another related behavioral feature of primate hand movements trajectories, the 2/3 power law, was discovered by Lacquaniti et al.
(in Flash and Sejnowski, 2001). In rhythmic drawing movements,
the authors noted a power law relationship with proportionality
constant k between the angular velocity a(t) of the hand and the
curvature of the trajectory path c(t):
a(t) ⳱ kc(t)2/3

(1)

There is no physical necessity for movement systems to satisfy this
relation between kinematic (i.e., velocity) and geometric (i.e., curvature) properties of hand movements. Since the power law has
been reproduced in numerous behavioral experiments (Viviani and
Flash, 1995, in Flash and Sejnowski, 2001) and even in population
code activity in motor cortices (Schwartz and Moran, 1999, in
Flash and Sejnowski, 2001), it may reflect an important principle
of movement generation in the CNS.
The origins of the power law, however, remain controversial.
Schaal and Sternad (2001) reported strong violations of the power
law in large-scale drawing patterns and, in accordance with other
studies, interpreted it as an epiphenomenon of smooth movement
generation (Flash and Sejnowski, 2001). Nevertheless, the power
law remains an interesting descriptive feature of regularities of
human motor control and has proved to be useful even in model-

ing the perception of movement (see MOTOR THEORIES
PERCEPTION).

111
OF

The Speed-Accuracy Trade-off
In rapid reaching for a target, the movement time MT of reaching
the target was empirically found to depend on the distance A of the
start point of movement from the target and the target width W—
equivalent to the required accuracy of reaching—in a logarithmic
relationship: MT ⳱ a Ⳮ b log2(2A/W), where a and b are proportionality constants in this so-called Fitts’ law or speed-accuracy
trade-off. Since Fitts’ law is a robust phenomenon of human arm
and hand movement, many computational models used it as a way
to verify their validity. Unfortunately, Fitts’ law has been modeled
in many different ways, including models from dynamic system
theory, noise properties of neuronal firing, and computational constraints in movement planning (for a review, see Mottet and
Bootsma, 2001; Bullock and Grossberg, 1988). Thus, it seems that
the constraints provided by Fitts’ law are too nonspecific to give
clear hints as to the organization of the nervous system. Nevertheless, the empirical phenomenon of Fitts’ law remains a behavioral
landmark.

Resolution of Redundancy
As mentioned earlier, during reaching for a target in external space,
the excess number of degrees of freedom in the human body’s
kinematic structure usually allows an infinite number of postures
for each hand position attained during the reaching trajectory. An
active area of research in motor control is thus concerned with how
redundancy is resolved, whether there is within- or across-subject
consistency of the resolution of redundancy, and whether it is possible to deduce constraints on motor planning and execution from
the resolution of redundancy.
Early studies by Cruse et al. (in Bullock, Grossberg, and
Guenther, 1993) demonstrated that redundancy resolution was well
described by a multiterm optimization criterion that primarily tries
to keep joint angular position as far as possible away from the
extreme positions of each joint and also minimizes some physiological cost. According to this explanation, when a reaching movement is initiated in a rather unnatural posture, the movement slowly
converges to the optimal posture on the way to the goal, rather than
achieving optimality immediately. This strategy resembles the
method of resolved motion rate control in control theory, suggested
as a neural network model of human motor planning by Bullock et
al. (1993). Grea, Desmurget, and Prablanc (2000) observed similar
behavior in reaching and grasping movements. Noting that the final
posture at a grasp target was highly repeatable even if the target
changed its position and orientation during the course of the reaching movement, the authors concluded that the CNS plans the final
joint space position for reaching and grasping, not just the final
hand position. However, the optimization methods proposed by
Bullock et al. (1993) could result in similar behavior, without the
CNS explicitly planning the final joint space posture. An elegant
alternative view to optimization methods is suggested in GEOMETRICAL PRINCIPLES IN MOTOR CONTROL (q.v.), where motor control
and planning based on force fields is emphasized. It is evident more
work will be needed before a final conclusion can be reached on
the issue of redundancy resolution.

Reaching and Grasping
The coordination of reaching and grasping offers at least three important windows onto motor control. First, reaching and grasping
require a resolution of redundancy, as outlined in the previous section. However, small changes in target orientation can lead to the

112

Part III: Articles

need for drastic changes in arm and hand posture at the target, such
that movement planning requires carefully chosen strategies for
successful control. Second, reaching and grasping are two separate
motor behaviors that may or not be executed independently of each
other. This issue allows researchers to examine the superposition
and sequencing of movement primitives. Third, grasping has a
more interesting perceptual component than reaching, since appropriate grasp points, grasping strategies, and grasping forces need
to be selected as a function of target shape, size, and weight. The
principles of perceptuomotor coordination can thus be examined in
well-controlled experiments, including the grasping of objects that
induce visual illusions.
Among the key features of reaching and grasping are the following: (1) a fast initial arm movement to bring the hand close to the
target, (2) a slow approach movement when the hand is near the
target, and (3) a preshaping phase of the hand with initial progressive opening of the grip, followed by closure of the grip until the
object size is matched and the object is finally grasped (Jeannerod
et al., 1995; Arbib and Hoff, 1993, in Jeannerod et al., 1995). Although early models of reaching and grasping assumed independence of these different phases and simply executed them in a programmatic way, behavioral perturbation studies that changed the
target size, orientation, or distance revealed a coupling between the
phases (for a review, see Jeannerod et al., 1995), such that, e.g.,
the preshaping partially reversed when the target distance was suddenly increased. Using optimization principles, Hoff and Arbib (in
Jeannerod et al., 1995) developed a model of these interactions by
structuring the reach-and-grasp system in appropriate perceptual
and motor schemas (see SCHEMA THEORY), including abstraction
of the multifingered hand in terms of two or more virtual fingers
to simultaneously model different grip types (e.g., precision grip,
power grip) and their opposition spaces for contact selection. This
model can also be mapped onto the known functional cortical anatomy in primates. Grip force selection and the anticipation of object
properties has been studied by a number of authors (e.g., Flanagan
and Beltzner, 2000), who generally agree that the CNS seems to
use internal models to adjust grip force. From studies of the resolution of redundancy, it was concluded that the entire arm posture
at the target seems to be planned in advance (Grea et al., 2000),
but this result may need differentiation as outlined in the previous
section. In general, there seems to be a consensus that behavioral
features of reaching and grasping are carefully planned by the CNS
and are not accidental.

Motor Learning
Because of continuous change in body size and biomechanical
properties throughout development, the ability to learn motor control is of fundamental importance in biological movement systems.
Moreover, when it comes to arm and hand control, primates show
an unusual flexibility in devising new motor skills to solve novel
tasks. Learning must therefore play a pivotal role in computational
models of motor control.
One of the most visible research impacts of motor learning was
the controversy between equilibrium point control (see EQUILIBRIUM POINT HYPOTHESIS) and internal model control (see SENSORIMOTOR LEARNING and CEREBELLUM AND MOTOR CONTROL).
Proponents of equilibrium point control believed that the learning
of internal models is too complicated to be plausible for biological
information processing, while proponents of internal model control
accumulated evidence that various, in particular fast, movement
behaviors cannot be accounted for by equilibrium point control. At
present, there seems to be an increasing consensus that internal
model control is a viable concept for biological motor learning, and
that the equilibrium point control strategy in its original and appealing simplicity is not tenable. Behavioral learning experiments

that were created in the wake of the equilibrium point control discussion sparked a new branch of research on motor learning (see
SENSORIMOTOR LEARNING and GEOMETRICAL PRINCIPLES IN MOTOR CONTROL). Adaptation to virtual force fields, to altered perceptual environments, or to virtual objects are among the main
behavioral paradigms to investigate motor learning, with the goal
of better understanding the time course, representations, control
circuits, retention, and functional anatomy of motor learning (see
SENSORIMOTOR LEARNING).

Interlimb Coordination
In robotics, the control of two limbs can be accomplished as if the
two systems were completely independent, thus reducing the control problem to that of controlling two robots instead of one. In
biological motor control, such independence does not exist, and a
rich area of behavioral investigation examines the computational
principles and constraints that arise from the coordination of multiple limbs. In arm and hand control, the approach of dynamic
pattern formation (e.g., Kelso, 1995) has been a prominent methodology to account for interlimb coordination. In this approach,
motor control in general and interlimb coordination in particular
are viewed as an assembly of the required degrees of freedom of
the motor system into a task-oriented attractor landscape (Saltzman
and Kelso, 1987, in Kelso, 1995). Interlimb coordination is thus
conceived of as the result of coupling terms in nonlinear differential
equations. An important question thus arises as to what kind of
equations model the control of movement, and what kind of variables cause the coupling. A variety of models of movement generation with nonlinear dynamics approaches were suggested, based
on differential equations, that either generate movement plans
(Kelso, 1995; Sternad, Dean, and Schaal, 2000) or directly generate
forces. The origin of coupling between limbs, however, remains an
issue of controversy. Possible sources could be perceptual, proprioceptive, purely planning-based, interaction force–based, a preference for homologous muscle activation, or neural crosstalk. By
demonstrating that the orientation of limbs in external space can
explain a certain class of interlimb coordination, recent behavioral
results (Mechsner et al., 2001) emphasized that perceptual coupling
may be much more dominant than previously suspected. In general,
however, there seems to be a strong need for detailed computational
modeling to elucidate the computational and neuronal principles of
interlimb coordination.

Intralimb Coordination
Investigations of intralimb coordination seek to uncover the specific principles by which individual segments of a limb move relative to one other. Models of arm and hand control that are based
on optimal control (see OPTIMIZATION PRINCIPLES IN MOTOR CONTROL) or optimal resolution of redundancy automatically solve the
intralimb coordination problem by means of their optimization
framework; any kind of special behavioral features would be
considered accidental. However, some research has considered
whether some special rules of information processing by the CNS
can be deduced from the regularities of intralimb coordination. For
reaching movements, the simple mechanism of joint interpolation
can account for a large set of behavioral features when the onset
times of the movements in individual degrees of freedom are staggered, an older observation that has been confirmed in more recent
work (Desmurget et al., 1995). For rhythmic movement, it is of
interest to know how the oscillations in individual degrees of freedom remain phase-locked to each other, and whether there are preferred phase-locked modes (Schaal et al., 2000). As in interlimb
coordination, models of nonlinear differential equations seem the

Artificial Intelligence and Neural Networks
most suitable for capturing the effects of rhythmic intralimb
dynamics.

Perception-Action Coupling
Most of the behavioral studies outlined in the previous sections
were primarily concerned with specific aspects of motor control
and less with issues of perceptuomotor control. However, the interaction of perception and action reveals many constraints on the
nervous system. In the behavioral literature, there is a large body
of research that examines particular perceptuomotor skills, such as
the rhythmic coordination of arm movement during the juggling of
objects or the interaction of external forces and limb dynamics to
generate movement (e.g., Sternad, Duarte, et al., 2000). This interesting topic cannot be discussed in detail here.

Discussion
Behavioral phenomena of arm and hand movement have sparked
a rich variety of computational models on various levels of abstraction. Although some topics, such as internal model control, have
gained solid ground in recent years (Flash and Sejnowski, 2001),
many other issues remain controversial and deserve more detailed
and computational investigations. Perhaps the most interesting topics for future research are the importance of the dynamic properties
of the musculoskeletal system in facilitating motor control, the role
of real-time perceptual modulation of motor control, and dynamic
systems models versus optimal control-based models.
Road Maps: Mammalian Motor Control; Robotics and Control Theory
Related Reading: Eye-Hand Coordination in Reaching Movements;
Grasping Movements: Visuomotor Transformations; Limb Geometry,
Neural Control; Robot Arm Control; Sensorimotor Learning

References
Bullock, D., and Grossberg, S., 1988, Neural dynamics of planned arm
movements: Emergent invariants and speed-accuracy properties during
trajectory formation, Psychol. Rev., 95:49–90. ⽧

113

Bullock, D., Grossberg, S., and Guenther, F. H., 1993, A self-organizing
neural model of motor equivalent reaching and tool use by a multijoint
arm, J. Cogn. Neurosci., 5:408–435.
Desmurget, M., Prablanc, C., Rossetti, Y., Arzi, M., Paulignan, Y., Urquizar, C., and Mignot, J. C., 1995, Postural and synergic control for threedimensional movements of reaching and grasping, J. Neurophysiol.,
74:905–910.
Flanagan, J. R., and Beltzner, M. A., 2000, Independence of perceptual and
sensorimotor predictions in the size-weight illusion, Nature Neurosci.,
3:737–741.
Flash, T., and Sejnowski, T., 2001, Computational approaches to motor
control, Curr. Opin. Neurobiol., 11:655–662. ⽧
Grea, H., Desmurget, M., and Prablanc, C., 2000, Postural invariance in
three-dimensional reaching and grasping movements, Exp. Brain Res.,
134:155–162.
Jeannerod, M., Arbib, M. A., Rizzolatti, G., and Sakata, H., 1995, Grasping
objects: The cortical mechanisms of visuomotor transformation, Trends
Neurosci., 18:314–320. ⽧
Kelso, J. A. S., 1995, Dynamic Patterns: The Self-Organization of Brain
and Behavior, Cambridge, MA: MIT Press.
Mechsner, F., Kerzel, D., Knoblich, G., and Prinz, W., 2001, Perceptual
basis of bimanual coordination, Nature, 414:69–73.
Mottet, D., and Bootsma, R. J., 2001, The dynamics of rhythmical aiming
in 2D task space: Relation between geometry and kinematics under examination, Hum. Movement Sci., 20:213–241.
Schaal, S., and Sternad, D., 2001, Origins and violations of the 2/3 power
law in rhythmic 3D movements, Exp. Brain Res., 136:60–72. ⽧
Schaal, S., Sternad, D., Dean, W., Kotoska, S., Osu, R., and Kawato, M.,
2000, Reciprocal excitation between biological and robotic research, in
Sensor Fusion and Decentralized Control in Robotic Systems III, Proceedings of the SPIE, Boston, MA: SPIE.
Sternad, D., Dean, W. J., and Schaal, S., 2000, Interaction of rhythmic and
discrete pattern generators in single joint movements, Hum. Movement
Sci., 19:627–665.
Sternad, D., Duarte, M., Katsumata, H., and Schaal, S., 2000, Dynamics of
a bouncing ball in human performance, Phys. Rev. E, 63:1–8.
Sternad, D., and Schaal, D., 1999, Segmentation of endpoint trajectories
does not imply segmented control, Exp. Brain Res., 124:118–136.

Artificial Intelligence and Neural Networks
John A. Barnden and Marcin Chady
Introduction

Relative Advantages

This article surveys the distinctions between symbolic artificial intelligence (AI) systems and neural networks (NNs), their relative
advantages, and ways of attempting to bridge the gap between the
two.
For this review we can take AI to consist of the development,
analysis, and simulation of computationally detailed, efficient systems for performing complex tasks, where the tasks are broadly
defined, involve considerable flexibility and variety, and are typically similar to aspects of human cognition or perception. These
broad tasks include natural language understanding and generation;
expert problem solving; common-sense reasoning; visual scene
analysis; action planning; and learning.
There is nothing in this description of AI that prevents the computational systems from being neural networks. Nevertheless, it is
fair to say that the bulk of AI can be called “traditional” or “symbolic” AI, relying on computation over symbolic structures (e.g.,
logic formulae). The rest of the review will therefore discuss relationships between symbolic AI and NNs.

Advantages of Neural Networks
One of the main benefits claimed for NNs is graceful degradation,
especially when they are of the distributed variety (LOCALIZED
VERSUS DISTRIBUTED REPRESENTATIONS). A computational system is said to exhibit graceful degradation when it can tolerate
significant corruption of its input or internal workings. The toleration consists of the system’s continuing to perform usefully,
though not necessarily perfectly. In NNs, input imperfection is typically a matter of corruption of individual input activation vectors.
Internal corruption usually takes the form of deletions of nodes or
links or corruptions of the link weights.
Symbolic AI systems, on the other hand, tend not to degrade
gracefully. Consider a simple rule-based system. A small corruption of an input data structure is likely to make it fail to match the
precise form expected by the rules that would otherwise have applied, so that they totally fail to be enabled. Equally, other rules
might erroneously be enabled. Similarly, even minor damage to a
rule can have very large effects on how the system operates.

114

Part III: Articles

As a special case of graceful degradation, NNs sometimes exhibit error correction, whereby an erroneous or corrupted pattern
on an input bank of units leads to a corrected version of the pattern
appearing in the network, enabling the network to proceed as if the
correct version had been provided. Related to this type of error
correction is pattern completion, whereby an incomplete pattern is
filled out to become a more complete pattern somewhere in the
network.
Also related to graceful degradation is automatic similaritybased generalization, in which previously unseen inputs that are
sufficiently similar to training inputs lead naturally to behavior that
is usefully similar to (or captures central tendencies in) the behavior
elicited by the training inputs. There is a sense in which similarity
of representations induces similarity of processing more readily
than it does in symbolic AI: there is, by and large, a higher degree
of naturally achievable continuity in the mapping from inputs to
outputs. In addition, previously unseen blends of different representations will naturally tend to lead to processing that is a blend
of the processing that would have arisen from the different representations that have been blended together. Such behavior is possible in symbolic AI but specific system-design effort is needed to
achieve it.
Importantly, NNs can learn generalizations or category prototypes by exposure to instances, through fairly straightforward, uniform weight modification procedures. These generalizations or prototypes come to be implicit in the adjusted weights. Although
learning is intensively studied in symbolic AI, and some learning
paradigms in symbolic AI involve adjustment of numerical parameters akin to NN weights, symbolic processing does not provide
any specific support to these paradigms. The paradigms could
therefore be said to arise less easily and naturally out of symbolic
processing than out of NN activity.
The preceding properties of NNs have found their application in
content-based access (or associative access) to long-term memory
(see ASSOCIATIVE NETWORKS). This can take two different forms.
First, let us assume, as usual, that a neural net’s long-term memory
is its set of weights. The manipulation of an input vector by the
network can be thought of as the bringing to bear of particular
content-relevant long-term memories on that vector. Second, in any
NN that learns a map from particular inputs to particular outputs,
an output can be thought of as a particular long-term memory recalled directly on the basis of the content of the input. Contentbased access is not as easily provided in symbolic systems implemented in conventional computers, although it can be obtained to
some useful degree by sophisticated indexing schemes (see Bonissone et al. in Barnden and Holyoak, 1994), associative computer
memories, or hashing (see Touretzky in Hinton, 1991, for
discussion).
NNs can have emergent rule-like behavior. Such behavior can
be described, approximately at least, as the result of following symbolic rules, even though the system does not contain representations of explicit rules (see Elman, 1991). Emergent rule-like behavior is a central issue in the application of neural networks to
high-level cognitive tasks.
More generally, NNs tend to be more sensitive to subtle contextual effects than symbolic AI systems are, because multiple sources
of information can more easily be brought to bear in a gracefully
interacting and parallel way. This property of NNs facilitates soft
constraint satisfaction. That is, it is possible to arrange for some
hypotheses to compete and cooperate with each other, gradually
influencing each other’s levels of confidence until a stable set of
hypotheses is found. Each hypothesis is represented by a node or
group of nodes in the neural network, and the constraints are encoded by links joining those nodes or groups. The constraintsatisfaction is soft because no individual constraint needs to be
satisfied absolutely. By contrast, although many symbolic AI sys-

tems are designed to do constraint satisfaction, the symbolic framework provides no special support for it, particularly when the constraints are soft.
Finally, NNs are an inherently parallel model of computation
whose parallelism is straightforwardly realizable in a physical
substrate.

Advantages of Symbolic AI Systems
The symbolic framework is better than NNs at encoding and manipulating the complex, dynamic structures of information that appear to be needed in cognition. These structures can, for instance,
be interpretations of natural language utterances, descriptions of
complex scenes, complex plans of action, or conclusions drawn
from other information. The encodings of such structures, whether
these encodings are symbolic or otherwise, need to have the following important properties. (See also Shastri in Barnden and Pollack, 1991.)
1. The encodings must often be highly temporary—for instance,
encodings of interpretations of natural language sentences and
encodings of intermediate conclusions during reasoning need to
be rapidly created, modified, and discarded. Although activation
patterns in NNs are temporary, temporariness is challenging for
NNs when it is combined with properties 2–6.
2. The encoding technique must allow the encoded structures to
combine information items (e.g., word senses) that have never
been combined before, or never been combined in quite the
same way, in the experience of the system.
3. The encodings must allow the encoded information to have
widely varying structural complexity. Natural language sentence interpretations provide illustrations of this point.
4. In particular, the encoded structures can be multiply nested. In
the sentence “John believes that Peter’s angry behavior toward
Mary caused her to write him a strongly worded letter,” the
anger description is nested within a causation description that
is nested within a belief report.
5. A given type of information can appear at different levels of
nesting. A system might have to represent a sitting room that
has a wall that bears a picture that itself depicts a dining room.
As another illustration, a belief might be about a hope that is
about a belief.
6. A given type of information may also have to be multiply instantiated in other ways, as when, for instance, there are three
love relationships that need to be simultaneously represented.
Turning to manipulations, cognitive systems must exhibit strong
properties of systematicity of processing—each information structure J that one cares to mention has an extremely large class of
variants that must be able to be subjected to the same sort of processing as J is; and the class of variants is far too large to imagine
that each variant is processed by a separate piece of neural network
or a separate symbolic module. So, we must have symbolic AI
systems and NNs capable of very flexible and general processing.
(See also SYSTEMATICITY OF GENERALIZATIONS IN CONNECTIONIST NETWORKS.)
The variable-binding problem for neural networks is one manifestation of the need for systematicity of processing. Suppose one
wishes a neural network to make inferences that obey the following
rule: X is jealous of Z whenever X loves Y, Y loves Z, and X, Y,
and Z are distinct people. In this statement of the rule, the variables
X, Y, and Z can be replaced by any suitable people-descriptions,
such as “Joe Bloggs’ father’s boss.” The systematicity issue in this
example is that of avoiding having replication of machinery for all
the different possible combinations of values for these variables.
(Each such combination is a J in the terms of the previous para-

Artificial Intelligence and Neural Networks
graph.) This issue can also be thought of as the variable binding
problem for the example, even if a neural network dealing with it
does not have any explicit representation of the rule or the variables
in it. In the special case in which a neural network implements the
rule as a subnetwork and has particular units, subnetworks, or activation patterns that play the role of the three variables, the variable
binding problem, in a narrower sense now, is the problem of how
the network is to be able to “bind” such a unit, subnetwork, or
activation pattern to a particular value at any given moment, and
how the binding is to be used in processing.
Cognitive systems must also exhibit a high degree of structuresensitivity in their processing. Pieces of information that have complex structure must be processed in ways that are heavily dependent
on their structure as such, not (just) on the nature of constituents
taken individually. For example, consider the operation of inferring
from “not both A and B” that “not A or not B.” The operation is
independent of what A and B are—it is only the “not both . . . and
. . .” structure that is important.
These features of information structure encodings and manipulations combine to distinguish the types of information that neural
networks for reasoning, natural language understanding, etc. must
deal with from the types of information that typical neural networks
cater for. Traditional NN techniques were originally developed
largely for specific “low level” applications, such as restricted
forms of pattern recognition, or for limited forms of pattern association. Because of the resulting continuing limitations in most
applications of NNs, it has been sufficient for NNs to adhere, by
and large, to the following restrictions (although almost every restriction is violated by some NN subparadigm). These restrictions
cause difficulty in trying to apply NNs to natural language understanding, common-sense reasoning, and the like.
1. There is typically no dynamic, rapid creation and destruction of
nodes and links. Therefore, temporary information cannot be
encoded in temporary network topology changes. (Some of this
effect can, however, be obtained by techniques such as dynamic
links described in DYNAMIC LINK ARCHITECTURE (q.v.), or by
higher-order units, whose activation is sensitive to weighted
sums of products of input values, rather than just to weighted
sums of input values.)
2. Links in NNs are not differentiated by labeling, unlike the links
in symbolic structures such as SEMANTIC NETWORKS (q.v.).
Therefore, in an NN, information that could otherwise be put
into link labels has to be encoded somehow in activation values,
weights, extra links, or other features of network topology, adding significantly to the cumbersomeness of the net and its processing. (See Barnden and Srinivas, 1991, for more discussion.)
3. The resolution of NN activation values is generally not fine
enough to allow them individually to encode complex symbolic
structures. Most typically, activation values merely encode confidence levels of some sort.
4. Pointers are usually not allowed. That is, activation values or
patterns are not allowed to act as names or addresses of parts
of the network itself.
5. Stored programs are not allowed. That is, activation values or
patterns cannot act as instructions (names of internal computational actions).

Further Comparative Remarks
The advantages claimed here for NNs are not clearcut. For instance,
there are types of AI systems that readily exhibit forms of graceful
degradation, pattern completion, and similarity-based generalization. In particular, as Barnden in Barnden and Holyoak (1994) argues in detail and other researchers have noted, these benefits are
natural properties of suitably designed symbolic analogy-based rea-

115

soning systems (see ANALOGY-BASED REASONING AND METAand also MEMORY-BASED REASONING).
Just as NNs support some types of learning more readily than
symbolic AI systems do, the converse holds as well. Symbolic AI
systems, by virtue of their ability to handle complex temporary
information structures, are in a better position to perform various
types of rapid learning, proceeding in large steps rather than
lengthy, gradual weight modification. For instance, a symbolic AI
system is in a good position to reason about why some plan of
action failed, and thus quickly and greatly amend relevant parts of
its knowledge base or planning strategies. Also, the ability of neural
networks to learn generalizations is often hindered by elaborate,
extensive training regimes. It is true that in some learning regimes,
such as some forms of Hebbian learning, final weights are calculated in a direct way from single presentations of training items.
But more typically, the number of training-item presentations one
needs to make to the network runs to tens or hundreds of thousands
before useful results can be obtained.
NNs are good at allowing hypotheses to be held with varying
degrees of confidence, the degrees being realized as activation levels. However, it is commonplace also in symbolic AI to have numerical degrees of confidence. These appear in DECISION SUPPORT
SYSTEMS AND EXPERT SYSTEMS (q.v.) and elsewhere. However,
the normal properties of activation spread and activation combination in NNs support confidence levels in a natural way. In symbolic AI systems the computations have to be specially and explicitly designed.
The contrasts between neural networks and symbolic AI that
were presented earlier are clouded by the fact that NNs can be
implementational. Implementational NNs are exact implementations of symbol processing schemes of the sort used in traditional
symbolic AI systems. That is, network-unit activations (and/or link
weights, possibly) can be regarded as exactly encoding symbolic
representations as used in traditional AI systems—such as logic
formulas, frames, schemas, or pieces of semantic network—and
changes in network state can be regarded as exactly encoding traditional symbolic manipulation steps—such as traversal, concatenation, and rearrangement of structures—that are used in traditional
AI for directly effecting reasoning, planning, natural language understanding, etc. See, for example, Barnden’s and Shastri’s chapters in Barnden and Pollack (1991) and Lange and Wharton’s chapter in Barnden and Holyoak (1994).
The nonimplementational style includes NNs that can be usefully viewed as approximately manipulating traditional symbolic
objects in traditional ways. However, the nearer an NN is to being
implementational, the more it runs the danger of inheriting the disadvantages of symbolic AI, such as the tendency to lack graceful
degradation.
PHOR

Bridging the Gap
The discrepancy in the relative advantages of (nonimplementational) NNs and symbolic AI systems has been the focus of much
attention during the last decade or so (see, e.g., Barnden and Pollack, 1991; Browne and Sun, 2001; Hinton, 1991; Jagota et al.,
1999; McGarry, Wermter, and McIntyre, 1999). We shall review
here some representative attempts to tackle the problem.
A common approach to extending conventional types of NN
processing to handle complex dynamic structures is to use reduced
representations, also known as compressed encodings. See, e.g.,
Hinton, Pollack, and St. John’s chapter and McClelland’s chapter
in Hinton (1991), as well as Elman (1991). A reduced representation is a single activation vector that is created from the several
activation vectors that encode the constituents of the structure in
such a way that the resulting vector is of roughly the same size as
each of the constituents’ vectors. For example, the constituents

116

Part III: Articles

could be words, and a sequence of word encodings could represent
a sentence. The reduced representation is then a roughly word-sized
vector for the whole sentence.
One architecture used to produce reduced representations of sequences of items is a Simple Recurrent Network (SRN). An SRN
is typically a three-layer network in which the input to the middle
layer consists of an item in the sequence together with the previous
activation pattern in the middle layer itself. As a result of backpropagation training, the encodings produced in the middle layer
are compressed vectors representing the current input in the context
of the history of items presented to the network so far. SRNs have
been successfully used, for instance, for predicting the category of
the next word in a sentence being inputed (Elman, 1991).
A more general-purpose architecture for producing recurrent
compressed encodings is Pollack’s Recursive Auto-Associative
Memory (RAAM) (see Pollack in Hinton, 1991). The input and
output layers are divided up into segments that hold constituent
encodings. The net is trained to map sequences of constituent encodings to themselves. The activation pattern that appears on the
hidden layer of the trained network in response to a particular sequence of constituent encodings on the input layer is the compressed encoding for the sequence. (And a compressed encoding
can be decoded by placing it in the hidden layer: a close approximation, hopefully, to the sequence of constituent encodings appears
on the output layer.) Also, during training, a hidden layer pattern
can be copied into any of the segments in the input and output
layer, leading to the ability of the network to handle recursive structures some of whose constituents are themselves sequences of constituents. An example of such a structure is the sentence “John
knows that Sally is clever,” thought of as having the sentence
“Sally is clever” as a constituent.
There are some indications that compressed encodings can support holistic structure-sensitive processing by means of conventional NN techniques such as feedforward association networks
(see, e.g., Chalmers, 1990; Pollack in Hinton, 1991). The processing is holistic in that the encodings are not uncompressed into the
individual activation vectors that encode their notional constituents. For example, Chalmers successfully trained a three-layer
backpropagation network to transform compressed encodings of
active English sentences into compressed encodings of their passive counterparts. The hidden layer had the same size as the input
and output layers (the size of a compressed encoding) and the net
operated in one pass of activation, so that it cannot have been working by first decoding the input compressed encodings into the corresponding sequence of constituent encodings.
However, in-depth analysis of RAAM-like systems (Kolen,
1994) reveals that the computation depends on very fine tuning of
synaptic weights and highly precise activation levels. The implication is that holistic computation based on RAAM-generated encodings is sensitive to noise, which is a significant drawback given
that graceful degradation is a major argument for using neural networks. An additional complication associated with all of the aforementioned techniques of generating reduced representations is the
lengthy process of weight training.
These problems are avoided in a related approach of which a
central example is the Holographic Reduced Representation (HRR)
technique of Plate (1995). See also Rachkovskij and Kussul (2001).
HRRs use predefined combination operations (circular convolutions) to produce compressed encodings. No training is required
and both encoding as well as decoding are performed in a single
step. What is more important, though, is that these transformations
offer a more comprehensive account of systematicity. Given a sufficiently large size of code vectors, not only can HRRs recursively
bind any number of elements, but also, using simple vector addition, multiple bindings can be combined further to form collections
of predicates. Such collections retain superficial and structural sim-

ilarity of structures, so that decoding is not necessary to estimate
an item’s relevance for certain types of computation. If necessary,
any one predicate can be readily extracted using inverse operations,
although not without some loss of information due to the nature of
circular convolution and vector addition. The operations introduce
some degree of noise, which is further amplified by the decoding
transformation. However, it can be rectified using an autoassociative memory to recover the original elements. Using this
approach, Plate (1995) shows how HRRs can be used to represent
sequences and more complex structures, and how to achieve chunking and variable binding.
In a different approach to bridging the gap, Barnden in Barnden
and Holyoak (1994) capitalizes on the comment made previously
that symbolic analogy-based reasoning possesses many of the main
advantages of nonimplementational NNs. The claim is that an implementational NN that implements a symbolic analogy-based reasoning system inherits those advantages, as well as the symbolic
AI advantage with respect to complex dynamic information
structures.
The preceding approaches assume that it is worthwhile to develop gap-bridging systems that are neural networks in their entirety, rather than developing systems that are some combination
of NN machinery with symbolic AI machinery (where the latter is
given no NN realization). The latter, hybrid, strategy is a popular
approach to bridging the gap (McGarry et al., 1999). The simpler
types of hybridization occur in systems that have largely separate
neural and symbolic modules (see, e.g., Hendler in Barnden and
Pollack, 1991). But more intimate hybridizations have been developed, for instance, in networks where an individual node or link
can act partially like those in neural networks and partially like
those in symbolic networks.
Although the more implementational an NN is the more it risks
inheriting disadvantages of symbolic AI, it may still be that some
of the implementational NN techniques could be adapted for use
in gap-bridging systems that escape those disadvantages. Therefore, we will now look at some of the techniques.
A crucial aspect of implementational NNs is the way in which
they allow representational items to be rapidly and temporarily
combined so as to form encodings of temporary complex information structures. One form of this dynamic combination (or temporary association) issue is the variable binding problem, and a
closely related form is the role binding problem. The variable binding problem was described previously. The role binding problem
is concerned with giving specific values to the roles (slots) in predicates, frames, schemas, and the like.
An immediately obvious, and somewhat natural, approach to
dynamic combination is to combine network nodes or assemblies
by adding new links or giving non-zero weights to existing zeroweight links. However, this method is highly cumbersome because
network structure is not data that is directly manipulable by the
network itself. Another rather similar approach is to facilitate existing (non-zero-weight) connection paths, between nodes/assemblies that are to be combined, by activating intermediate nodes on
the paths. These nodes are called binding nodes. Since the dynamic
combination structure is now encoded in the activation levels of
binding nodes, the net can more easily analyze that structure. However, the processing is still cumbersome (Barnden and Srinivas,
1991).
A distinctly different approach is to deem nodes/assemblies to
be bound together when they fire in synchrony (see STRUCTURED
CONNECTIONIST MODELS, COMPOSITIONALITY IN NEURAL SYSTEMS, and DYNAMIC LINK ARCHITECTURE). See in particular Shastri in Barnden and Pollack (1991), and Henderson and Lane (1998).
The method is an important special case of the more general notion
of binding nodes together by giving them similar spatiotemporal
activation patterns. This is the pattern-similarity association tech-

Associative Networks
nique: see Barnden and Srinivas (1991) and Barnden in Barnden
and Pollack (1991).
Distinctly different again is the use of positional encodings of
dynamic combinations. In the more developed forms of this idea
(see Barnden in Barnden and Pollack, 1991), activation patterns are
dynamically combined by being put into suitable relative positions
with respect to each other, much as bit-strings in computer memory
can be put into contiguous memory locations to form records.
A somewhat pointer-like technique has been implemented: see
Lange and Wharton in Barnden and Holyoak (1994). Different
parts of the network are capable of emitting activation patterns that
are thought of as their “signatures.” Other parts can then temporarily hold signatures and thereby point, in a sense, to the parts that
possess the signatures.
One noteworthy way of achieving temporary association is the
use of auto-associative memories with rapid Hebbian learning, as
in van der Velde (1995). Van der Velde demonstrates how multiple
elements can be stored in a single network while preserving their
ordering. Each element in the sequence refers to the next and previous ones by unique pointers that constitute part of the memory
trace in an auto-associative module. Using this approach, van der
Velde builds a conventional stack-based generator of centerembedded sentences. Despite its implementational architecture,
this model manages to retain the graceful-degradation property of
nonimplementational NNs. Hebbian association was also used by
Hadley et al. (2001) to achieve a strong from of systematicity.
Finally, Smolensky in Hinton (1991) proposed an abstract but
influential binding and structure-representation approach based on
tensors. Some realizations of this approach involve binding nodes,
but the approach can be seen to subsume other concrete techniques
as well.

Discussion
One theme of this review has been that the relative advantages of
symbolic AI and NNs are less clear-cut than is usually implied. In
particular, although NNs have been successful for some purposes
and can have advantages such as graceful degradation, most NN
research has not addressed the complex information processing issues routinely tackled in symbolic AI research. The latter field has
contributed much more, for instance, to the study of how natural
language discourse can be understood and common-sense reasoning performed. Nevertheless, pursuing nonsymbolic approaches to
the problems is beneficial for as long as the symbolic approaches
fail to provide all the answers.
Some of the open questions in the area of this review are: Is it
actually necessary to go beyond symbolic AI in order to account
for complex cognition? If it is, should symbolic AI be dispensed
with entirely, or is some amount of complex symbol-processing
unavoidable? How can reasoning, natural language understanding,

117

etc. be effected by neural networks without just implementing conventional symbol processing? How can different styles of system,
e.g., implementational and nonimplementational neural networks,
or neural networks and non-neural systems, be gracefully combined
into hybrid systems?
Road Map: Artificial Intelligence
Related Reading: Compositionality in Neural Systems; Connectionist and
Symbolic Representations; Hybrid Connectionist/Symbolic Systems;
Multiagent Systems; Philosophical Issues in Brain Theory and Connectionism; Structured Connectionist Models; Systematicity of Generalizations in Connectionist Networks

References
Barnden, J. A., and Holyoak, K. J. (Eds.), 1994, Advances in Connectionist
and Neural Computation Theory, Vol. 3: Analogy, Metaphor and Reminding, Norwood, NJ: Ablex Publishing Corp.
Barnden, J. A., and Pollack, J. B. (Eds), 1991, Advances in Connectionist
and Neural Computation Theory, Vol. 1: High Level Connectionist Models, Norwood, NJ: Ablex Publishing Corp. ⽧
Barnden, J. A., and Srinivas, K., 1991, Encoding techniques for complex
information structures in connectionist systems, Connection Science,
3:263–309.
Browne, A., and Sun, R., 2001, Connectionist inference models, Neural
Networks, 14:1331–1355. ⽧
Chalmers, D. J., 1990, Syntactic transformations on distributed representations, Connection Science, 2:53–62.
Elman, J. L., 1991, Distributed representations, simple recurrent networks,
and grammatical structure, Machine Learning, 7:195–225.
Hadley, R. F., Rotaru-Varga, A., Arnold, D. V., and Cardei, V. C., 2001,
Syntactic systematicity arising from semantic predictions in a Hebbiancompetitive network, Connection Science, 13:73–94. ⽧
Henderson, J., and Lane, P., 1998, A connectionist architecture for learning
to parse, in Proceedings of COLING-ACL (Montreal, Canada, 1998), pp.
531–537.
Hinton, G. E. (Ed.), 1991, Connectionist Symbol Processing, Cambridge,
MA: MIT Press.
Jagota, A., Plate, T., Shastri, L., and Sun, R. (Eds.), 1999, Connectionist
symbol processing: Dead or alive?, Neural Computing Surveys, 2:1–
40. ⽧
Kolen, J. F., 1994, Fool’s gold: Extracting finite state machines from recurrent network dynamics, in Advances in Neural Information Processing Systems, Vol. 6 (J. D. Cowan, G. Tesauro, and J. Alspector, Eds.),
San Mateo, CA: Morgan Kaufmann, pp. 501–508.
McGarry, K., Wermter, S., and MacIntyre, J., 1999, Hybrid neural systems:
From simple coupling to fully integrated neural networks, Neural Computing Surveys, 2:62–93.
Plate, T. A., 1995, Holographic reduced representations, IEEE Transactions
on Neural Networks, 6:623–641.
Rachkovskij, D. A., and Kussul, E. M., 2001, Binding and normalization
of binary sparse distributed representations by context-dependent thinning, Neural Computation, 13:411–452.
van der Velde, F., 1995, Symbol manipulation with neural networks: Production of a context-free language using a modifiable working memory,
Connection Science, 7:247–280.

Associative Networks
James A. Anderson
Introduction
The operation of association involves the linkage of information
with other information. Although the basic idea is simple, association gives rise to a particular form of computation, powerful and
idiosyncratic. The mechanisms and implications of association
have a long history in psychology and philosophy. Association is

also the most natural form of neural network computation. This
article will discuss association as realized in neural networks as
well as association in the more traditional senses.
Neural networks are often justified as abstractions of the architecture of the nervous system. They are composed of a number of
computing units, roughly modeled on neurons, joined together by

118

Part III: Articles

connections that are roughly modeled on the synapses connecting
real neurons together. The basic computational entity in a neural
network is related to the pattern of activity shown by the units in
a group of many units.
Because of the use of activity patterns—mathematized as state
vectors—as computational primitives, the most common neural
network architectures are pattern transformers which take an input
pattern and transform it into an output pattern by way of system
dynamics and a set of connections with appropriate weights. In a
very general sense, therefore, neural networks are frequently designed as pattern associators, which link an input pattern with the
“correct” output pattern. Learning rules are designed to construct
accurate linkages. The most common feedforward neural network
architectures realize this linkage by way of connections between
layers of units (Figure 1). There may be a single set of modifiable
connections between input and output (Figure 1B), or multiple layers of connections (Figure 1C). Another common architecture is
realized by a single layer of units where the units in the layer are
recurrently interconnected (Figure 1A).
One common design goal of a feedforward associator (Figures
1B and 1C) is to realize what Kohonen (1977) has labeled heteroassociation, that is, to link input and output patterns that need have
no relation to each other. Another possibility is to realize what
Kohonen has called autoassociation, where the input and output
patterns are identical. Recurrent networks (Figure 1A) are well
suited to autoassociation.

Because the input and output patterns must correspond to information about the real world, the data representation is of critical
importance at all levels of network operation. For example, simple
pattern recognizers are often realized by neural networks as a special form of pattern associator by assuming a particular output representation, one where a single active output unit corresponds to
the category of the input. Different categories correspond to different active output units. This highly localized representation is
sometimes called a grandmother cell representation, because it implies that only when one particular unit is active is “grandmother”
being represented. The alternative representation is called a distributed representation, where representation of a concept like
“grandmother” may contain many active units. Choice of representation makes a major difference in how networks are used and how
well they work, and is usually more important than the exact choice
of network architecture and learning rule. A common situation in
engineering applications of neural networks is to have a distributed
representation at the input of the network and a grandmother cell
representation at the output. In the vertebrate nervous system there
is little evidence for this output representation; essentially all normal motor acts involve the coordinated discharge of large groups
of neurons. Distributed activity patterns are associated with distributed patterns from one end of most biological networks to the
other in vertebrates (but see LOCALIZED VERSUS DISTRIBUTED
REPRESENTATIONS), though there are some examples of extreme
selectivity in invertebrates. The degree of distribution is a matter
for experimental investigation.

Neural Network Associators

Figure 1. Three common basic neural network architectures. A, A set of
units connects recurrently to itself by way of modifiable connections. (The
connections are drawn as reciprocal.) B, A feedforward network in which
an input pattern is transformed to an output pattern by way of a layer of
modifiable connections. C, A more general feedforward network. An input
layer projects to an intermediate layer of units. The intermediate layer is
often called a hidden layer because it may not be accessible from outside
the network. The hidden layer then projects to the output units.

Let us give an example of how easily neural network learning rules
and architectures give rise to associative behavior. Consider the
two-layer network diagrammed in Figure 1B. Consider a situation
where a pattern of activity, a state vector f, is present at the input
set of units and another pattern, state vector g, is shown by the
output set of units.
We want to link two patterns so that when f is presented to the
input of the network, g will be generated at the output. In this twolayer network (two layers of units, one layer of connections), we
will assume that the connections initially are zero and we want to
change them to make the association between patterns f and g. We
will also assume that all connection strengths are potentially
changeable and the set of connection strengths forms a connection
matrix (or synaptic matrix) which we will call W, for “weights.”
We have to propose a learning rule, but we also have to make
some additional assumptions about the entire system. For example,
virtually all artificial neural network learning assumes that the network is learning discrete pairs of patterns, that is, learning takes
place only occasionally, when the time is ripe. One could speculate
that learning in animals is a dangerous operation—after all, the
nervous system is being rewired—and is kept under tight control.
Primates are unusual in the degree of learned flexibility their nervous system allows. There is physiological evidence that amount
of learning is controlled by diffuse biochemical processes. Dangerous and striking events, causing a biochemical upheaval, give
rise to what have been called “flashbulb memories” where everything, including totally irrelevant detail, is learned. (“Where were
you when John F. Kennedy was assassinated?” is practically guaranteed to involve a flashbulb memory in those old enough to remember it. September 11, 2001, provides a modern example.) Presumably this corresponds to an undiscriminating “learn” command.
In terms of modeling, these observations mean that the decision to
learn is decoupled from the act of learning.
Let us assume that we have an input pattern and an output pattern
and we wish to associate them for good and sufficient reasons. We
assume that we can impress pattern f on the input set of units and
pattern g on the output set of units. By far the most common net-

Associative Networks
work learning rule used is one or another variant of what is called
the “Hebb synapse,” described in Hebb (1949). Perhaps the most
quoted sentence in the neural network literature is from Hebb:
“When an axon of cell A is near enough to excite a cell B and
repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells, such that
A’s efficiency as one of the cells firing B, is increased.” (Hebb,
1949:62). The essence of the Hebb synapse is that there has to be
a conjunction of activity on the two sides of the connection.
There is good physiological evidence for the existence of some
form of Hebb synapse in parts of the mammalian central nervous
system (see HEBBIAN SYNAPTIC PLASTICITY). However, there are
a number of “technical” problems involved in mathematically describing the resulting system. The original formulation by Hebb
was concerned with coincident excitation. Nothing was said about
coincident inhibition or about coincident excitation and inhibition.
Also, the exact function determining strength of modification was
not given, and, in fact, is not known. A common assumption in
artificial network theory is to assume some version of what is called
the generalized Hebb rule or the outer product rule. This states
that the change in strength of a connection during learning is given
by the product of activities on the two sides of the connection, that
is, if Wij is the strength of the connection, then the change in
strength DWij is proportional to the product, fj gi, where fj is the
activity of the jth input unit and gi is the activity of the ith output
unit. This convenient expression may have only a weak relationship
to physiological reality.
Given the generalized Hebb rule, if we have only a single pair
of vectors to associate, the results can be written compactly as
W ⳱ ggf T
where g is a learning constant and W is the connection (or weight)
matrix.
By making an additional assumption about the properties of the
individual neural elements, this rule leads almost immediately to a
simple pattern associator called the linear associator. Suppose the
elementary computing units are linear, so that the output is given
by the inner product between input activity and connection
strengths. Then the output pattern is given by the matrix product
of an input pattern f and the connection matrix W; that is, the output
of the network is Wf. Because we know what W is—it was constructed by the generalized Hebb rule—we can compute the output
pattern,
(output pattern) ⳱ Wf ⳱ ggf Tf ⳱ (constant)g
since f Tf is a constant, the squared length of f. The output pattern
is a constant multiple of g and, except for length, we have reconstructed the learned associate of f, that is, g.
Suppose we have a whole set of associations { f i r gi} that we
want to teach the network. (Superscripts stand for individual pattern
vectors.) If we assume that the overall strength of a connection is
the algebraic sum of its past history (an unsupported assumption),
then we have the weight matrix W given by
W⳱

兺i gg if i T

Notice that in the special case where the input patterns { f i} are
orthogonal, that is, f if j ⳱ 0 if i ⬆ j,
Wf i ⳱ (constant)gi
because the contributions to the output pattern from the other terms
forming W are identically zero since they involve the inner product
[ fi, fj] ⳱ fiTfj. This model, and in fact most simple network models,
make the prediction that outer product associators will work best
and most reliably with representations where different input associations are as orthogonal as possible. For this reason, some cortical

119

models in the neuroscience literature have explicitly discussed aspects of cortical processing in terms of orthogonalization. The most
complete reference for the linear associator and related models is
Kohonen (1977, 1984).
It is possible to change almost any assumption and still have an
associator. Hebb learning rules of virtually any kind give rise to
associative systems. As only one example, the nonlinear Hebbian
associator proposed by Willshaw, Buneman, and Longuet-Higgins
(1969) used binary connections—with strengths either one or
zero—and the resulting system still worked nicely as a pattern
associator.

Supervised Networks
The outer product associator is less accurate with nonorthogonal
patterns. However, observed distortions and human performance
are sometimes remarkably similar. (See Anderson, 1995, chap. 11,
for a model of “concept formation” that emerges when correlated
inputs are stored in the linear associator.)
Most designers of artificial networks prefer networks to produce
accurate reproductions of learned associations rather than interesting distortions. (This seemingly natural assumption is not necessarily a good one.) Supervised network algorithms can perform
more accurate association. Examples of such algorithms would include the Widrow-Hoff (LMS) algorithm, the perceptron, backpropagation, and many others. The basic mechanism employed is
error correction. Suppose we have an initial training set of patterns
to be learned. This means we know what the output patterns are
for a number of input patterns. We take an input from the training
set and let the network generate an output pattern. We then compare
the desired output pattern and the actual output pattern in some
way. This process generates an error signal. The network is then
modified using a learning rule so as to reduce the error signal.
The most commonly used error signal is based on the distance
between the actual and desired output; however, other error signals
can be more desirable. For example, one could incorporate a term
penalizing large numbers of connections or large values of connection strength. The network learning problem reduces to a minimization problem where the space formed by the connection
strengths (weight space) is searched to find the point where error
is reduced to as low a value as possible. This process requires the
use of control structures that can be complex; for example, there
is assumed to be an omniscient supervisor who compares desired
and actual network output and computes the error term as well as
implements the mechanisms to change connection strengths appropriately. The structure of these algorithm is designed to produce
good pattern association whether or not this is the aim of the network architects. (See PERCEPTRONS, ADALINES, AND
BACKPROPAGATION.)

Autoassociative Models
We have described association as pattern linkage. However, there
are alternative descriptions in the neural network literature. For
example, in the first sentence of the second chapter of their textbook, Introduction to the Theory of Neural Computation, Hertz,
Krogh, and Palmer (1991) write, “Associative memory is the ‘fruit
fly’ or ‘Bohr atom’ problem of the field” (p. 11). Their definition
of association is: “Store a set of patterns f . . . in such a way that
when presented with a new pattern fi, the network responds by
producing whichever one of the stored patterns most closely resembles fi” (p. 11). This is not, however, a description of association
but of a content addressable memory where input of partial or noisy
information is used to retrieve the correct stored information. The
source of this limited view of association lies in the ability of auto-

120

Part III: Articles

associative systems to reconstruct missing or noisy parts of learned
patterns.
Consider the autoassociative version of the linear associator.
Suppose we learn one pattern, f, of length 1, with learning constant
g ⳱ 1. Then
W ⳱ ff T and Wf ⳱ f
Suppose we take vector f, with n elements, and set to zero some of
the elements, forming a new vector, f ⬘. Let us make a second
vector, f ⬙, from only the elements that were set to zero in f ⬘. Then
f ⬘ Ⳮ f ⬙ ⳱ f and f ⬘ f ⬙T ⳱ 0. If f ⬘ is input to the autoassociator,
Wf ⬘ ⳱ ( f⬘ Ⳮ f ⬙)( f ⬘ Ⳮ f ⬙) f ⬘T ⳱ (constant) f
where the constant is related to the length of f ⬘. In operation, by
putting a part of f, f ⬘, into the network, we retrieve all of f, bar a
constant. This behavior is often referred to as the reconstructive or
holographic property of neural networks. Of course, more subtle
problems arise when W stores multiple vectors. Anyway, this type
of memory is associative because if, for example, the state vector
was meaningfully partitioned, then f ⬘ is associatively linked to f ⬙
and vice versa in the sense that input of one pattern will produce
the other. This kind of associator produces intrinsically bidirectional links (i.e., f ⬘ r f ⬙ and f ⬙ r f ⬘), unlike feedforward heteroassociators ( f r g).
Some nonlinear “attractor” neural networks with dynamics that
minimize energy functions develop their associative abilities
largely from their autoassociative architecture. The best-known examples of this kind of associator are Hopfield networks and parallel
feedback networks such as the BSB (Brain State in a Box) model
(Anderson, 1995, chap. 15). For a general review of attractor networks, see Amit (1989) and COMPUTING WITH ATTRACTORS.
Multilayer autoassociators are also possible. The multilayer encoder networks, which require the output pattern to be as accurate
a reconstruction as possible of the input pattern, also have this form.
Many autoassociative networks have close ties to known statistical
techniques such as PRINCIPAL COMPONENT ANALYSIS.
A related associative attractor model, called a bidirectional associative memory, or BAM (Kosko, 1988), is a nonlinear dynamical
system with a reciprocal feedback structure. It assumes two layers
of units, as well as pairs of associations to be learned, as in a
heteroassociator. There are connections from both input to output
and output to input. Given f and g patterns to be learned, assumed
to be binary vectors, we can form both a forward and a backward
connection matrix. If f is input, then g will be given as the output;
g at the output will give rise to f at the input because of the backward connections. Suppose the input is not exactly what was
learned. After a few passes back and forth through the system, it
can be shown that the network will stabilize, in the noise-free case,
to the learned f and g.

cussed at length how one “computes” with memorized sense images. The word recollection was used in the translation to denote
this process: “Acts of recollection happen because one change is
of a nature to occur after another.” That is, Aristotle proposed a
linkage mechanism between memories. He suggested several ways
that linkage could occur: by temporal succession or by “something
similar, or opposite, or neighboring.” This list of the mechanisms
for the formation of associations is approximately what would be
given today by psychologists.
Recollection in Aristotle’s sense was computation. It was a dynamic and flexible process: “[R]ecollecting is, as it were, a sort of
reasoning.” Aristotle argued that properly directed recollection is
capable of discovering new truths, using memorized sense images
as the raw material and learning to traverse new paths through
memory (Figure 2).
A practical problem with such an associative net is branching,
that is, what to do if there is more than one link leaving an elementary memory. Aristotle was aware of this problem: “[I]t is possible to move to more than one point from the same starting point.”
A general solution to the branching problem requires a nonlinear
mechanism to select one or the other branch.
The most influential psychologists in the twentieth century were
the behaviorists, in particular B. F. Skinner, the Harvard psychologist whose ideas about reinforcement learning unfortunately dominated much of the theoretical discussion in psychology for several
decades. This school held that learning formed an associative link
between a stimulus and a specific response. The link could be
strengthened by positive reinforcement (to a first approximation,
something useful or pleasant, or the cessation of something unpleasant) or weakened by negative reinforcement (either absence
of something pleasant or something actively unpleasant) when the
response followed the stimulus. A number of careful experiments
showed that there were accurate quantitative “laws of learning” that
were followed by animals in some simple situations.
It was debatable whether this view of association is useful in
more complex situations. From the beginning, human behavior has
seemed to humans to be far richer than stimulus-response (S r R)
association. In the 1950s Skinner wrote a book attempting to explain language behavior using associative rules. In a famous book
review, Chomsky (1957) pointed out that simple S r R association
cannot do some kinds of linguistic computation. The argument used

Psychological Association
We have shown how neural networks easily form associators of
many different kinds. We will now discuss a little of the history
of association in psychology to show how associators form a style
of computation with considerable power as well as severe
limitations.
The major outlines of one way to use an associative computer
can be found clearly expressed in Aristotle in the fourth century
B.C. Aristotle made two important claims about memory structure:
First, the elementary unit of memory is a sense image, that is, a
sensory-based set of information. Second, links between these elementary memories serve as the basis for higher-level cognition.
An English translation by Richard Sorabji (1969) used the term
memory for the elementary memory unit and recollection for reasoning by associations between elementary units. Aristotle dis-

Figure 2. A simple model of associative computation. Elementary memories (“sense images,” according to Aristotle) are associatively linked (arrows) to other sense images. Branches are possible, and they present some
difficulties. There are many possible paths through the network. Forming
and traversing links between elementary memories is the basis of mental
computation.

Associative Networks
was that Skinner was proposing a well-defined computing machine
with his associative model and that this computing machine was
not powerful enough to do the computations we know language
users perform. The simple S r R models of Skinner had about as
much computing power as the simplest heteroassociative neural
networks, which no one claimed were general-purpose computers.
However, supervised network learning algorithms applied without
insight may produce systems with only this degree of overall computational power.

“Connectionist” Models
Much modern work using association assumes that the entities
linked, and the links themselves, can have complex internal structure. Flexible systems capable of complex reasoning can be produced by using labeled links: for example, a robin IS-A bird, an
IS-A link, or “Fred is the father of Herb,” meaning that there is an
associative link between Fred and Herb and that the link carries
the relationship “Father-of.” Complex and sophisticated computational models, semantic neworks, can be built from these pieces.
In the 1980s, many of those interested in semantic network models started working with neural networks. The term connectionism
was often used to indicate the application of neural networks to
high-level cognition. Recently there have been many attempts to
apply networks to reasoning, to complex concept structures, and,
in particular, to language understanding. A heated but illuminating
debate arose from an early connectionist paper by Rumelhart and
McClelland (1986) that used a neural network to simulate the way
young children learn past tenses of verbs. Past tense learning had
always been considered to be a good example of the application
and misapplication of a specific rule, suggesting symbolic processing. Rumelhart and McClelland’s neural network acted as if it
were using rules, but the rule-like behavior was the result of generalizing from examples and learning specific cases (see PAST
TENSE LEARNING). Perhaps because this model was such a direct
attack on the existence of rules in language, a vigorous counterattack developed. As one example, a long paper by Pinker and Prince
(1988) finished its abstract with the sentence, “We conclude that
connectionists’ claims about the dispensability of rules in explanations in the psychology of language must be rejected, and that,
on the contrary, the linguistic and developmental facts provide
good evidence for such rules” (p. 74). The vigor of the attack is
perhaps due in part to the authors’ feeling that the connectionists
had violated the “central dogma of modern cognitive science,
namely that intelligence is the result of processing symbolic expressions” (Pinker and Prince, p. 74). Many other cognitive scientists feel that the “central dogma” is actually more like a central,
and open, question.
Less well known outside psychology are several associative neural network models that were constructed to explain the fine structure of experimental data in more traditional areas of psychology
such as verbal learning. An interesting example of such a model is
the TODAM model of Murdock (see CLASSICAL LEARNING THEORY AND NEURAL NETWORKS in the first edition). TODAM and
variants blur the distinction between the network and the representation. In the associative networks we have discussed, there are two
formally distinct entities, state vectors and connection matrices. In
the TODAM class of models, the association is stored with the
items themselves and is therefore the same type of entity. TODAM
makes a number of testable qualitative predictions about a wide
range of data from the classical verbal learning literature. Recently,
models assuming networks composed of large numbers of local
networks (a “network of networks”) suggest that networks like
TODAM might be realizable with neural networks.

121

Discussion and Open Questions
An often proclaimed virtue of neural networks is their ability to
generalize effectively and to do computation based on similarity.
Having learned example associations from a training set, the network can then generate correct answers to new examples. Many
have pointed out the formal similarity of neural networks to approximation and interpolation as studied in numerical analysis. A
properly designed neural network can act as a useful adaptive interpolator with good, even optimal, generalization around the region of the learned examples. However, it is not easy for neural
networks to make good generalizations other than by approximation and interpolation. On this basis, Fodor and Pylyshyn (1988)
made some telling arguments against the promiscuous application
of connectionism to cognition (see SYSTEMATICITY OF GENERALIZATIONS IN CONNECTIONIST NETWORKS). The essential criticism
they made is one that an engineer would be happy to make: Associative neural networks are such an inefficient way to compute
that it would be foolish to build a cognitive system like that. Neural
networks do not generalize well outside of a restricted definition
based on mathematical interpolation, they cannot reason effectively, and they cannot extrapolate in any meaningful sense. These
criticisms are part of a battle involving the limitations of association that has been going on for centuries. Fodor and Pylyshyn commented, “It’s an instructive paradox that the current attempt to be
thoroughly modern and ‘take the brain seriously’ should lead to a
psychology not readily distinguishable from the worst of Hume
and Berkeley” (p. 64).
Fodor and Pylyshyn contrasted neural network associators with
what they call the classical view of mental operation. In essence,
this view postulates “a language of thought”; that is, “mental representations have a combinatorial syntax and semantics” (p. 12).
The classical view is dominant in virtually all branches of traditional artificial intelligence and linguistics. The power of the digital
computer arises in part from the fact that it is designed to be an
extreme example of this organization: a programming language
operating on data is the prototype of the classical view.
Suppose we have a sentence of the form A and B that we hold
is true. An example Fodor and Pylyshyn used is John went to the
store and Mary went to the store. The truth of this sentence logically entails the truth of Mary went to the store. This conclusion
arises from the rules of logic and of grammar. It is not easy for an
associative neural network to handle this problem. Such a network
could easily learn that John went to the store and Mary went to the
store is associated with Mary went to the store. But the power of
the classical approach arises from the fact that every sentence
of this form gives rise to the same result. Given the huge number
of possible sentences, it makes practical sense to assume that some
kind of logical syntax exists. It would be hard to figure out how
language could function without some global rule-like operations,
however implemented.
The ability to understand and answer sentences or phrases that
are new to the listener is hard to explain purely with association.
To give one example (see MENTAL ARITHMETIC USING NEURAL
NETWORKS in the first edition), consider number comparisons such
as “Is 7 bigger than 5?” There are nearly 100 such single-digit
comparisons, nearly 10,000 two-digit comparisons, and so on.
Children cannot possibly learn them as individual cases.
If there is a qualitative difference between human and animal
cognition, it lies right here. There have been attempts to build neural networks that realize parts of the classical account, with indifferent success (see Hinton, 1991). Is it possible to build a neural
network based largely on natural associators that can reproduce the
kind of rule-governed behavior—even in limited domains—that
does in fact seem to be part of human cognition? A neural network
with this ability would allow for much more powerful and useful

122

Part III: Articles

generalization than current networks provide. It may not be easy
to find this solution. There are many animals with complex nervous
systems capable of associative learning, but only our own species,
one out of millions of species, is really effective at using these
powerful extensions to association.
[Reprinted from the First Edition]
Road Maps: Grounding Models of Networks; Learning in Artificial
Networks
Background: I.3. Dynamics and Adaptation in Neural Networks
Related Reading: Artificial Intelligence and Neural Networks; Computing
with Attractors

References
Amit, D. J., 1989, Modelling Brain Function: The World of Attractor Neural Networks, Cambridge, Engl.: Cambridge University Press.
Anderson, J. A., 1995, Introduction to Neural Networks, Cambridge, MA:
MIT Press. ⽧
Anderson, J. R., 1983, The Architecture of Cognition, Cambridge, MA:
Harvard University Press.
Chomsky, N., 1957, A review of Skinner’s Verbal Behavior, Language,
35:26–58.

Fodor, J. A., and Pylyshyn, Z. W., 1988, Connectionism and cognitive
architecture: A critical analysis, in Connections and Symbols (S. Pinker
and J. Mehler, Eds.), Cambridge, MA: MIT Press.
Hebb, D. O., 1949, The Organization of Behavior, New York: Wiley.
Hertz, J., Krogh, A., and Palmer, R. G., 1991, Introduction to the Theory
of Neural Computation, Redwood City, CA: Addison-Wesley. ⽧
Hinton, G. E., 1991, Connectionist Symbol Processing, Cambridge, MA:
MIT Press. ⽧
Kohonen, T., 1977, Associative Memory: A System Theoretic Approach,
Berlin: Springer-Verlag. ⽧
Kohonen, T., 1984, Self-Organization and Associative Memory, Berlin:
Springer-Verlag. ⽧
Kosko, B., 1988, Bidirectional associative memories, IEEE Trans. Sys.,
Man Cybern., 18:49–60.
Pinker, S., and Prince, A., 1988, On language and connectionism: Analysis
of a parallel distributed processing model of language acquisition, in
Connections and Symbols (S. Pinker and J. Mehler, Eds.), Cambridge,
MA: MIT Press.
Rumelhart, D. E., and McClelland, J. L., 1986, On learning the past tenses
of English verbs, in Parallel Distributed Processing: Explorations in the
Microstructure of Cognition (D. E. Rumelhart, J. L. McClelland, and
PDP Research Group, Eds.), vol. 2, Psychological and Biological Models, Cambridge, MA: MIT Press.
Sorabji, R., 1969, Aristotle on Memory, Providence, RI: Brown University
Press.
Willshaw, D. J., Buneman, O. P., and Longuet-Higgins, H. C., 1969, Nonholographic associative memory, Nature, 222:960–962.

Auditory Cortex
Shihab A. Shamma
Introduction
The auditory cortex plays a critical role in the perception and localization of complex sounds. It is the last station in a long chain
of processing centers that begins with the cochlea of the inner ear
and passes through the cochlear nuclei (CN), the superior olivary
complex (SOC), the lateral lemniscus, the inferior colliculus (IC),
and the medial geniculate body (MGB) (Figure 1). Recent studies
have expanded our knowledge of the neuroanatomical structure,
the subdivisions, and the connectivities of all central auditory
stages (Winer, 1992). However, apart from the midbrain cochlear

Figure 1. Schematic representation of the multiple stages of processing in
the mammalian auditory pathway. Sound is analyzed in the cochlea, and
an estimate of the acoustic spectrum (an auditory spectrum) is known to be
extracted at the cochlear nucleus (Blackburn and Sachs, 1990). The tono-

and binaural SOC nuclei, relatively little is known about the functional organization of the central auditory system, especially compared to the visual and motor systems. Consequently, modeling
cortical auditory networks is complicated by uncertainty about exactly what the cortical machinery is trying to accomplish.
One exception to this state of affairs is the highly specialized
echolocating bat, in which these uncertainties are much relieved by
the existence of a stereotypical behavioral repertoire that is closely
linked to the animal’s acoustic environment (see ECHOLOCATION:
COCHLEOTOPIC AND COMPUTATIONAL MAPS). This has made it
possible to construct a functional map of the auditory cortex, which

topic organization of the cochlea is preserved all the way up to the cortex,
where it has a two-dimensional layout. The isofrequency plane encodes
perhaps other features of the stimulus.

Auditory Cortex
revealed the specific acoustic features extracted and represented in
the cortex. In turn, these cortical maps have acted as a guide to
discovering the organization and nature of the transformations occurring in lower auditory centers such as the MGB, IC, and SOC.
Thus, it has become meaningful in these species to investigate and
model cortical and other central auditory neural networks.
In other mammals, it is more difficult to isolate an auditory behavior and its associated stimulus features with comparable specificity. Nevertheless, a few tasks have been broadly accepted as
vital for all species, such as sound localization, timbre recognition,
and pitch perception. For each, evidence of various functional and
stimulus feature maps has been found or postulated, a significant
number of them in the last few years. In this review, we elaborate
on a few examples of such maps and relate them to the more intuitive and better understood case of the echolocating bats. In each
example, our goal is to determine how and whether models of the
underlying neural networks can further our understanding of the
auditory cortex.

Parcellization and Neuroanatomy
of the Auditory Cortex
The layout and neural structure of the auditory cortex is in many
respects similar to that of other sensory cortices (Winer, 1992). For
instance, based on cytoarchitectonic criteria and patterns of connectivity, it is subdivided into a primary auditory field (AI) and
several other surrounding fields, e.g., the anterior auditory field (A)
and the secondary auditory cortex (AII). The number and specific
arrangement of surrounding fields vary among different species,
reflecting presumably the complexity of the animal’s acoustic environment. The AI, and possibly other fields, is further subdivided
into smaller regions, serving perhaps different functional roles,
such as echo delay and amplitude measurements in the bat (see
ECHOLOCATION: COCHLEOTOPIC AND COMPUTATIONAL MAPS).
The anatomical parcellization of the auditory cortex into different fields is mirrored by physiologically based divisions. Most important is the systematic frequency organization in different fields,
or so-called tonotopic maps. For example, AI cells are spatially
ordered based on the tone frequency to which they best respond,
i.e., their best frequency (BF). They also respond vigorously to the
onset of a tone and exhibit little evidence of adaptation to its repeated presentations. In other fields, cells may be less frequency
selective, may respond more adaptively, or may be totally unresponsive to single tones, preferring more spectrally or temporally
complex stimuli. A sudden change in these response patterns or in
the gradual spatial order of the tonotopic map is usually taken to
signify a border between different fields. In the cat, which has the
most extensively mapped auditory cortex, four well-ordered tonotopic fields have been described, together with many other less
precise secondary areas (Clarey, Barone, and Imig, 1992).

123

An important organizational feature of the central auditory system is the expansion of the 1D tonotopic axis of the cochlea into
a 2D sheet, with each frequency represented by an entire sheet of
cells (Figure 1). An immediate question thus arises as to the functional purpose of this expansion and the nature of the acoustic
features that might be mapped along these isofrequency planes. For
example, one might conjecture that the amplitude or the local shape
of the spectrum is explicitly represented along this new dimension.
In general, there are two ways in which the spectral profile can
be encoded in the central auditory system. The first is absolute,
that is, the spectral profile is encoded in terms of the absolute intensity of sound at each frequency. Such an encoding would in
effect combine both the shape information and the overall level.
The second way is relative, in which the spectral profile shape is
encoded separately from the overall loudness of the stimulus. Examples of each of these two hypotheses are discussed next.

The Best-Intensity Model
The first hypothesis is motivated primarily by the strongly nonmonotonic responses as a function of stimulus intensity observed
in many cortical and other central auditory cells (Clarey et al.,
1992). In a sense, one can view such a cell’s response as being
selective to (or encoding) a particular intensity. Consequently, a
population of such cells, tuned to different frequencies and intensities, can provide an explicit representation of the spectral profile
by their spatial pattern of activity (Figure 2). This scheme is not a
true transformation of the spectral features represented, but rather
is strictly a change in the means of the representation. The most
compelling example of such a representation is that in the DSCF
area of AI in the mustache bat. However, an extension of this hypothesis to multicomponent stimuli (as depicted in Figure 2) has
not been demonstrated in any species.

The Multiresolution Analysis Model
The second hypothesis, in which the relative shape of the spectrum
is encoded, is supported by physiological experiments in cat and

Timbre: Models for the Encoding of Spectral Profiles
Recognizing and classifying environmental sounds is critical for
the survival and propagation of many animals. Although a multitude of cues are responsible, the single most important one is the
shape of the so-called spectral envelope (or the spectral profile) of
the sound. It is largely this cue that allows us to distinguish between
speech vowels or between different instruments playing the same
note. The spectral profile emerges early in the auditory system as
the sound is analyzed into different frequency bands, in effect distributing its energy across the tonotopic axis (the auditory sensory
epithelium) (Figure 1). As far as the central auditory system is
concerned, the spectral profile is a one-dimensional (1D) pattern of
activation analogous to the two-dimensional (2D) distribution of
light intensity on the retina.

Figure 2. Schematic diagram of the way in which the spectral profile (lower
plot) can be encoded by arrays of nonmonotonic cells (circles) tuned to
different BFs (along the tonotopic axis) and best intensifies (BIs). The black
circles signify strongly activated cells, whereas the white circles indicate
weakly activated cells. Thus, a peak in the input pattern located at a given
BF and at an intensity of 40 dB would best activate cells with the same BF
and BI

124

Part III: Articles

ferret AI, coupled with psychoacoustical studies in human subjects.
The data reveal a substantial transformation of the way the spectral
profile is represented centrally. Specifically, besides the tontopic
axis, two features of the response areas of AI neurons (the analogue
of the receptive fields in the visual system) are found to be topographically mapped across the isofrequency planes. They are the
bandwidth and symmetry of the response areas, depicted schematically in Figure 3A as the scale and symmetry axes, respectively.
In addition, auditory cortical units exhibit systematic response patterns to dynamic spectra that give rise to complex and varied spectrotemporal response areas, as depicted in Figure 3B. These response properties are discussed in greater detail below.
Changes in response area bandwidths. Cell response areas, i.e.,
the excitatory and inhibitory responses they exhibit to a tone of
various frequencies and intensities, change their bandwidth in
orderly fashion along the isofrequency planes (Mendelson and
Schneiner, 1990). Near the center of AI, cells are narrowly tuned.
Toward the edges, they become more broadly tuned. This orderly
progression occurs at least twice, and it correlates with several other
response parameters such as increasing response thresholds toward
the edges.
An intuitively appealing implication of this finding is that response areas of different bandwidths are selective to spectral profiles of different widths. Thus, broad spectral profiles (e.g., broad
peaks or gross trends, such as spectral tilts due to preemphasis)
would best drive cells with wide response areas. Similarly, narrower spectral profiles (e.g., sharp peaks or edges, or fine details
of the spectral profile) would best be represented in the responses
of cells with more compact response areas. In effect, having a range
of response areas at different widths allows us to encode the spectral profile at different scales or levels of detail (resolution). From
a mathematical perspective, this is basically equivalent to analyzing
the spectral profile into different scales or “bands,” much like performing a Fourier transform of the profile, hence representing it as
a weighted sum of elementary sinusoidal spectra (usually known
as ripples; Shamma, Versnel, and Kowalski, 1995). Coarser scales
then correspond to the “low-frequency” ripples, while finer scales
correspond to the “high-frequency” ripples.
Changes in response area asymmetry. Response areas exhibit
systematic changes in the symmetry of their inhibitory response
areas. For instance, cells in the center of AI have sharply tuned
excitatory responses around a BF, flanked by symmetric inhibitory
response areas. Toward the edges, the inhibitory response areas
become significantly more asymmetric, with inhibition dominated
by either higher or lower than BF frequencies. This trend is repeated at least twice across the length of the isofrequency plane.
It is intuitively clear that response areas with different symmetries would respond best to input profiles that match their symmetry. For instance, an odd-symmetric response area would respond best if the input profile had the same local odd-symmetry
and worst if it had the opposite odd-symmetry. As such, one can
state that a range of response areas of different symmetries (symmetry axis in Figure 3A) is capable of encoding the shape of a local
region in the profile. From an opposite perspective, it can be shown
mathematically that the local symmetry of a pattern can be changed
by manipulating only the phase of its Fourier transform (Wang and
Shamma, 1995). Therefore, the axis of response area asymmetries
in effect is able to encode the phase of the profile transform, thus
providing a complementary description to that of the magnitude
along the scale axis described above.
Dynamics of cortical responses to spectral profile changes.
Auditory cortical units also exhibit systematic and selective responses to dynamic spectra. Specifically, when stimulated by com-

Figure 3. A, Schematic diagram of the three representational axes thought
to exist in AI: the tonotopic (BF) axis, the scale (or bandwidth) axis, and
the symmetry axis. B, Examples of spectrotemporal response fields measured from two auditory cortical units of the ferret. In each panel, the
strength of the response is represented by the darkness of the display, with
black indicating excitatory areas and white indicating regions of suppressed
activity. Note that the excitatory central region defines the BF of the unit.
Such STRFs exhibit a variety of bandwidths, asymmetry of inhibition relative to the BF, directional selectivity, and temporal dynamics. For instance,
the unit in the top panel has significantly slower dynamics and much more
asymmetric inhibition about the BF than the unit in the bottom panel. (From
Simon, J. Z., Depireux, D. A., and Shamma, S. A., 1998, Representation
of complex spectrain auditory cortex, in Psychophysical and Physiological
Advances in Hearing: Proceedings of the 11th International Symposium on
Hearing (A. R. Palmer, A. Ress, A. Q. Summerfield, and R. Meddis Eds.),
London: Whurr, 1998, pp. 513–520. Reprinted with permission.)

Auditory Cortex
plex sounds with rippled spectra like those described above, cortical units display preference not only to ripple density and phase,
but also to the velocity at which the ripple is drifted past the BF
of the cell. Unit selectivities span a wide range of best ripple velocities, from about 20 cycles/s (Hz), down to as low as 1–2 Hz
(Kowalski, Depireux, and Shamma, 1996). In addition, auditory
cortical units usually exhibit a range of directional sensitivities to
upward- and downward-moving ripples.
This directional selectivity is probably directly linked to responses to frequency-modulated (FM) tones, a subject that has been
the focus of extensive neural network modeling. These stimuli are
important because they mimic the dynamic aspects of many natural
vocalizations, as in speech consonant-vowel combinations or the
trills of many birds and other animal sounds. The effects of manipulating two specific parameters of the FM sweep, its direction
and rate, have been well studied. In several species and at almost
all central auditory stages, cells can be found that are selectively
sensitive to the FM direction and rate. Most studies have confirmed
a qualitative theory in which directional selectivity arises from an
asymmetric pattern of inhibition in the response area of the cell

Figure 4. Schematic of the cortical representation of complex dynamic sound
spectra. A, The time waveform of the
acoustic signal /Come home right away/.
B, The time-frequency representation of
the signal (or the auditory spectogram)
generated in the early stages of the auditory system. The y-axis represents the
logarithmic frequency axis of the cochlea
(or the tonotopic axis, as depicted in Figure 1). C, Cortical multiscale analysis of
the auditory spectrogram along the spectral and temporal dimensions. Each panel
represents the activity of a population of
cortical cells with the (idealized model)
STRF shown in the inset above it. Arrow
direction represent the phase of the response; the strength of the response is indicated by the darkness of the display, as
in Figure 3B. The two top panels are for
broadly tuned but relatively fast STRFs
that are selective to motion in opposite
directions. The bottom panels are for narrowly tuned and relatively slow units.
Different features of the spectrogram are
emphasized in different panels. For instance, harmonics (pitch cues) are seen in
the lower (fine-scale) panels, whereas onsets due to different consonents are seen
only in the upper (fast-rate) panels.

125

(Wang and Shamma, 1995), whereas rate sensitivity is correlated
to the bandwidth of the response area (Heil, Langner, and Scheich,
1992).
The full spectrotemporal response fields. All of above mentioned
response area features are integrated into a unified spectrotemporal
response area (or field) as illustrated in Figure 3B (deCharms,
Blake, and Merzenich, 1998). The full spectrotemporal response
field (STRF) summarizes all the response selectivities of a unit by
the relative locations, widths, duration, and orientation of its excitatory and inhibitory fields. The overall picture that emerges from
these findings is that AI decomposes the auditory spectrum into a
multidimensional representation with multiple resolutions along
both spectral and temporal dimensions, as illustrated in Figure 4.
This spectrotemporal decomposition essentially segregates diverse
perceptual features into different streams, e.g., fast, spectrally broad
sounds (consonants) from the relatively slow, voiced vowels and
the finely resolved harmonics (pitch cues) (Wang and Shamma,
1995). This kind of multiscale analysis is closely analogous to the
well-studied organization of receptive fields in the primary visual

126

Part III: Articles

cortex (De-Valois and De-Volois, 1990), and may reflect a general
principle of analysis of sensory patterns in all other sensoricortical
areas.

Models of Pitch Representation in the Central
Auditory System
A sound complex consisting of several harmonics is heard with a
strong pitch at the fundamental frequency of the harmonic series,
even if there is no energy at all at that frequency. This percept has
been variously called the missing fundamental, virtual pitch, or
residue pitch. A large number of psychoacoustical experiments
have been carried out to elucidate the nature of this percept and its
relationship to the physical parameters of the stimulus. Basically,
all models fall into one of two camps. In the first camp, the pitch
is extracted explicitly from the harmonic spectral pattern. This can
be accomplished in a variety of ways, such as by finding the best
match between the input pattern and various harmonic templates
assumed to be stored in the brain (Goldstein, 1973). In the second
camp, the pitch is extracted from the periodicities in the timewaveform of responses in the auditory pathway, which can be estimated, for example, by computing their autocorrelation functions.
In this kind of model, some form of organized delay lines are assumed to exist so that the computations can be done, much like
those that seem to exist in the FM-FM area of the mustached bat.
In all pitch models, however, the extracted pitch is assumed to
be finally represented as a spatial map in higher auditory centers.
This is because many studies have confirmed that neural synchrony
to the repetitive features of a stimulus, be it the waveform of a tone
or its AM modulations, becomes progressively worse toward the
cortex (Langner, 1992). It is a remarkable aspect of pitch that,
despite its fundamental and ubiquitous role in auditory perception,
only a few reports exist of physiological evidence of spatial pitch
maps, and none has been independently confirmed. One source is
NMR scans of the primary auditory cortex in human subjects. The
other source of evidence is multiunit mappings in various central
auditory structures (Schreiner and Langner, 1988).
Of course, the difficulty of finding spatial pitch maps in the auditory cortex may be due to the fact that it does not exist. This
possibility is counterintuitive, given the results of ablation studies
showing that bilateral cortical lesions in the auditory cortex severely impair the perception of pitch of complex sounds but do not
affect the fine discrimination of frequency and intensity of simple
tones. Another possibility is that the maps sought are not at all as
straightforward as we imagine. For example, harmonic complexes
may evoke stereotypical patterns that are distributed over large areas in the auditory cortex, and not localized, as the simple notion
of a pitch map implies (Wang and Shamma, 1995). Finally, it is
also possible that AI simply functions as one stage that projects
sufficient temporal or spectral cues for later cortical stages to extract the pitch explicitly.

Models of Sound Localization
It has been recognized for many years that the auditory cortex (and
especially the AI) is involved in sound localization. Detailed physiological studies further confirmed that AI cells are rather sensitive
to all kinds of manipulations of the binaural stimulus (Clarey et
al., 1992). For instance, changing either of the two most important
binaural cues, the interaural level difference (ILD) or interaural
time difference (ITD), causes substantial changes in their firing rate
patterns. This sensitivity to interaural cues has its origins early in
the auditory pathway, at the SOC, where the first convergence of
binaural inputs occurs. However, despite this diversity, two elements typical of a functional organization of AI have been lacking.
The first missing element is a significant transformation of the
single-unit responses. For example, if ILD-sensitive cells are to

encode the location of a sound source based on this cue, they ought
to become uniformly more stable with overall sound intensity. This,
however, does not seem to be the case (Semple and Kitzes, 1993).
The second element lacking is a topographical distribution of the
responses with respect to these cues or to a more complex combination of features (e.g., a map of acoustic space derived from
ILD and ITD cues, as in the barn owl) (Sullivan and Konishi, 1986).
A map of auditory space has indeed been found in the superior
colliculus of several mammals. No such map, however, has yet
been detected in AI or other cortical fields despite intensive efforts
(Clarey et al., 1992). What has been found, however, is a topographic order of certain binaural responses along the isofrequency
planes of AI. Specifically, cells excited equally well by sounds from
both ears (called EE cells) and others inhibited by ipsilateral sounds
(called EI cells) are found clustered in alternating bands that parallel the tonotopic axis. One possible functional model that utilizes
such maps assumes that EI cells are tuned to particular ILDs, and
hence encode the location of a sound source based on this cue. EE
cells, in contrast, would encode the absolute level of the sound.
However, there is little evidence to support this hypothesis in the
sense that neither EE nor EI cells are particularly stable encoders
of specific ILD or absolute sound levels. An alternative hypothesis
recently proposed is that these cells encode the absolute levels of
the stimulus at each ear, rather than the difference and average
binaural levels, as previously postulated (Semple and Kitzes,
1993). Finally, it has also been proposed that AI units encode the
spatial location of a stimulus through unique patterns of temporal
firing, ones that can be discerned using more elaborate pattern recognition neural networks (Middlebrooks et al., 1994).

Discussion
The study of central auditory function has reached a sufficiently
advanced stage to allow meaningful quantitative and neuronal network models to be formulated. In most mammals, these models are
still systemic in nature, with a primary focus on understanding the
overall functional organization of the cortex and other central auditory structures. In the bat and other specialized animals, the models are somewhat more detailed, addressing specific neuronal mechanisms, such as the coincidences and the delay lines of the FM-FM
areas. The auditory system, with its multitude of diverse functions
and its combination of temporal and spatial processes, should thus
prove to be a valuable window into the brain and an effective vehicle for understanding the brain’s underlying mechanisms.
Road Maps: Mammalian Brain Regions; Other Sensory Systems
Related Reading: Auditory Periphery and Cochlear Nucleus; Auditory
Scene Analysis; Echolocation: Cochleotopic and Computational Maps;
Sound Localization and Binaural Processing

References
Blackburn, C. C., and Sachs, M. B., 1990, The representations of the steadystate vowel sound phoneme e in the discharge patterns of cat anteroventral cochlear nucleus neurons, J. Neurophysiol., 63(5):1191–1212.
Clarey, J., Barone, P., and Imig, T., 1992, Physiology of thalamus and
cortex, in The Mammalian Auditory Pathway: Neurophysiology (R. Fay,
D. Webster, and A. Popper, Eds.), New York: Springer-Verlag, pp. 232–
334.
deCharms, R. C., Blake, D. T., and Merzenich, M. M., 1998, Optimizing
sound features for cortical neurons, Science, 280:1439. ⽧
De-Valois, R., and De-Valois, K., 1990, Spatial Vision, New York: Oxford
University Press.
Goldstein, J., 1973, An optimum processor theory for the central formation
of pitch of complex tones, J. Acoust. Soc. Am., 54:1496–1516.
Heil, P., Langner, G., and Scheich, H., 1992, Processing of FM stimuli in
the chick auditory cortex analogue: Evidence of topographic representations and possible mechanisms of rate and directional sensitivity, J.
Comp. Physiol. A, 171:583–600.
Kowalski, N., Depireux, D., and Shamma, S., 1996, Analysis of dynamic

Auditory Periphery and Cochlear Nucleus
spectra in ferret primary auditory cortex: Characteristics of single unit
responses to moving ripple spectra, J. Neurophysiol., 76:3503–3523.
Langner, G., 1992, Periodicity coding in the auditory system, Hearing Res.,
6:115–142.
Mendelson, J., and Schreiner, C., 1990, Functional topography of cat primary auditory cortex: Distribution of integrated excitation, J. Neurophysiol., 64:1442–1459.
Middlebrooks, J. C., Clock, A. E., Xu, L., and Green, D. M., 1994, A
panoramic code for sound location by cortical neurons, Science,
264:842–844.
Schreiner, C., and Langner, G., 1988, Periodicity coding in the inferior
colliculus of the cat: 2. Topographical organization, J. Neurophysiol.,
60:1823–1840.
Semple, M., and Kitzes, L., 1993, Binaural processing of sound pressure
level in cat primary auditory cortex: Evidence for a representation based

127

on absolute levels rather than level differences, J. Neurophysiol.,
69:449–461.
Shamma, S., Versnel, H., and Kowalski, N., 1995, Ripple analysis in the
ferret primary auditory cortex: 1. Response characteristics of single units
to sinusoidally rippled spectra, J. Aud. Neurosci., 1:233–254.
Sullivan, W., and Konishi, M., 1986, Neural map of interaural phase difference in the owl’s brainstem, Proc Natl Acad. Sci. USA, 83:8400–8404.
Winer, J., 1992, The functional architecture of the medial geniculate body
and primary auditory cortex, in The Mammalian Auditory Pathway: Neuroanatomy (D. Webster, A. Popper, and R. Fay, Eds.), New York:
Springer-Verlag, pp. 232–334.
Wang, K., and Shamma, S., 1995, Representation of spectral profiles in
primary auditory cortex, IEEE Trans. Speech Audio Process., 3:382–
395.

Auditory Periphery and Cochlear Nucleus
David C. Mountain
Introduction
The auditory periphery transforms a very high information rate
acoustic signal into a group of lower information rate neural signals. This process of parallelization is essential because the potential information rate in the acoustic stimulus is on the order of 0.5
megabits per second, and yet typical auditory nerve (AN) fibers
have maximum sustained firing rates of 200 per second. The cochlear nucleus (CN) continues the process of parallelization by creating multiple representations of the original acoustic stimulus,
with each representation emphasizing different acoustic features.
The major ascending auditory pathways are summarized in Figure 1. Sound is collected by the external ear (pinna) and passes
through the ear canal to the eardrum (tympanic membrane), where
it excites the middle ear. The middle ear couples the acoustic energy to the fluids of the cochlea, where transduction takes place.
The sensory cells of the cochlea (hair cells) convert the mechanical
signal to an electrical signal, which is then encoded by the fibers
of the auditory nerve and transmitted to the CN in the brainstem.
Within the CN, parallel information streams are created that feed
other brainstem structures such as the superior olivary complex
(SOC), the nuclei of the lateral lemniscus (NLL), and the inferior
colliculus (IC). These parallel pathways are believed to be specialized for the processing of different auditory features that are
used for sound source classification and localization. From the IC,
auditory information is passed on to the medial geniculate body
(MGB) in the thalamus, and from there to the auditory cortex.

sound waves from some directions but not others, resulting in an
HRTF with peaks and valleys that change with sound source
direction.

Middle Ear and Cochlear Mechanics
The middle ear consists of the tympanic membrane, the three
middle-ear bones (ossicles), and the Eustachian tube. The primary
function of the middle ear is to match the low acoustic impedance
of air to the high acoustic input impedance of the cochlea. The
middle-ear transfer function (ratio of intracochlear pressure to ear
canal pressure) is high-pass in nature (Rosowski in Hawkins et al.,

External Ear
The head and pinna modify the magnitude and phase of the acoustic
signal reaching the tympanic membrane in such a way as to provide
important cues for sound source localization (Shaw in Gilkey and
Anderson, 1997). The transfer function relating tympanic membrane pressure to pressure in the free field is called the head-related
transfer function (HRTF) and changes with sound source elevation
and azimuth.
Three major mechanisms contribute to the creation of the HRTF.
The distance between the ears in most mammals is sufficient to
create significant interaural time delays (ITDs) for sound sources
off to the side of the head, and the head is large enough to create
interaural level differences (ILDs) for frequencies where the wavelength is comparable or smaller than the head. For higher frequencies (above 5 kHz in humans), multiple resonant modes in the pinna
add further complexity. These modes are preferentially excited by

Figure 1. The major ascending auditory pathways. See text for explanation
of abbreviations.

128

Part III: Articles

1996) and plays a major role in determining the audiogram for a
given species.
The cochlea consists of a spiral-shaped, fluid-filled tube embedded in the temporal bone (Slepecky in Dallos, Popper, and Fay,
1996). It is separated into three longitudinal compartments by two
membranes: the basilar membrane (BM) and Reissner’s membrane.
From a hydromechanical and physiological point of view, the BM
is the more important of the two. It supports the organ of Corti,
which contains the sensory hair cells. Pressure changes in the cochlear fluids produced by the middle ear excite a mechanical traveling wave that propagates along the BM. The traveling wave magnitude peaks at a location that depends on stimulus frequency: high
frequencies peak near the base and low frequencies peak near the
apex.
Direct measurements of BM motion demonstrate that, at low
sound levels, the response can be highly tuned, with each cochlear
location only responding to a narrow range of frequencies (Hubbard and Mountain in Hawkins et al., 1996). BM tuning decreases
at high sound levels and appears to involve the presence of a group
of sensory cells, the outer hair cells (OHCs). All hair cells respond
to mechanical stimuli with voltage changes, but in the case of the
OHCs, voltage changes result in cell length changes (Holley in
Dallos et al., 1996). These voltage-dependent length changes appear to be mediated by voltage-sensitive transmembrane proteins.
This novel form of electromotility is piezoelectric in nature, allowing the length changes to achieve very high velocities.
Many hydromechanical models have been proposed to explain
these findings (Hubbard and Mountain in Hawkins et al., 1996; de
Boer in Dallos et al., 1996), but these hydromechanical models are
computationally intense. As a result, it is common practice to represent cochlear mechanics with a bank of digital bandpass filters
that capture the salient features of the mechanical frequency response (Hubbard and Mountain in Hawkins et al., 1996). Filters of
this type reproduce the magnitude of the cochlear frequency response reasonably well, but they cannot reproduce the changes in
cochlear tuning that occur with changes in stimulus level. In order
to replicate the nonlinear features of cochlear mechanics in filterbank models, some authors have used filters with parameters that
change with stimulus level (cf. Zhang et al., 2001).

membrane acts as a low-pass filter with a cutoff frequency of
around 1 kHz. The effect of this filter is to produce an IHC response
that follows the fine structure of the stimulus waveform at low
frequencies, while at high frequencies it follows the signal envelope
(Mountain and Hubbard in Hawkins et al., 1996).
If a linear filter bank is used to represent cochlear mechanics,
then it is often desirable to use a rectification function that includes
considerable compression to accommodate the large dynamic range
of many acoustic signals. Since the D.C. receptor potentials of IHCs
measured using best-frequency tones appear to grow as a logarithmic function of sound pressure, a combination of a half-wave rectifier followed by a logarithmic compressor provides a reasonable
model (Mountain and Hubbard in Hawkins et al., 1996).

Auditory Nerve
AN fibers, the cell bodies of which are located in the SG, are divided into two classes, depending on their morphology. Each IHC
synapses with 10 to 30 type I AN (AN-I) fibers (Ryugo in Webster,
Popper, and Fay, 1992). In most mammals, AN-I fibers synapse
only with a single IHC. In contrast, type II fibers (AN-II), which
innervate the OHCs, synapse with multiple hair cells. AN-I fibers
exhibit spontaneous activity in the absence of sound, and they are
often segregated into low (LSR), medium (MSR), and high (HSR)
spontaneous rate categories. The pattern of this spontaneous activity is random and is usually modeled as a Poisson or dead-time
modified Poisson process. Spontaneous rate tends to correlate with
threshold, with HSR fibers being the most sensitive to sound
stimuli.
The average firing rate of AN fibers in response to sustained
tones is tuned, mimicking the responses at the BM. The peristimulus time histogram (PSTH) exhibits an initial rapid increase,
followed by adaptation (Figure 2) to a lower steady-state rate (Ruggero in Popper and Fay, 1992). The steady-state response has only

Inner Hair Cells
The inner hair cells (IHCs) are the receptor cells that provide most
of the input to the auditory nerve. Although much progress has
been made in measuring basilar membrane motion, little direct data
exist to explain how this motion gets coupled to the IHC hair bundle. Comparisons of IHC receptor potentials to inferred BM motion
have led to the hypothesis that hair-bundle motion is a high-pass
filtered version (cutoff frequency 400 Hz) of BM motion. Alternatively, Mountain and Cody (1999) have proposed a model in
which the OHCs, through their electromotility, displace the IHC
hair bundles more directly, perhaps via movements of the tectorial
membrane, rather than via enhanced BM motion.
The mechanical-to-electrical transduction process in hair cells is
extremely sensitive, resulting in receptor potentials on the order of
1 mV for hair-bundle displacements of 1 nm. This transduction
process is believed to be the result of tension-gated channels located in the hair bundle (Mountain and Hubbard in Hawkins et al.,
1996). The relationship between stereocilia displacement x and the
mechanically induced conductance change G(x) is most commonly
modeled using a first-order Boltzmann model (Mountain and Hubbard in Hawkins et al., 1996).
Although IHCs also contain voltage-dependent conductances
(Kros in Dallos et al., 1996), most models include only the mechanically sensitive conductance coupled to a linear leakage resistance and a linear membrane capacitance. The RC nature of the

Figure 2. Typical auditory nerve and cochlear nucleus peristimulus time
histograms. See text for explanation of abbreviations.

Auditory Periphery and Cochlear Nucleus
a limited dynamic range, typically saturating at sound levels of
approximately 20 dB above the fiber’s threshold. There are three
components to the adaptation. The fastest component, rapid adaptation, has a time constant of a few milliseconds and creates an
onset response with a large dynamic range. The second component,
short-term adaptation, has a time constant of a few tens of milliseconds. It creates a slower component immediately after the onset
response that has a smaller dynamic range, similar to that of the
steady-state response. The third component of adaptation operates
on a time scale of seconds and is not included in most auditory
models.
On a finer time scale, the instantaneous firing rate (IFR) of AN
fibers can be modulated on a cycle-by-cycle basis by the acoustic
stimulus (phase locking) up to about 4 kHz (Ruggero in Popper
and Fay, 1992). The fast dynamics of the AN IFR, coupled with
only modest frequency resolution, suggests that we should think of
the AN representation as that of a spectrogram that has been optimized more for temporal resolution than for spectral resolution.
This excellent temporal resolution plays an important role in soundsource localization, which relies heavily on cues from interaural
time delays.
Scant biophysical data are available for the IHC synapse, but
since adaptation is not observed in the IHC receptor potentials,
adaptation must be taking place in the IHC-AN synapse. The adaptation processes are most commonly assumed to be the result of
synaptic vesicle depletion. Synaptic vesicles are typically divided
into two or more pools. One of these pools represents vesicles that
are docked at the active zones and is often referred to as the releasable pool or the immediate pool. Additional vesicles, which are
located near the release sites but appear to be tethered to the cytoskeleton, are not available for immediate release (Mountain and
Hubbard in Hawkins et al., 1996).

Cochlear Nucleus Anatomy
The two CN are the first and only brainstem structures to receive
input from the AN. The CN can be anatomically subdivided into
several subdivisions, each of which appears to perform a different
physiological function. The major subdivisions are the ventral cochlear nucleus (VCN), which is further divided into anteroventral
(AVCN) and posteroventral (PVCN) subdivisions, and the DC nucleus (DCN). Fibers of the AN travel through the core of the cochlear spiral and enter the AVCN, where they branch. The ascending branch innervates the AVCN and the descending branch travels
through the PVCN and enters the DCN. The ventral regions are
surrounded by the marginal shell, which is made up of the smallcell cap (SCC) and the granule-cell layer (GCL). Within a subdivision, the low-frequency fibers project to more ventral regions and
the high-frequency fibers project to more dorsal regions. This orderly arrangement of characteristic frequencies is referred to as a
tonotopic projection.

Cochlear Nucleus Response Types
The most commonly used physiological classification scheme in
the CN is based on the PSTH. These histograms are derived by
averaging the responses to short tone bursts presented at the cell’s
characteristic frequency (Rhode and Greenberg in Popper and Fay,
1992). Figure 2 illustrates six of the most common PSTH types
found in the CN. The primary-like (PL) PSTHs are similar to the
PSTHs recorded from AN fibers. The primary-like with notch
(PLN) response type is similar to the PL type but with better synchrony to the stimulus onset, followed by a transient dip in response
due to refractory effects. The chopper-cell PSTHs exhibit periodically modulated activity at the beginning of the histogram, which
is the result of the regular firing pattern of these cells becoming

129

synchronized to the stimulus onset. This chopping effect can either
be sustained (CS) or transient (CT). The onset cell PSTHs all have
large responses to the stimulus onset, followed by reduced or nonexistent activity during the remainder of the stimulus. The onset
chopper (OC) PSTH exhibts a transient chopping response after the
onset, whereas the OL type (not shown) shows little or no response
after the onset response. Other PSTH types include the pauser (P)
and build-up (B) types. The P-type PSTH is characterized by an
onset response followed by a period of no activity, which is then
followed by a slow build-up of activity. The B-type (not shown) is
similar to the P-type but lacks the initial onset response. The different PSTH types are believed to be the result, in part, of differences in intrinsic membrane properties and different degrees of AN
fiber convergence and synaptic effectiveness.

Cochlear Nucleus Neural Circuits
Octopus Cells
Octopus cells are located in the PVCN and are characterized by
long, thick primary dendrites that usually arise from one side of
the cell body and give the cell the appearance of an octopus. The
dendrites of octopus cells are oriented perpendicular to the path of
incoming AN fibers (Oertel et al., 2000) which means that they
receive input from a range of characteristic frequencies. The lower
CF fibers synapse on the soma and the higher CF fibers synapse
on the dendrites. Octopus cells generally exhibit onset (OL) responses (Rhode and Greenberg in Popper and Fay, 1992) and
appear to be more sensitive to broadband stimuli than AN fibers
(Oertel et al., 2000). Functionally, octopus cells appear to act as
coincidence detectors that detect synchronous events across AN
fibers and may form part of networks involving other subthalamic
nuclei devoted to processing temporal features such as duration,
periodicity, and echo delay. The octopus cells project to contralateral VNLL terminating in calyx endings (Schwartz in Webster et
al., 1992). The secure nature of these terminals reinforces the notion that octopus cells play a role in temporal processing. VNLL
is primarily a monaural nucleus (Irvine in Popper and Fay, 1992)
that provides inhibitory input to the IC (Schwartz in Webster et al,
1992). The octopus cells also provide diffuse innervation to periolivary areas of the SOC (Schwartz in Webster et al., 1992).

Stellate Cells
The stellate cells (SCs) of the VCN are hypothesized to be part of
a system that uses a rate code to represent the acoustic spectrum
(Rhode and Greenberg in Popper and Fay, 1992). Stellate cells have
dendrites that extend away from the soma in all directions and often
divide to form secondary and tertiary dendrites. The SCs can be
divided into two classes, based on the path taken by their axons.
The T-stellate cells project out of the CN by way of the trapezoid
body (hence the name T), and the D-stellate cells are interneurons
with axons that follow a descending path (hence the name D) on
their way to the DCN and contralateral CN. The dendrites of Tstellate cells (also called planar cells) end in tufts and are generally
aligned with the isofrequency plane created by the path of AN
fibers, whereas those of D-stellate cells (also called radiate cells)
extend radially across the isofrequency planes and branch sparingly. Both T- and D-stellate cells have terminal collaterals in the
multipolar cell region of the PVCN and in the DCN (Oertel et al.,
1990; Doucet and Ryugo, 1997).
D-stellate cells exhibit OC responses (Figure 2), have a large
dynamic range (80 dB or more), and, as would be expected from
their dendritic morphology, are more broadly tuned than AN fibers.
In contrast, T-stellate cells exhibit CT responses (Figure 2) and have
frequency tuning characteristics similar to those of AN fibers. Stel-

130

Part III: Articles

late cells have a more regular firing pattern than AN fibers and do
not preserve timing information as well as the AN fibers do (Rhode
and Greenberg in Popper and Fay, 1992).
The basic stellate-cell circuit is illustrated in Figure 3A. The Dstellate cells receive excitatory input from AN-I fibers with a range
of CFs and inhibitory input that is believed to come from Golgi
cells, which in turn appear to receive their input from AN-II fibers
(Ferragamo, Golding, and Oertel, 1998). Golgi cells are small inhibitory interneurons located in the marginal shell with dendrites
that branch extensively near the cell body (Cant in Webster et al.,
1992). The Golgi cell axons branch even more extensively, forming
a plexus with thousands of endings in nearby regions. In contrast
to the D-stellate cells, the T-stellate cells receive excitatory input
from AN-I fibers with a narrow range of CFs, and they receive
inhibitory input from D-stellate cells as well as from vertical cells
located in the deep DCN (Ferragamo et al., 1998). The DCN vertical cells receive excitatory input from AN-I fibers with a narrow
range of CFs and are strongly inhibited by the more broadly tuned
D-stellate cells. The T-stellate cells are the output neurons of this
circuit; their axons project to the IC.

Bushy Cells
The major cell types of the AVCN are the spherical bushy cells
(SBCs) and the globular bushy cells (GBCs). The term bushy cell
refers to their appearance, with short primary dendrites that originate from one hemisphere of the cell body and give rise to a profusion of thin, shrub-like appendages. SBCs are believed to play
an important role in sound localization and are specialized to preserve timing information. The basic SBC circuit is shown in Figure
3B. SBCs receive their excitatory input from AN-I fibers tuned to
a narrow range of frequencies, and in most respects SBCs have
response properties, such as their primary-like PSTHs (Figure 2),
that are very similar to those of AN fibers. The AN fibers that
synapse on the SBC cell body give rise to very large synaptic end-

ings known as the endbulbs of Held (Cant in Webster et al., 1992).
These very secure synapses, in combination with a low-threshold
potassium conductance in the SBCs, enhance phase locking to the
acoustic stimulus (presumably through a coincidence mechanism)
to be even more precise than that found in the AN. SBCs receive
inhibitory input from vertical cells in the DCN. This inhibitory
input is delayed with respect to the excitatory input from the AN
and not as effective as the excitatory input, but it may play a role
in echo suppression (Rhode and Greenberg in Popper and Fay,
1992).
SBCs project to the ipsilateral and contralateral medial superior
olive (MSO), where timing differences (ITD) between the two ears
are processed, and the ipsilateral lateral superior olive (LSO),
where amplitude differences (ILD) are processed (Oliver in Webster et al., 1992). SBCs project as well to more central nuclei such
as the contralateral ventral nucleus of the lateral lemniscus (VNLL)
(Oertel and Wickesberg in Oertel, Fay, and Popper, 2002).
The innervation pattern of the GBC is similar to that of the SBC
(Figure 3B) and forms a second part of the ILD pathway. They tend
to have dendritic fields that are somewhat larger than SBCs and
are preferentially contacted by HSR fibers. GBCs, like SBCs, receive extensive input from AN-I fibers on their cell bodies, but
these synapses are smaller than the endbulb type found on SBCs.
GBCs have response properties similar to those of AN fibers with
PLN PSTHs (Figure 2B), but with a higher degree of synchronization to the acoustic input (Yin in Oertel et al., 2002).
SBCs project primarily to the contralateral SOC, specifically the
medial nucleus of the trapezoid body (MNTB), which provides
inhibitory input to the LSO (Oliver and Huerta in Webster et al.,
1992). The SBC axonal endings terminate in a calyx surrounding
the cell bodies of the principal cells of the MNTB. This synaptic
specialization suggests that timing is important in the ILD pathway
as well as in the ITD pathway. Other projections of the GBCs
include the ipsilateral lateral nucleus of the trapezoid body (LNTB),
the contralateral VNLL, and periolivary nuclei on both sides (Yin
in Oertel et al., 2002).

Figure 3. Examples of neural circuits in the cochlear nucleus. Excitatory connections are indicted by solid lines and inhibitory connections are indicated by
broken lines. A, The stellate cell circuit. B, The sperical bushy cell circuit. C, The fusiform circuit. See text for explanation of abbreviations.

Auditory Periphery and Cochlear Nucleus

Fusiform and Giant Cells
The DCN is believed to be involved in processing spectral cues
that are important for sound source location, especially source elevation. Cats with lesions to the DCN output pathways exhibit
significant deficits in their ability to orient to sources at different
locations (Young and Davis in Oertel et al., 2002). The DCN is
usually subdivided into three layers, a superficial or molecular
layer, an intermediate layer called the granular or fusiform cell
layer, and a polymorphic or deep layer. The principal cells of the
DCN are the fusiform cells (from which the fusiform cell layer gets
its name) and the giant cells located in the deep layer. The apical
dendrites of fusiform cells (also called pyramidal cells) are highly
branched and extend up into the molecular layer, while the less
highly branched basal dendrites extend down into the deep layer.
The dendritic morphology of the giant cells is more diverse, ranging from elongate to radiate (Cant in Webster et al., 1992). Beneath
the fusiform cells are a group of cells called vertical cells. These
cells have their dendritic and axonal arbors confined to an isofrequency lamina. There are two groups of vertical cells. The more
superficial group gives rise to only a local axon, the deeper group
gives rise to axons that project to the VCN (Rhode, 1999).
The fusiform cell circuit is shown in Figure 3C. The basal dendrites of fusiform cells receive excitatory input from AN-I fibers
with a limited range of CFs, a narrow-band inhibitory input from
the vertical cells of the DCN, and a wide-band inhibitory input
from the D-stellate cells of the VCN. The inhibitory input from the
vertical cells is quite strong, and as a result, fusiform cells respond
poorly or not at all to pure tones. They respond well to broadband
stimuli except when there is a spectral notch at the characteristic
frequency, in which case these cells are strongly inhibited (Rhode
and Greenberg in Popper and Fay, 1992). As a result of these properties, fusiform cell models create a spectral representation that
accentuates spectral notches (Hancock and Voigt, 1999), which are
important features of the HRTF for determining sound source elevation. The fusiform cells also receive excitatory input on their
apical dendrites from the granule cells, which in turn receive input
from the dorsal column and spinal trigeminal nuclei of the somatosensory system. This somatosensory input appears to modify
DCN response properties based on head and pinna position (Kanold
and Young, 2001).
The giant cell circuit (not shown) is similar to the fusiform cell
circuit except that the giant cells do not receive direct input from
granule cells and AN input to the giant cells spans a large range of
characteristic frequencies. Fusiform and giant cells project to the
contralateral ICC and also project to the contralateral DNLL, which
provides inhibitory input to both ICs (Oliver and Huerta in Webster
et al., 1992).

Discussion
Significant progress has been made in understanding the anatomy
and physiology of the subthalamic auditory pathways, but many
questions remain. For example, the experimental data suggest that
OHCs contribute to the tuned response of the BM and IHCs, but
how OHCs perform their function is not well understood. Much of
the basic circuitry of the CN has been worked out, but it is not
clear how many subpopulations exist for each of the basic cell types

131

described in this article. And perhaps the greatest question of all
is, how is information in the parallel pathways leaving the CN
reintegrated into a unified percept by higher centers? To answer
these questions, future research will need to take an integrated approach, with computational models being used to aid the design
and interpretation of anatomical and physiological experiments.
These models will need to incorporate the major features of individual cell types as well as the interactions between cell types at
different levels of the auditory system. Efforts to create suitable
large-scale models have begun (cf. Hawkins et al., 1996), but much
remains to be done.

Road Maps: Mammalian Brain Regions; Other Sensory Systems
Related Reading: Auditory Cortex; Auditory Scene Analysis; Echolocation: Cochleotopic and Computational Maps; Sound Localization and
Binaural Processing; Thalamus

References
Dallos, P., Popper, A. N., and Fay, R.-R., 1996, The Springer Handbook
of Auditory Research, vol. 8, The Cochlea, New York: SpringerVerlag. ⽧
Doucet, J. R., and Ryugo, D. K., 1997, Projections from the ventral cochlear
nucleus to the dorsal cochlear nucleus in rats, J. Comp. Neurol.,
385:245–264.
Ferragamo, M. J., Golding, N. L., and Oertel, D., 1998, Synaptic inputs to
stellate cells in the ventral cochlear nucleus, J. Neurophysiol., 79:51–63.
Gilkey, R. H., and Anderson, T. R., 1997, Binaural and Spatial Hearing
in Real and Virtual Environments, Mahwah, NJ: Erlbaum.
Hancock, K. E., and Voigt, H. F., 1999, Wideband inhibition of dorsal
cochlear nucleus type IV units in cat: A computational mode, Ann. Biomed. Eng., 27:73–87.
Hawkins, H. L., McMullen, T. A., Popper, A. N., and Fay, R.-R., 1996,
The Springer Handbook of Auditory Research, vol. 6, Auditory Computation, New York: Springer-Verlag. ⽧
Kanold, P. O., and Young, E. D., 2001, Proprioceptive information from
the pinna provides somatosensory input to cat dorsal cochlear nucleus,
J. Neurosci., 21:7848–7858.
Mountain, D. C., and Cody, A. R., 1999, Multiple modes of inner hair cell
stimulation, Hear. Res., 132:1–14.
Oertel, D., Bal, R., Gardner, S. M., Smith, P. H., and Joris, P. X., 2000,
Detection of synchrony in the activity of auditory nerve fibers by octopus
cells of the mammalian cochlear nucleus, Proc. Natl. Acad. Sci. USA,
97:11773–11779.
Oertel, D., Fay, R. R., and Popper, A. N., 2002, The Springer Handbook
of Auditory Research, vol. 15, Integrative Functions in the Mammalian
Auditory Pathway, New York: Springer-Verlag. ⽧
Oertel, D., Wu, S. H., Garb, M. W., and Dizack, C., 1990, Morphology
and physiology of cells in slice preparations of the posteroventral cochlear nucleus of mice, J. Comp. Neurol., 295:136–154.
Popper, A. N., and Fay, R.-R., 1992, The Springer Handbook of Auditory
Research, vol. 2, The Mammalian Auditory Pathway: Neurophysiology,
New York: Springer-Verlag. ⽧
Rhode, W. S., 1999, Vertical cell responses to sound in cat dorsal cochlear
nucleus, J. Neurophys., 82:1019–1032.
Webster, D. B., Popper, A. N., and Fay, R.-R., 1992, The Springer Handbook of Auditory Research, vol. 1, The Mammalian Auditory Pathway:
Neuroanatomy, New York: Springer-Verlag.
Zhang, X., Heinz, M. G., Bruce, I. C., and Carney, L. H., 2001, A phenomenological model for the responses of auditory-nerve fibers: I. Nonlinear
tuning with compression and suppression, J. Acoust. Soc. Am., 109:648–
670.

132

Part III: Articles

Auditory Scene Analysis
Guy J. Brown
Introduction
We usually listen in environments that contain many simultaneously active sound sources. The auditory system must therefore
parse the acoustic mixture that reaches our ears to segregate a target
sound source from the background of other sounds. Bregman
(1990) describes this process as auditory scene analysis (ASA) and
suggests that it takes place in two conceptual stages. The first stage,
segmentation, decomposes the acoustic mixture into its constituent
components. In the second stage, acoustic components that are
likely to have arisen from the same environmental event are
grouped, forming a perceptual representation (stream) that describes a single sound source. Streams are subjected to higher-level
processing, such as language understanding.
Bregman’s account makes a distinction between schema-driven
and primitive mechanisms of grouping. Schema-driven grouping
applies learned knowledge of sound sources such as speech in a
top-down manner (in this regard, the term “schema” refers to a
recurring pattern in the acoustic environment). Primitive mechanisms operate on the acoustic signal in a bottom-up fashion and
are well described by Gestalt heuristics such as proximity and common fate (see CONTOUR AND SURFACE PERCEPTION). Primitive organization is both simultaneous and sequential. Simultaneous
grouping operates on concurrent sounds, using principles such as
similarity of fundamental frequency. Sequential grouping combines acoustic events over time, according to heuristics such as
temporal proximity and frequency proximity.
At the physiological level, segmentation corresponds (at least in
part) to peripheral auditory processing, which performs a frequency
analysis of the acoustic input. To a first approximation, this frequency analysis can be modeled by a bank of band-pass filters with
overlapping passbands, in which each channel simulates the filtering characteristics of one location on the basilar membrane (see
AUDITORY PERIPHERY AND COCHLEAR NUCLEUS). From the output
of each filter, a simulation of the auditory nerve response can be
obtained by rectification and compression or from a detailed model
of inner hair cell function. In contrast, the physiological substrate
of auditory grouping is much less well understood (Feng and Ratnam, 2000). As a result, models of ASA tend to be functional in
approach. In the current review, we focus on models that are at
least physiologically plausible; however, it should be noted that
there is also a substantial literature that has addressed computational modeling of ASA from a more abstract informationprocessing perspective (e.g., Rosenthal and Okuno, 1998).

and Meddis (1996) describe a model of auditory streaming that has
its basis in mechanisms of peripheral auditory function. The model
utilizes two pathways: one in which auditory nerve activity is
smoothed by temporal integration and an “excitation-level” path
that adds a cumulative random element to the output of the temporal integration path. Firing activity is considered in three auditory
filter channels: one at the frequency of each tone and one in between them. The channel with the highest activity in the excitationlevel pathway is selected as the dominant “foreground” percept;
the remaining channels are attenuated and become the “background.” This simple model quantitatively matches auditory
streaming phenomena, such as the effect of rate of presentation and
frequency separation. Furthermore, the inclusion of a random element in the model (which is assumed to originate from the stochastic nature of auditory nerve firing patterns) explains how spontaneous shifts of attention can occur.
McCabe and Denham (1997) have extended the Beauvois and
Meddis model by applying similar principles within a two-layer
neural architecture. In their model, “foreground” and “background”
neural arrays are connected by reciprocal inhibitory connections,
which ensure that activity appearing in one array does not appear
in the other (Figure 1). Their network is sensitive to frequency
proximity because the strength of inhibitory feedback is related to
the frequency difference between acoustic components. Additionally, each layer receives a recurrent inhibitory feedback related to
the reciprocal of its own activity. As a result, previous activity in
the network tends to suppress differences in subsequent stimuli.
This mechanism may be viewed as a neural implementation of
Bregman’s (1990) “old plus new heuristic,” which states that the
auditory system prefers to interpret a current sound as a continuation of a previous sound unless there is strong evidence to the
contrary. The inclusion of this heuristic within McCabe and Denham’s model allows it to explain the effect of background organization on the perceptual foreground.

Models of Sequential Grouping
Sequential grouping can be demonstrated by playing listeners a
repeating sequence of two alternating tones with different frequencies (ABAB . . .). When the sequence is played rapidly or when
the frequency separation between the tones is large, the sequence
is heard to split into separate streams (A-A- . . . and B-B- . . .).
This phenomenon is known as auditory streaming (Bregman,
1990). Listeners are able to direct their attention to only one of the
streams, which appears to be subjectively louder than the other.
Auditory streaming may therefore be regarded as an example of
figure-ground separation.
Auditory streaming may be viewed as a consequence of sequential grouping heuristics that allocate tones to streams depending on
their proximity in time and frequency. Several modeling studies
have demonstrated that such principles can be implemented by relatively low-level physiological mechanisms. For example, Beauvois

Figure 1. McCabe and Denham’s model of auditory stream segregation.
Foreground and background streams are encoded by separate neural arrays,
which have reciprocal inhibitory connections. Each layer also receives recurrent inhibition. Solid circles indicate excitatory connections; open circles
indicate inhibitory connections. (Modified from McCabe and Denham
(1997).)

Auditory Scene Analysis
A more central explanation for auditory streaming is given by
Todd (1996), who suggests that mechanisms of rhythm perception
and stream segregation are underlain by cortical maps of
periodicity-sensitive cells. In his model, periodicity detection leads
to a spatial representation of the temporal pattern of the stimulus
in terms of its amplitude modulation (AM) spectrum. Acoustic
events whose AM spectra are highly correlated (as judged by a
neural cross-correlation mechanism) are perceptually grouped,
whereas events with uncorrelated AM spectra are segregated.
Todd’s model is able to qualitatively replicate the dependence of
auditory streaming on tone frequency and temporal proximity.
The models of Todd (1996) and McCabe and Denham (1997)
suggest that the auditory responses associated with different
streams are encoded spatially in neural arrays. An alternative is that
auditory streams are encoded temporally. For example, Wang
(1996) suggests a principle of oscillatory correlation, which is a
development of von der Malsburg’s temporal correlation theory
(von der Malsburg and Schneider, 1986). In Wang’s scheme, neural
oscillators alternate rapidly between relatively stable states of activity (the active phase) and inactivity (the silent phase). Oscillators
that encode features of the same stream are synchronized (phase
locked with zero phase lag) and are desynchronized from oscillators
that represent different streams.
Wang has implemented the oscillatory correlation principle in a
model of auditory streaming, in which oscillators are arranged
within a two-dimensional time-frequency network. The time axis
of the network is assumed to be constructed by a systematic arrangement of neural delay lines. Each oscillator is connected to
others in its neighborhood with excitatory connections whose
strength diminishes with increasing distance in time and frequency.
In addition, every oscillator sends excitation to a global inhibitor,
which feeds back inhibition to each oscillator in the network. Oscillators that are close in time and frequency tend to synchronize
because the excitatory connections between them are strong. However, groups of oscillators that do not receive mutually supportive
excitation tend to desynchronize because of the action of the global
inhibitor. As the network dynamics evolve, the combined effects
of local excitation and global inhibition cause streams of synchronized oscillators to form. The model qualitatively reproduces a
number of auditory streaming phenomena. However, the oscillatory dynamics proceed rapidly, so Wang’s network is not able to
account for the gradual buildup of the auditory streaming effect
over time.

Models of Simultaneous Grouping
Simultaneous grouping mechanisms exploit differences in the characteristics of concurrent sounds in order to perceptually segregate
them. For example, the ability of listeners to identify two simultaneously presented vowels (“double vowels”) can be improved by
introducing a difference in fundamental frequency (F0) between
the vowels (Bregman, 1990). Apparently, simultaneous grouping
mechanisms are able to segregate the acoustic components related
to each F0 and hence retrieve the spectra of the two vowels.
Meddis and Hewitt (1992) describe a model of double-vowel
identification based on the correlogram, a model of auditory pitch
analysis. A correlogram is formed by computing a running autocorrelation at the output of each auditory filter channel, giving a
two-dimensional representation in which frequency and time lag
are represented on orthogonal axes. Meddis and Hewitt suggest that
the correlogram could be computed neurally by using a system of
delay lines and coincidence detectors. The F0 of one of the vowels
is identified from the correlogram, and channels whose response is
dominated by that F0 are removed, thus allowing a clearer view of
the second vowel. This mechanism fails to separate the vowels
when they both have the same F0 and successfully predicts that

133

vowel identification performance improves when a difference in F0
is introduced.
The Meddis and Hewitt model is based on a strategy of “exclusive allocation” (Bregman, 1990); all of the energy in a single auditory filter channel is allocated to one vowel or the other. However, this need not be the case. De Cheveigné (1997) describes an
approach that uses a “neural cancellation filter” to partition the
energy in each channel between vowel percepts. In his scheme, a
correlogram is computed, and the fundamental period of one of the
vowels is identified. This period is canceled in each channel by a
neural comb filter, which is implemented by a neuron with a delayed inhibitory input. This mechanism removes firing activity with
a periodicity equal to the inhibitory delay. An advantage of de
Cheveigné’s approach is that it predicts an increase in listeners’
performance with increasing difference in F0 for vowels that are
weak in comparison to a harmonic background. The Meddis and
Hewitt model is unable to reproduce this result because its exclusive allocation scheme tends to remove the evidence for a weak
vowel when the stronger vowel is canceled.
Brown and Wang (1997) have described a neural oscillator
model of vowel segregation, which is essentially an implementation of Meddis and Hewitt’s (1992) scheme within an oscillatory
correlation framework. In their model, each channel of the correlogram is associated with a neural oscillator. Oscillators corresponding to channels that are dominated by the same F0 become
synchronized and are desynchronized from channels that are dominated by a different F0. When there is no difference in F0 between
the two vowels, a single group of synchronized oscillators forms.
However, when a difference in F0 is introduced, the two vowels
are segregated according to their F0s, and the channels making up
each vowel are encoded as separate groups of synchronized
oscillators.
Von der Malsburg and Schneider (1986) describe a related model
of simultaneous grouping based on temporal correlation of neural
responses. Their scheme employs a neural architecture in which
each member of a fully connected network of excitatory cells (Ecells) receives an input from one auditory filter channel. In addition,
E-cells receive inhibition from a common inhibitory cell (H-cell).
E-cells that receive simultaneous inputs tend to become synchronized by the excitatory links between them and tend to become
desynchronized from other cells owing to the influence of inhibition from the H-cell. The network therefore displays a sensitivity
to the common onset of acoustic components and may be regarded
as implementing a Gestalt principle of common fate (Bregman,
1990).

The Role of Temporal Continuity
With the exception of that of Wang (1996), relatively few modeling
studies have demonstrated the integration of simultaneous and sequential grouping principles within the same computational framework. However, several studies have shown how a complex timefrequency mixture can be organized using simultaneous grouping
principles and temporal continuity constraints.
Grossberg (1999) describes a multistage model of ASA that implements grouping by common F0 and good continuation. The first
stage of his model builds redundant spectral representations of the
acoustic input in a “spectral stream” layer. Each stream is represented by a separate neural array. These representations are filtered
by neural “harmonic sieves,” which connect a node in a “pitch
stream” layer with spectral regions near to the harmonics of the
corresponding pitch value. Pitch representations compete across
streams to select a winner, and the winning pitch node sends topdown signals via harmonic connections to the spectral stream layer.
According to an adaptive resonance theory (ART) matching rule,
frequency components in the spectral stream that are consistent
with the top-down signal are selected, and others are suppressed
(see ADAPTIVE RESONANCE THEORY). Selected components reac-

134

Part III: Articles

tivate their pitch node, and further top-down signals are produced.
In this way, a resonance develops that binds together the frequency
components constituting a sound source and its corresponding
pitch.
Grossberg’s model is able to account for simple simultaneous
grouping phenomena, such as the perceptual fusion of components
with the same F0. His model also reproduces the auditory continuity illusion (Bregman, 1990), in which a pure tone is heard to
continue through a brief interrupting noise, even though the tone
is not physically present during the noise burst. It is able to do so
because a resonance develops for the tone that is maintained during
the noise burst. The ART matching rule then selects the tone from
the noise, and competitive interactions cause the tone and residual
noise to be allocated to different streams.
Wang and Brown (1999) describe a neural oscillator model
whose two-layer architecture echoes the two conceptual stages of
ASA. The first (segmentation) layer consists of a two-dimensional
time-frequency grid of oscillators with a global inhibitor (Figure
2). In this layer, excitatory connections are formed between auditory filter channels that have a similar temporal response. As a
result, segments form in the time-frequency plane and thus correspond to harmonics and formants. The global inhibitor ensures that
each segment desynchronizes from the others; the first layer therefore embodies the segmentation stage of ASA, in which the acoustic signal is split into its constituent elements. The second layer
receives an input from the first layer. Also, segments in the second
layer are connected by excitatory links if they represent timefrequency regions that are dominated by the same F0. As a result,
synchronized groups of segments emerge in the second layer, each
of which corresponds to a stream with harmonically related
components.

Models of Schema-Driven Grouping
Liu, Yamaguchi, and Shimizu (1994) describe a neural oscillator
model of vowel recognition that may be regarded as an implementation of schema-driven grouping. The model consists of an input
layer and three layers of oscillators labeled A, B, and C, which are
likened to regions of the auditory cortex. The A (“feature extraction”) layer identifies local peaks in the acoustic spectrum and en-

Figure 2. The two-layer neural oscillator model of Wang and Brown
(1999). In the first layer, segments are formed that correspond to harmonics
and formants. The second layer groups segments according to their fundamental frequency (F0); those with the same F0 form a stream in which
all oscillators are synchronized and are desynchronized from other streams.
(Modified from Wang and Brown (1999).)

codes them as separate groups of oscillations, which are assumed
to correspond to vowel formants. The B (“feature linking”) layer
acts as a simple associative memory, in which hardwired connections encode the relationship between formant frequencies for different vowels. Associative interactions between the B layer, together with top-down and bottom-up interactions between the A
and B layers, lead to the activation of a vowel in terms of a global
pattern of synchronized oscillations. The C (“evaluation”) layer
assesses the synchronization in each formant region and outputs a
vowel category. Top-down reinforcement from the B center confers
robustness in noise; it is demonstrated that the model is able to
recognize vowels robustly in the presence of multispeaker babble.

Discussion
The modeling studies reviewed here propose a neurobiological basis for the principles of auditory organization expounded in Bregman’s account of ASA. Clearly, the models differ in their level of
explanation, ranging from peripheral (Beauvois and Meddis, 1996)
to cortical (Todd, 1996). Without exception, their approach is functional; currently, there is insufficient knowledge about the physiological mechanisms of ASA to attempt a detailed physiological
model. It is likely that future research in this field will see a closer
synergy between computational modeling studies and neurophysiological investigation.
Various strategies have been proposed for the neural encoding
of auditory streams. Beauvois and Meddis (1996) stress that the
perceptual separation of sounds need not imply a physical separation of their corresponding representations; in their model, auditory
filter channels belonging to a nonattended stream are simply attenuated. This contrasts with the approaches described by Grossberg
(1999) and McCabe and Denham (1997), in which different auditory streams are encoded by separate neural arrays. A further approach is to encode streams temporally in the responses of synchronized neural firing patterns (von der Malsburg and Schneider,
1986; Wang, 1996). Although all of these approaches are plausible,
none are currently supported by strong neurophysiological
evidence.
Many models of ASA require systematic time delays longer than
those currently known to exist in the auditory system. Models of
double-vowel separation based on the correlogram require delays
of the order of 20 ms (Meddis and Hewitt, 1992; de Cheveigné,
1997). Similarly, it is questionable whether the system of delay
lines employed in Wang’s (1996) neural oscillator model is physiologically realizable. The temporal correlation architecture of von
der Malsburg and Schneider (1986) does not suffer from this difficulty, since their network does not have an explicit time axis.
However, the explanatory power of their model is weak in comparison to Wang’s model, because temporal and frequency relationships between acoustic inputs are not preserved.
Generally speaking, the role of auditory attention has been neglected in computer models of ASA. In auditory streaming, a listener’s attention can shift randomly between organizations or may
be consciously directed to the high or low tones. The model of
Beauvois and Meddis (1996) accounts for the former but not the
latter. Similarly, McCabe and Denham’s model includes an attentional input (Figure 1), but it is not utilized in their simulations.
Wang (1996) suggests that in a neural oscillator framework, attention is paid to a stream when its constituent oscillators reach their
active phases; attention therefore alternates quickly among the
streams in turn. However, such a scheme does not explain how
listeners are able to direct their attention to a particular stream over
a sustained period of time.
Also, few models have attempted to model the interaction between top-down and bottom-up grouping mechanisms. In principle,
the mechanism of recurrent neural connections described by Liu et
al. (1994) could form the basis for such a model. Similarly, Gross-

Axonal Modeling
berg’s (1999) ART scheme could form the basis for a grouping
mechanism in which bottom-up features interact with top-down
information about the characteristics of sound sources.
The motivation for most of the studies reviewed here is to gain
insight into the mechanisms of ASA through computational modeling. However, computer sound separation devices have many
real-world applications, such as in hearing prostheses and as preprocessors for robust automatic speech recognition in noise. For
example, Wang and Brown (1999) have applied their model to the
separation of voiced speech from interfering sounds, with some
success. Because they are founded on neurobiological principles,
such approaches to sound separation may offer performance advantages over other techniques, such as blind statistical methods.
Road Map: Other Sensory Systems
Related Reading: Auditory Periphery and Cochlear Nucleus; Contour and
Surface Perception; Dynamic Link Architecture; Echolocation: Cochleotopic and Computational Maps

References
Beauvois, M. W., and Meddis, R., 1996, Computer simulation of auditory
stream segregation in alternating-tone sequences, J. Acoust. Soc. Am.,
99:2270–2280.
Bregman, A. S., 1990, Auditory Scene Analysis, Cambridge, MA: MIT
Press. ⽧
Brown, G. J., and Wang, D., 1997, Modelling the perceptual segregation

135

of double vowels with a network of neural oscillators, Neural Networks,
10:1547–1558.
de Cheveigné, A., 1997, Concurrent vowel identification: III. A neural
model of harmonic interference cancellation, J. Acoust. Soc. Am.,
101:2857–2865.
Feng, A. S., and Ratnam, R., 2000, Neural basis of hearing in real-world
situations, Annu. Rev. Psychol., 51:699–725. ⽧
Grossberg, S., 1999, Pitch-based streaming in auditory perception, in Musical Networks: Parallel Distributed Perception and Performance (N.
Griffith and P. Todd, Eds.), Cambridge, MA: MIT Press, pp. 117–140.
Liu, F., Yamaguchi, Y., and Shimizu, H., 1994, Flexible vowel recognition
by the generation of dynamic coherence in oscillator neural networks:
Speaker-independent vowel recognition, Biol. Cybern., 71:105–114.
McCabe, S. L., and Denham, M. J., 1997, A model of auditory streaming,
J. Acoust. Soc. Am., 101:1611–1621.
Meddis, R., and Hewitt, M. J., 1992, Modelling the identification of concurrent vowels with different fundamental frequencies, J. Acoust. Soc.
Am., 91:233–245.
Rosenthal, D., and Okuno, H. G. (Eds.), 1998, Computational Auditory
Scene Analysis, Mahwah, NJ: Lawrence Erlbaum Associates. ⽧
Todd, N., 1996, An auditory cortical theory of primitive auditory grouping,
Network: Comput. Neural Syst., 7:349–356.
von der Malsburg, C., and Schneider, W., 1986, A neural cocktail-party
processor, Biol. Cybern., 54:29–40.
Wang, D., 1996, Primitive auditory segregation based on oscillatory correlation, Cogn. Sci., 20:409–456. ⽧
Wang, D., and Brown, G. J., 1999, Separation of speech from interfering
sounds based on oscillatory correlation, IEEE Trans. Neural Networks,
10:684–697.

Axonal Modeling
Christof Koch and Öjvind Bernander
Introduction
Axons are highly specialized “wires” that conduct the neuron’s
output signal to target cells—in the case of cortical pyramidal cells
up to 10,000 other cortical neurons. As such they are highly specialized, with a relatively stereotypical behavior. Most authors
agree that their role in signaling is largely limited to making sure
that whatever pulse train is put into one end of the axon is rapidly
and faithfully propagated to the other end. This is in contrast to the
complexity of electrical events occurring at the cell body and in
the dendritic tree, where the information from thousands of synapses is integrated.
Despite the uniformity of electrical behavior, there is great morphological variability that largely reflects a trade-off between propagation speed and packing density. Axonal size varies over four
orders of magnitude: diameters range from 0.2-lm fibers in the
mammalian central nervous system to 1 mm in squid; lengths range
from a few hundred microns to over a meter for motor neurons
(Kandel, Schwartz, and Jessell, 2000). Some axons are bound only
by the thin cellular membrane, while others are wrapped in multiple
sheaths of myelin. An example of an axonal arbor is shown in
Figure 1.
The majority of nerve cells encode their output as a series of
brief voltage pulses. These pulses, also referred to as action potentials or spikes, originate at or close to the cell body of nerve cells
and propagate down the axon at constant amplitude. Their shape
is relatively constant across species and types of neurons. Common
to all is the rapid upstroke (depolarization) of the membrane above
0 mV and the subsequent, somewhat slower, downstroke (repolarization) toward the resting potential and slightly beyond to more
hyperpolarized potentials. At normal temperatures, the entire sequence occurs within less than 1 ms. A minority of cell types are
axonless and appear to use graded voltage as output, such as cells

in the early part of the retina or interneurons in invertebrates (Roberts and Bush, 1981). Action potentials are such a dominant feature
of the nervous system that for a considerable period of time it was
widely held—and still is in parts of the neural network community—that all neuronal computations involve only these all-or-none
events. This belief provided much of the impetus behind the neural
network models originating in the late 1930s and early 1940s (see
SINGLE-CELL MODELS).
The ionic mechanisms underlying the initiation and propagation
of action potentials were elucidated in the squid giant axon by a
number of workers, most notably Hodgkin and Huxley (1952). Today, with the widespread availability of cheap and almost unlimited
computational power, it is very difficult to imagine the difficulty
that Hodgkin and Huxley faced 50 years ago. Not only did they
have to derive a proper mathematical formalism based on incomplete data, they also had to solve a nonlinear partial differential
equation, the cable equation, using a very primitive hand calculator.
For this work they shared, together with Eccles, the 1963 Nobel
prize in physiology and medicine (for a historical overview, see
Hodgkin, 1976). Their model has played a paradigmatic role in
biophysics; indeed, the vast majority of contemporary biophysical
models use essentially the same mathematical formalism Hodgkin
and Huxley introduced 50 years ago. This is all the more surprising
because the kinetic description of the continuous, deterministic, and
macroscopic membrane permeability changes within the framework of the Hodgkin-Huxley model was achieved without any
knowledge of the underlying all-or-none, stochastic, and microscopic ionic channels.
Given its importance, we will describe the Hodgkin-Huxley
model and its assumptions in some detail in the following section.
We then introduce the two classes of axons found in most animals,
myelinated and nonmyelinated, and describe their differences. Ax-

136

Part III: Articles
Figure 1. Axonal terminations. An axon from nucleus
isthmi terminating in turtle tectum was labeled with
horseradish peroxidase and reconstructed from a series
of parallel sections. The thick parent trunk (3 lm) is
wrapped in myelin and shows a node of Ranvier (triangles). The thin (1 lm) branches are nonmyelinated and
are home to approximately 3,600 synaptic boutons (bulbous thickenings), where contact is made onto other
cells. The boutons vary greatly in size but average about
1.5 lm in diameter. (From Sereno, M. I., and Ulinski,
P. S., 1987, Caudal topographic nucleus isthmi and the
rostral nontopographic nucleus isthmi in the turtle, Pseudemys scripta, J. Comp. Neurol., 261:319–346. Copyright 䉷 1987 by Wiley-Liss. Reprinted by permission of
John Wiley & Sons, Inc.)

ons possess heavily branched axonal trees. We conclude the overview by briefly alluding to additional complications that arise when
attempting to understand the role and function of axonal trees in
information processing. For a useful book on the biophysics of
dendrites and axons and their computational function, see Koch
(1999). For a monograph on the axon in health and disease, see
Waxman, Kocsis, and Stys (1995).

The Hodgkin-Huxley Model of Action
Potential Generation
Electrical current in nerve cells is carried by the flow of ions
through membrane proteins called channels. The concentration of
sodium ions is high in the extracellular fluid and low in the intracellular axoplasm. This concentration gradient gives rise to a tendency for sodium ions to flow into the cell. At some membrane
potential, termed the reversal potential, the effect of the concentration gradient will be canceled by the electrical gradient, and so
the net flow of sodium ions will be zero at that point. The channel

transitions into its closed state by virtue of a conformational state
in the underlying molecular structure. In the model, this is described by a change in the m variable. A similar situation holds for
potassium ions flowing through separate potassium-selective channels, except that the concentration gradient is reversed.
In the squid giant axon, the membrane potential is determined
by three conductances: a voltage-independent (passive) leak conductance, gl, a voltage-dependent (active) sodium conductance,
gNa, and an active potassium conductance, gK. The equivalent circuit used to model the membrane is shown in Figure 2. The conductances are in series with batteries, the values of which correspond to the respective reversal potentials of the ionic currents, El,
ENa, and EK. The outside is connected to ground under the assumption that the resistivity of the external medium is negligible.
The time course of an action potential is illustrated in Figure 3.
In this simulation of a membrane patch, a brief current pulse initiates an action potential. Before stimulation, the membrane voltage
is at rest, Vm ⳱ ⳮ65 mV. At this potential, gNa and gK are almost
fully inactivated. gK is still much larger than gNa, and the membrane

Axonal Modeling

137

Figure 2. Schematic of ionic channel and neuronal membrane: Equivalent
circuit of axonal membrane. The Hodgkin-Huxley model of squid axon
incorporates a capacitance and three conductances. Two of the conductances are voltage dependent (active), gNa and gK, while the third is a passive “leak” conductance, gl. The maximal conductances are 120, 36, and
0.3 ms/cm2, respectively. Each conductance is in series with a battery that
defines the reversal potential for each conductance type. The values are
ENa ⳱ 50, EK ⳱ ⳮ77, and El ⳱ ⳮ54.3 mV. See text for the voltage
dependences of gNa and gK. The top rail corresponds to the axoplasm (inside) of the axon, while the bottom rail, grounded, is the external medium.
When a membrane action potential or space clamp is modeled, only one
compartment is used, as shown, and the spatial structure of the membrane
is ignored. When propagating action potentials are modeled, the specific
resistivity of the axoplasm, Ra ⳱ 34.5 Xcm, cannot be ignored. Ra is then
modeled as a series of resistors connecting identical compartments that
correspond to different spatial locations along the axon. The membrane
capacitance is 1 lm F/cm2.

is dominated by the leak current and the residual potassium current.
The applied current slowly depolarizes the membrane by charging
up the capacitance. As Vm approaches threshold (Vt  ⳮ50 mV),
sodium channels begin to open up, allowing for the influx of NaⳭ
ions, which further depolarizes the membrane. About 1 ms later,
two events occur to bring the voltage back toward and somewhat
beyond the resting value: the sodium conductance inactivates, that
is, the sodium channels slowly close again, and potassium channels
open up, causing an outward current to flow. This outward current
forces the membrane potential below the resting value of ⳮ65 mV
(hyperpolarization), but the KⳭ conductance too eventually deactivates, allowing gl to pull Vm back to rest.

Mathematical Formulation
The equation describing the circuit in Figure 2 is:
C

dVm
⳱ gl(El ⳮ Vm) Ⳮ gNa(ENa ⳮ Vm) Ⳮ gK(EK ⳮ Vm)
dt

(1)

While gl is constant, gNa and gK are time and voltage dependent:
¯ Na • m(t)3h(t)
gNa ⳱ G
¯ K • n(t)4
gK ⳱ G
where the constants ḠNa and ḠK are the maximal conductances and
the time and voltage dependence reside in the so-called gating variables, described by the state variables m, h, and n. These fictitious
variables follow first-order kinetics, relaxing exponentially toward
a steady-state value x with a time constant sx:
dm
m (V ) ⳮ m
⳱  m
dt
sm(Vm)
dh
h(Vm) ⳮ h
⳱
dt
sh(Vm)
dn
n(Vm) ⳮ n
⳱
dt
sn(Vm)

Figure 3. Action potential. Computed action potential in response to a 0.5ms current pulse of 0.4-nA amplitude (solid lines) compared to a subthreshold response following a 0.35-nA current pulse (dashed lines). A, Time
course of the two ionic currents. Note their large sizes compared to the
stimulating current. B, Membrane potential in response to sub- and suprathreshold stimuli. The injected current charges up the membrane capacity
(with an effective membrane time constant s ⳱ 0.85 ms), enabling sufficient INa to be recruited to outweigh the increase in IK (due to the increase
in driving potential). The smaller current pulse fails to trigger an action
potential, but causes a depolarization followed by a small hyperpolarization
due to activation of IK. C, Dynamics of the gating variables. Sodium activation m changes much more rapidly than either h or n. The long time
course of potassium activation n explains why the membrane potential takes
12 ms after the potential has first dipped below the resting potential to return
to baseline level. (From Koch, C., 1999, Biophysics of Computation, Cambridge, MA: MIT Press, p. 150.)

The steady-state activations (m, h, and n) have a sigmoidal dependence on voltage. The activation variables m and n have the
asymptotes limVmrⳮ m, n ⳱ 0, limVmr m, n ⳱ 1, while the
reverse holds for the inactivation variable h. That is, for very negative voltages, the m and n variables shut off current flow through
both channel types, while at very positive potentials, the h particle
shuts off the sodium current. The time “constants” (sm, sh, and sn)

138

Part III: Articles

are not constant with respect to voltage but rather have a roughly
bell-shaped dependence, with peaks in the ⳮ80 to ⳮ40 mV range.
The x and sx values were the ones actually measured by Hodgkin
and Huxley using a series of voltage clamp steps. Instead of fitting
these curves directly with mathematical functions, which would be
sufficient for simulation purposes, they chose to express x and sx
in terms of the variables ␣x and bx:

␣x
␣x Ⳮ bx
1
sx ⳱
␣x Ⳮ bx

x ⳱

0.1(Vm ⳮ 40)
e(Vmⳮ40)/10 ⳮ 1

␣h ⳱ 0.07e(Vmⳮ65)/20
␣n ⳱

0.01(Vm ⳮ 55)
e(Vmⳮ55)/10 ⳮ 1

bm ⳱ 4e(Vmⳮ65)/18
bh ⳱

Equation 1 describes a patch of membrane with no spatial extent.
This corresponds to the original experiments, in which the axon
was “space-clamped”: a long electrode was inserted into the axon
along its axis, removing any spatial dependence. In response to
stimulation, the whole membrane would fire simultaneously as a
single isopotential unit. More commonly, one end of the axon is
stimulated and an action potential propagates to the other end. The
equation that governs extended structures is the cable equation:
C

where ␣x and bx depend on Vm as follows:

␣m ⳱

Action Potential Propagation

1
(Vmⳮ35)/10

e

Ⳮ 1

(Vmⳮ65)/80

bn ⳱ 0.125e

Note that the dimensions of sx, ␣x, and bx are all in units of 1/s,
while nx is a pure number.
These rate constants assume a temperature of 6.3C. At higher
temperatures, they should be multiplied by a factor of around 3 per
10C. The functional forms were chosen by Hodgkin and Huxley
for two reasons. First, they were among the simplest that fit the
data, and second, they resemble the equations that govern the
movement of a charged particle in a constant field.
There is no direct way to map this set of equations in a simple
manner onto the known molecular correlates of ionic channels,
except that many voltage-dependent ionic channels possess four
identical subunits, close or identical to the exponent of the activation variable that determines the momentary conductance. How
the molecular structure and physical chemistry of these membrane
pores explain the high throughput (up to 108 ions per second) and
selectivity (the potassium channel is at least 10,000 times more
permeant to KⳭ than to NaⳭ ions) has been revealed in stunning
detail for potassium channels by atomic-resolution pictures of them
(Doyle et al., 1998).
Conceptually, x(Vm) can be thought of as the probability that
an x particle will be in the open state at potential Vm. Each particle
follows a two-state Markov model, where ␣x is the rate constant
from the closed to the open state and bx is the rate constant from
the open to the closed state. The time courses of the three variables
are graphed in Figure 3.
This mathematical formalism was laid down in 1952. Since then,
most models of voltage-dependent conductances—not only in axons, but also in cell bodies, and dendrites—have used the same
formalism, with only minor modifications (Koch and Segev, 1998).
The macroscopic Hodgkin-Huxley equations are both continuous and deterministic, yet the underlying microscopic ionic channels are binary and stochastic. That is, a correct biophysical formulation of the dynamics of the membrane potential needs to take
into account the well-known probabilistic behavior of these ionic
channels. However, given the large number of channels involved
in axonal spike initiation and propagation, it is usually appropriate
to approximate the system using the deterministic Hodgkin-Huxley
equations. This is not to say, however, that for thin fibers with very
high input impedances, a small number of channels, and close to
the threshold, stochastic variation in channel behavior might not
have large-scale effects on the timing of action potentials (Schneidman, Freedman, and Segev, 1998; Koch, 1999).

Vm
d 2Vm
⳱
Ⳮ gl (El ⳮ Vm)
t
Ra x2
Ⳮ gNa(ENa ⳮ Vm) Ⳮ gK(EK ⳮ Vm)

(2)

where d is the axon diameter, C is the membrane capacity, and Ra
is the intracellular resistivity. The equation rests on the assumption
of radial symmetry, i.e., radial current flow can be neglected, leaving only one spatial dimension, the distance x along the cable, in
addition to t. If the last two (active) terms are dropped from the
right-hand side, we are left with the classical cable equation for
passive cables (see DENDRITIC PROCESSING). Associated with that
equation is the space constant k ⳱ 1/冪gl Ra , which is the distance
across which the membrane potential decays a factor e in an infinite
cable under steady-state conditions.
Figure 4 shows the result of a simulation of a 100-cm-long axon
of diameter d ⳱ 1 mm. One end was stimulated with a brief current
pulse and the voltage was graphed for five positions along the axon.
The form of the action potential is very similar to that in Figure 3;
furthermore, the action potential is self-similar as it propagates,
showing no signs of dispersion.
The total delay from one end to the other is about 5 ms, giving
an average velocity of about 20 m/s. By assuming a constant conduction velocity—that is, by postulating the existence of a wave,
Vm(x, t) ⳱ Vm(x ⳮ vt)—Equation 2 shows that the velocity is
proportional to the square root of axon diameter: v  冪d (Rushton,
1951). Indeed, in a truly remarkable test of their model, Hodgkin
and Huxley estimated the velocity to be 18.8 m/s, a value within
10% of the experimental value of 21.2 m/s. This is surprisingly

Figure 4. A propagating action potential. Solution to the complete Hodgkin-Huxley model for a long piece of squid axon for a brief suprathreshold
current pulse. This pulse generates an action potential that travels down the
cable and is shown here at the origin as well as 2 and 3 cm away from the
stimulating electrode (solid lines). Note that the shape of the action potential
remains invariant due to the nonlinear membrane. If the amplitude of the
current pulse is halved, only a local depolarization is generated (dashed
curve), which depolarizes the membrane 2 cm away by a mere 0.5 mV (not
shown). This illustrates the dramatic difference between active and passive
voltage propagation. (From Koch, C., 1999, Biophysics of Computation,
Cambridge, MA: MIT Press, p. 162.)

Axonal Modeling

139

accurate, considering that the model was derived from a spaceclamped axon. This represents one of the rare instances in which a
neurobiological model has made a successful quantitative prediction. The square root relationship had been discovered experimentally in the squid in the late 1930s.

Myelinated and Nonmyelinated Fibers
The principle of action potential generation and propagation appears to be very similar across neuronal types and species. One
important evolutionary invention is that of myelination in the vertebrate phylum. Myelin sheaths are white fatty extensions of
Schwann cells or neuroglial cells that are wrapped in many layers
around axons. Myelin is a major component of the white matter of
the brain, as opposed to the gray matter of neocortex, which has a
high concentration of cell bodies and dendrites. The myelin sheaths
extend for up to 1–2 mm along the axon (the internodes) and are
separated by the nodes of Ranvier, which are only a few micrometers long. The internodal distance appears to be approximately
linear in fiber diameter.
Myelin insulates the axon from the surrounding medium, increasing the membrane resistance and decreasing the capacitance.
This reduces the electrotonic length of the axon for both DC and
AC signals, making the cable electrically shorter, thereby significantly increasing the propagation speed. While a 1-mm nonmyelinated axon in the squid has an associated propagation speed of
only about 20 m/s (Hodgkin and Huxley, 1952), a myelinated 20lm vertebrate axon can reach over 100 m/s. For a nonmyelinated
axon to reach that velocity, it would have to be an inch thick! This
reduction in axon diameter allows for a much higher packing density while conserving speed.
It has been shown both experimentally and theoretically that the
velocity of propagation is linear or slightly sublinear in the fiber
diameter for myelinated axons. Figure 5 compares the spike propagation velocity for myelinated and nonmyelinated axons for small
diameters. The myelinated axons overtake nonmyelinated ones already in the submicrometer range.
As opposed to their uniform distribution in nonmyelinated nerve,
the voltage-gated channels in myelinated nerve are highly segregated between node and internode (Hille, 1992). The nodal membrane has a high concentration of fast sodium channels (between
700 and 2,000 per lm2) and voltage-independent leak channels.
The internodal membrane has a low concentration of potassium
and leak channels and is virtually devoid of sodium channels. Here,
the repolarization of the membrane following the initial phase of
the spike is via the leak channels and sodium inactivation. This
low density of channels in the internodal membrane, which makes
up the more than 99% of the axonal membrane, reduces the average
current density across the membrane, resulting in great savings in
metabolic energy. Most of the activity occurs at the nodes of Ranvier, while the propagation along the internodes is chiefly passive.
In summary, myelin provides three advantages: propagation
speed and packing density are both dramatically increased, while
power consumption is decreased.

The Axonal Tree
Some axons branch profusely in the vicinity of the cell body. Others send off one or a few branches that course through the body
for up to a meter before branching. Others extend for a few millimeters, giving rise to axonal arbors at regular intervals. The axon
often arises at the “axon hillock,” a somatic bulge opposite from
the trunk of the dendritic tree, though other arrangements are possible, such as the axon’s emanating from the dendrite rather than
the soma.

Figure 5. Spike propagation velocity and axonal diameter. The propagation
velocity has a square root dependence on diameter for nonmyelinated axons. For myelinated axons, the dependence is linear or slightly sublinear.
Myelination increases velocity for axons as thin as 0.2 lm, which are
among the smallest found in the brain. (Adapted from Waxman and Bennett, 1972.)

Figure 1 shows an example of a terminal arbor in turtle tectum
from a cell originating in nucleus isthmi. This particular axon has
a 3-lm myelinated parent trunk and initial daughter branches.
These give rise to hundreds of thin, highly varicosed daughter
branches that lack myelin. The varicosities are usually the location
of synaptic boutons, a local thickening where action potentials trigger the release of neurotransmitter, which in turn induces a conductance change in the postsynaptic target neuron. Boutons of
some neurons may receive synaptic input that can inhibit this signal
transmission, a process known as presynaptic inhibition. The 3,600
boutons on this arbor average 1.5 lm in diameter, though the size
is highly variable, with a few boutons being as large as 7 lm.
The propagation speed along an unbranched axon depends on
the diameter, as discussed earlier. In addition, a delay might be
introduced at branch points, at varicosities at presynaptic terminals,
and at locations where the diameter changes abruptly (Manor,
Koch, and Segev, 1991). The delay may be negative (a speed-up),
depending on the geometrical aspects, in particular the diameter of
the parent branch in relation to that of the daughter branches. In a
simulation of a 3.5-mm-long branched terminal axonal tree, Manor
et al. found that the total axonal delay from the cell body to the
synaptic terminals ranged from about 3 ms to 6 ms. Most of this
delay (67%–78%) arose from the properties of unbranched, uniform cables; 16%–26% resulted from branch point delays, and 6%–
7% from the presence of varicosities. In theory, the delay at a single
branch point may be as large as 2 ms or more, if the temperature
is low and the impedance mismatch is large. If the mismatch is too
large, however, branch point failure may occur, a condition in
which the action potential fails to propagate beyond the branch
point. The concept of branch point filtering has been put forth by
Chung, Raymond, and Lettvin (1970): the branch point may constitute a point of control where selective transmission occurs, allowing the axonal tree to distribute action potentials only to a sub-

140

Part III: Articles

set of nerve terminals. Experimentally, little is known concerning
the amplitude of temporal dispersion of action potentials due to
axonal branching.
While the axonal propagation delay may seem an unavoidable
fact of life that slows down neural communication, it may also have
important computational advantages. For instance, sound localization (see SOUND LOCALIZATION AND BINAURAL PROCESSING) in
the barn owl depends on interaural time differences as small as a
tenth of a millisecond and is apparently obtained by using the axon
as a delay line (Konishi, 1992), and several models of brain function depend critically on the exact timing of inputs from different
sources. Although delays may be imposed by the dendritic trees at
the input end of the neuron, the axons are also important candidates
for this function.
Debanne and colleagues (1997) discovered that action potentials
can be selectively filtered at or beyond axonal branch points via a
fast-inactivating A-type of potassium conductance. When a cell is
hyperpolarized, a depolarizing step within 10–20 ms that would
normally trigger an action potential fails to do so in hippocampal
cell bodies. The reason for this selective block is a GA-like KⳭ
conductance present somewhere along the axon. If de-inactivated
by long-lasting hyperpolarization, it filters out single isolated
spikes. It could thereby act to enhance the signal-to-noise ratio of
neuronal firing. To what extent this is a general mechanism or an
exception to the rule that axons faithfully transmit action potentials
from their site of initiation close to the cell body to their postsynaptic target structures remains to be seen.
Over the past several decades, the formalism introduced by
Hodgkin and Huxley in 1952—voltage- and time-dependent activation and inactivation variables that determine the current value
of the various membrane conductances—has become the de facto
standard for modeling an amazing variety of phenomena, including
adaptation, calcium-dependent conductances, plateau potentials,
first- and second-order inactivation, oscillatory discharges, and several varieties of bursting.

Road Maps: Biological Neurons and Synapses; Grounding Models of
Neurons
Related Reading: Activity-Dependent Regulation of Neuronal Conductances; Ion Channels: Keys to Neuronal Specialization; Oscillatory and
Bursting Properties of Neurons

References
Chung, S. H., Raymond, S. A., and Lettvin, J. Y., 1970, Multiple meaning
in single visual units, Brain Behav. Evol., 3:72–101.
Debanne, D., Guérineau, N. C., Gähwiler, B. H., and Thompson, S. M.,
1997, Action-potential propagation gated by an axonal IA-like KⳭ conductance in hippocampus, Nature, 389:286–289.
Doyle, D. A., Cabral, J. M., Pfuetzner, R. A., Kuo, A., Gulbis, J. M., Cohen,
S. L., Chait, B. T., and MacKinnon, R., 1998, The structure of the potassium channel: Molecular basis of KⳭ conduction and selectivity, Science, 280:69–77.
Hille, B., 1992, Ionic Channels of Excitable Membranes, 2nd ed., Sunderland, MA: Sinauer. ⽧
Hodgkin, A. L., 1976, Chance and design in electrophysiology: An informal
account of certain experiments on nerve carried out between 1934 and
1952, J. Physiol., 263:1–21.
Hodgkin, A. L., and Huxley, A. F., 1952, A quantitative description of
membrane current and its application to conduction and excitation in
nerve, J. Physiol., 117:500–544.
Kandel, E. R., Schwartz, J. H., and Jessell, T. M., Eds., 2000, Principles
of Neural Science, 4th ed., New York: McGraw-Hill.
Koch, C., 1999, Biophysics of Computation, Cambridge, MA: MIT
Press. ⽧
Koch, C., and Segev, I., Eds., 1998, Methods in Neuronal Modeling, 2nd
ed., Cambridge, MA: MIT Press.
Konishi, M., 1992, The neural algorithm for sound localization in the owl,
Harvey Lect., 86:47–64.
Manor, Y., Koch, C., and Segev, I., 1991, Effect of geometrical irregularities on propagation delay in axonal trees, Biophys. J., 60:1424–1437.
Roberts, A., and Bush, B. M. H., Eds., 1981, Neurones without Impulses:
Their Significance for Vertebrates, Cambridge, Engl.: Cambridge University Press. ⽧
Rushton, W. A. H., 1951, A theory of the effects of fibre size in medullated
nerve, J. Physiol., 115:101–122.
Schneidman, E., Freedman, B., and Segev, I., 1998, Ionic channel stochasticity may be critical in determining the reliability and precision of spike
timing, Neural Computat., 10:1679–1704.
Sereno, M. I., and Ulinski, P. S., 1987, Caudal topographic nucleus isthmi
and the rostral nontopographic nucleus isthmi in the turtle, Pseudemys
scripta, J. Comp. Neurol., 261:319–346.
Waxman, S. G., and Bennett, M. V. L., 1972, Relative conduction velocities
of small myelinated and non-myelinated fibers in the central nervous
system, Nature, 238:217–219.
Waxman, S. G., Kocsis, J. D., and Stys, P. K., Eds., 1995, The Axon:
Structure, Function and Pathophysiology, New York: Oxford University
Press. ⽧

Axonal Path Finding
Geoffrey J. Goodhill
Introduction
Many stages are involved in constructing a biological nervous system. Following the migration of neurons to their proper locations
and their phenotypic specification, the initial pattern of connections
forms between different regions (Sanes, Reh, and Harris, 2000).
Making the right connections is crucial for proper function, and
often requires axons to navigate over long distances with great
precision (Tessier-Lavigne and Goodman, 1996). Until recently,
relatively little was known about this process experimentally; however, the past decade has seen a dramatic increase in knowledge
(at least qualitatively) concerning the molecules and mechanisms
involved (Mueller, 1999). These insights are now being applied to
understanding how axons can be made to regenerate to appropriate

targets after injury to the adult nervous system, such as spinal cord
injury.
Until now, the bulk of theoretical work in the neural network
tradition has focused on changes in synaptic strengths within a
fixed connectional architecture. Although local sprouting within the
target has sometimes been considered (as has “sculpting,” based
on the assumption that when synaptic strengths go to zero, the
physical connection is lost), how axons chart their initial path toward the correct target structure has generally not been addressed.
An example is the mapping from the eye to more central targets in
the brain. Abundant theoretical models address how topographic
maps form once axons reach the tectum or visual cortex (see DEVELOPMENT OF RETINOTECTAL MAPS; SELF-ORGANIZING FEATURE
MAPS; and OCULAR DOMINANCE AND ORIENTATION COLUMNS),

Axonal Path Finding
but no theoretical work specifically addresses how retinal ganglion
cell axons find the optic disc, how they then exit the retina, why
they grow toward the optic chiasm, why some then cross at the
midline while others do not, and so on. In recent years important
insight into such issues has been gained through innovative experimental work, creating a body of knowledge that now has the potential to be framed and interpreted in terms of theoretical models.
A crucial point is that, whereas work in neural networks has usually
focused on processes such as synaptic plasticity that are dependent
on neural activity, models for axon guidance must generally be
phrased in terms of activity-independent mechanisms, particularly
guidance by molecular gradients. In this article we first review
some of the important experimental data regarding axon guidance,
and then discuss some of the theoretical concepts that are relevant
to this area.

Experimental Data
Several basic types of mechanisms have been identified to guide
axons. (For more detailed discussions of the data summarized in
this section, see Tessier-Lavigne and Goodman, 1996; Mueller,
1999; and Song and Poo, 2001.) First, axons can be channeled in
particular directions by boundaries of permissive or inhibitory molecules. For instance, a “railroad track” of a permissive molecule
may lock an axon into a particular trajectory, or a “wall” of an
inhibitory molecule may keep it away from an undesired region.
Second, axons can be pushed or pulled by “vector” signals in the
form of molecular gradients. These gradients are often established
by diffusion of a soluble molecule away from the target region.
Third, the path to a distant target may be broken into several short
segments, each involving a different type of cue, thus simplifying
the problem of long-range guidance. Fourth, once one “pioneering”
axon has reached the target, it is often the case that following axons
simply fasciculate with (stick to) the pioneering axon. In each of
these cases the molecules involved may be substrate-bound (expressed on cell membranes or bound to cells or to the extracellular
space) or diffusible (diffusing through the extracellular space).

Figure 1. A, Electron micrograph of a growth cone at the end of an axon
(here the growth cone is resting on a surface irregularly covered with coated
beads). The long finger-like protrusions are filopodia. Growth cones are
typically about 0.01 mm across. (From Rosentreter, S. M., Davenport,
R. W., Löschinger, J. Huf, J., Jung, J., and Bonhoeffer, F., 1998, Response
of retinal ganglion cell axons to striped linear gradients of repellent guidance molecules, J. Neurobiol., 37:541–562. 䉷 1998, John Wiley & Sons,
Inc. Reprinted with permission.) B, Interaction of constraints for guidance
by a target-derived diffusible factor. The graph shows, at each distance, the

141

In the last few years the number of molecules specifically implicated in axon guidance has jumped from virtually none to around
100, most of them previously unknown. (Note that we distinguish
between guidance factors and growth factors: the latter category,
which includes the neurotrophins, are often essential for axons to
extend, but so far have mostly not been shown to play an active
role in axon guidance in vivo.) Guidance factors can be organized
into several main families based on their molecular structure, including the netrins, semaphorins, slits, and ephrins. There is an
astonishing amount of evolutionary conservation in these familes.
Homologous molecules perform analogous guidance functions in
animals ranging from nematodes to flies to mammals, indicating
that the basic molecular tools for wiring a nervous system were
established hundreds of millions of years ago. Molecules involved
in axon guidance are also often involved in the analogous chemotactic event of cell migration, and recent findings even suggest
some commonality with the signal transduction mechanisms important for chemotaxis of leukocytes. Although it was originally
thought that the different types of guidance mechanisms might be
segregated between different families of molecules, it is now clear
that this is not the case. For instance, the same molecule can be
attractive in one context but repulsive in another, or it may normally be substrate-bound but have a soluble fragment that can
diffuse.
Guidance signals for axons are detected and transduced by the
growth cone, a dynamic and motile structure at the tip of the developing axon. This consists of a central region surrounded by weblike veils called lamellipodia, and long, finger-like protrusions
called filopodia (Figure 1A). Receptors expressed on the surface of
the growth cone can bind molecules of the families mentioned
above. The resulting signals are then converted by complex internal
transduction pathways into differential rates of actin polymerization in different parts of the growth cone so as to move it forward,
left, or right. Dissection of the signaling networks responsible for
converting a graded difference in receptor binding into directed
movement is currently a very active area of research. One intriguing
finding is that the concentration of cAMP within the growth cone

time at which two constraints are satisfied: the low concentration limit,
where not enough receptors are bound for a gradient signal to be detected
(assumed to be KD/100, with KD ⳱ 1 nM), and the fractional change constraint (assumed to be DC/C ⳱ 1%). The region between the two curves
in each graph is where guidance is possible. The guidance limit imposed
by the fractional change constraint once the gradient has stabilized is 1 mm.
However, guidance range is extended at earlier times, when the fractional
change constraint has yet to take full effect.

142

Part III: Articles

helps determine how the growth cone responds to a gradient cue:
if the cAMP level is above a certain threshold, the growth cone is
attracted; if the cAMP level is below that threshold, the growth
cone is repelled.

Theoretical Models
Gradient Detection
Several different areas of theoretical development are relevant to
the emerging picture of axon guidance. The general topic of chemotaxis has inspired a large body of theoretical analysis. However,
most work has focused on bacteria and leukocytes, and it remains
to be established how relevant these models are to axon guidance.
Perhaps most important in this category are theories describing the
fundamental physical limits on the minimum steepness of gradients
detectable by any small sensing device. The key hypothesis, first
rigorously formulated by Berg and Purcell (1977), is that gradient
detection is limited by inherent statistical fluctuations in receptor
binding. They calculated the statistical noise in a concentration
measurement DCnoise by a small sensing device that arises from
inevitable stochastic variations in the number of receptors bound
at any instant. The fractional root mean square error in the measurement of a concentration difference between two spatially or
temporally separated points is then 冪2DCnoise /C, where C is the
average concentration at the sensing device. For a true gradient
to be detected, it must be steep enough so that the actual concentration difference between the two points, DCgrad, is such that
DCgrad  冪2DCnoise. DCnoise /C can be calculated from first principles using various simplifying assumptions. This approach has
been applied to growth cones by Goodhill and Urbach (1999), who
derived estimates for the minimum gradient steepness detectable
by an axon for a diffusible gradient of order 1% and for a substratebound gradient of order 10%. The difference arises because of the
lower encounter rate between receptor and ligand molecules for a
bound versus a diffusible gradient. Goodhill and Urbach (1999)
also showed that the movement of filopodia does not significantly
increase the encounter rate, which suggests that filopodia increase
gradient sensitivity only by increasing the effective size of the
growth cone. However, this approach assumes that the receptorligand reaction is diffusion-limited, which may not be the case for
the molecules involved in axon guidance. Theoretical work following Berg and Purcell (1977), while still generally founded on the
basic assumption that gradient detection is limited by the signalto-noise ratio, has attempted to relax this and some other assumptions, but these models have not yet been specifically applied to
growth cones.

Growth Cones
Theoretical models have been proposed to account for filopodial
dynamics. Based on experimentally determined distributions for
parameters such as rates of filopodial initiation, extension, and retraction, filopodial length, and angular orientation, Buettner and
colleagues (e.g., Buettner, 1995) have developed simulation models describing filopodial structure as a function of time, and growth
cone trajectories both during normal growth and when a target is
encountered. Goodhill and Urbach (1999) presented a model of
growth cone trajectories based on the assumption that each filopodium makes a noisy (in Berg and Purcell’s sense) estimate of
the concentration in the direction it is pointing, that more filopodia
are generated in the direction of higher concentration, and that each
filopodium exerts a pull on the growth cone. Other models have
proposed hypotheses about how actin and microtubule dynamics
lead to filopodia formation, though these models have yet to fully
engage with what is known about these processes experimentally.

Another theoretically interesting aspect of growth cones is the
signaling events that convert a small difference in receptor binding
into a large directed movement. Meinhardt (1999) and others have
proposed reaction-diffusion-type models in which a small inhomogeneity in an initially uniform system is amplified via the interaction of a short-range activator with a longer-range inhibitor.
However, to return the system to a uniform state so that the directional preference of the growth cone can change with time, a second
type of reaction with a longer time constant is invoked, and direct
experimental evidence for such processes in growth cones is currently lacking. Tranquillo and Lauffenburger (1987), in the context
of leukocyte chemotaxis, simulated and analyzed a model in which
two pools of receptors (one on each side of the cell) communicated
information about the degree of binding via a single intracellular
messenger. This model was quite successful at accounting for various aspects of leukocyte movement, and subsequent versions by
Tranquillo and colleagues have examined more complex assumptions regarding the internal signaling dynamics. Bacterial chemotaxis has been extensively studied from the perspective of signal
transduction, and theoretical models have been effective at explaining the large amount now known experimentally about this system.
A major focus of such models has been to explain the process by
which bacteria adapt to background levels of ligand so that they
can detect small changes in concentration over many orders of magnitude of absolute concentration. Although such analyses of signaling mechanisms in other chemotacting systems have the potential to be applied to growth cones, it is unclear how similar these
systems really are. More generally there is increasing interest in
mathematical modeling of the signal transduction pathways underlying cell behavior as a whole, although again, there is little application as yet of these theoretical ideas to axon guidance.

Diffusible Gradients and Optimal Gradients
An important class of gradients for guiding axons both in vivo and
in vitro consists of gradients established by diffusion. Hentschel
and van Ooyen (1999) investigated a possible role for diffusion in
controlling axon fasciculation. They considered a population of
axons being guided by a target-derived diffusible factor, and hypothesized that in addition, each axon releases a diffusible attractant that pulls it toward the other axons, hence leading to fasciculation as they grow together toward the target. To account for
defasciculation at the target, they hypothesized that each axon also
releases a repulsive factor for other axons at a rate dependent on
the concentration of the target-derived factor. As the axons approach the target, this repulsive force overcomes the attractive
force, leading to defasciculation.
Another approach is to analyze the gradient shapes expected
from diffusion processes in particular situations and how these constrain the spatiotemporal domains in which guidance is possible
(see Goodhill, 1998, for a review). Goodhill considered a source
releasing a diffusible factor at a constant rate into an infinite, spatially uniform three-dimensional volume, a problem for which there
is a closed-form solution. As long as the gradient is not too steep,
the fractional change in concentration DC/C across the growth cone
width a is DC/C ⳱ (C/r)(a/C), and can be straightforwardly calculated. It has the perhaps surprising characteristic that, for fixed
r, DC/C decreases with t. That is, the largest fractional change at
any distance occurs immediately after the source starts releasing
factor. For large t, DC/C asymptotes at a/r. Thus: (1) At small times
after the start of production the factor is very unevenly distributed.
The concentration C falls quickly to almost zero moving away from
the source, the gradient is steep, and the percentage change across
the growth cone DC/C is everywhere large. (2) As time passes, the
factor becomes more evenly distributed. C everywhere increases,
but DC/C everywhere decreases. (3) For large times, C tends to an

Axonal Path Finding
inverse variation with the distance from the source r, while DC/C
tends to a/r independent of all other parameters. The equation for
DC/C can be compared with the size of the smallest gradient the
growth cone can detect to yield the regions of parameter space
found in which guidance is possible (Figure 1B). Based on data for
leukocyte chemotaxis it was assumed that gradient detection occurs
when DC/C ⱖ p and C ⱖ Cmin, where p is a threshold assumed
independent of C. The positions and times for which the gradient
calculated above satisfies these criteria were examined, given appropriate estimates for the relevant parameters. For large times (a
few days) after the start of factor production, the maximum range
is independent of the diffusion constant and is about 1 mm. This
value fits well with both in vitro and in vivo observations. At earlier
times, however, the factor is more unevenly distributed, being more
concentrated around the source. This makes the fractional change
larger than at later times, increasing the range over which guidance
can occur. Depending on the parameters, the model predicts that
guidance may be possible at distances of several millimeters before
the distribution of factor equilibriates. It is conceivable that such a
mechanism might be utilized in vivo to extend guidance range beyond the 1 mm limit imposed once the gradient has stabilized.
Similarly, one may inquire as to the optimal gradient shape, in
the sense of the shape that guides an axon over the largest possible
distance. Assuming the minimal fractional change is constant (not
dependent on absolute concentration), the optimal shape is clearly
exponential; the maximum guidance distance turns out to be about
1 cm (Goodhill, 1998). It is conceivable that substrate-bound gradients could achieve this shape, and in fact the size of the chick
tectum at the time retinotectal maps are forming is about 1 cm.
Assuming instead that the minimal fractional change varies with
concentration, as predicted by Berg and Purcell, and also assuming
a high concentration limit, the order of magnitude of the result
remains at 1 cm (Goodhill and Urbach, 1999). More generally, this
type of analysis raises the issue of overall scaling between different
species. A guidance mechanism (e.g., target-derived diffusible gradient) that works for a small animal will not work in a large animal
if the anatomy is simply scaled up. In general, the scale and structure of the anatomy of, say, the elephant nervous system at the time
at which long-range navigation occurs are not known in sufficient
detail to allow proper comparison with the same features in, say,
the rat.

Retinotectal Maps
The most well-developed area of axon guidance modeling concerns
the formation of topographic maps in the optic tectum (reviewed
in Goodhill and Richards, 1999). The hypothesis of chemospecificity, that graded distributions of molecules are somehow matched
to graded distributions of complementary molecules in the tectum
so as to form a topographic map, was first proposed qualitatively
by Sperry (1963). Although a great deal of experimental work ensued to investigate how such gradients may actually operate,
matched gradients of receptors in the retina and ligands in the tectum were discovered only in the mid-1990s. These receptors/
ligands are of the Eph/ephrin family, which currently are under
intense experimental investigation. Theoretical modeling started in
the 1970s (e.g., Willshaw and von der Malsburg, 1979), and early
models were based directly on molecular gradients. A key finding
was that some kind of normalization is essential to prevent all axons

143

from targeting the same part of tectum. Although modeling based
on gradients has continued (e.g., Gierer, 1987), the focus of most
modeling work changed to activity-dependent processes. Here only
synaptic strength changes within a fixed architecture are generally
considered, rather than the earlier stage of how axons traverse large
expanses of the tectum. Data and models in this area are discussed
in greater detail in DEVELOPMENT OF RETINOTECTAL MAPS (q.v.).

Discussion
Current experimental work in axon guidance is dominated by techniques and hypotheses at the molecular level. The data are also
rapidly evolving, with new molecules and mechanisms important
for guidance being discovered at a very fast rate. However, many
fundamental questions remain unresolved, and theoretical models
have the potential to make an important contribution to answering
these questions. What is the minimum gradient steepness detectable
by a growth cone, and how does this vary with the properties of
the receptor-ligand interaction and the internal state of the growth
cone? How is a graded difference in receptor binding internally
converted into a signal for directed movement? How do axons integrate multiple cues? And, perhaps most relevant to human health,
how can regenerating axons be encouraged to grow toward and
reconnect with appropriate targets after injury?
Road Map: Neural Plasticity
Related Reading: Development of Retinotectal Maps

References
Berg, H. C., and Purcell, E. M., 1977, Physics of chemoreception, Biophys.
J., 20:193–219.
Buettner, H. M., 1995, Computer simulation of nerve growth cone filopodial dynamics for visualization and analysis. Cell Motil. Cytoskelet.,
32:187–204.
Gierer, A., 1987, Directional cues for growing axons forming the retinotectal projection, Development, 101:479–489.
Goodhill, G. J., 1998, Mathematical guidance for axons, Trends Neurosci.,
21:226–231. ⽧
Goodhill, G. J., and Richards, L. J., 1999, Retinotectal maps: Molecules,
models, and misplaced data, Trends Neurosci., 22:529–534.
Goodhill, G. J., and Urbach, J. S., 1999, Theoretical analysis of gradient
detection by growth cones, J. Neurobiol., 41:230–241.
Hentschel, H. G. E., and van Ooyen, A., 1999, Models of axon guidance
and bundling during development, Proc. R. Soc. Lond. B, 266:2231–
2238.
Meinhardt, H., 1999, Orientation of chemotactic cells and growth cones:
Models and mechanisms, J. Cell Sci., 112:2867–2874.
Mueller, B. K., 1999, Growth cone guidance: First steps towards a deeper
understanding. Annu. Rev. Neurosci., 22:351–388. ⽧
Sanes, D. H., Reh, T. A., and Harris, W. A., 2000, Development of the
Nervous System, San Diego, CA: Academic Press.
Song, H., and Poo, M-M., 2001, The cell biology of neuronal navigation,
Nature Cell. Biol., 3:E81–E88.
Sperry, R. W., 1963, Chemoaffinity in the orderly growth of nerve fiber
patterns and connections, Proc. Natl. Acad. Sci. USA, 50:703–710.
Tessier-Lavigne, M., and Goodman, C. S., 1996, The molecular biology of
axon guidance, Science, 274:1123–1133. ⽧
Tranquillo, R. T., and Lauffenburger, D. A., 1987, Stochastic model of
leukocyte chemosensory movement, J. Math. Biol., 25:229–262.
Willshaw, D. J., and von der Malsburg, C., 1979, A marker induction mechanism for the establishment of ordered neural mappings: Its application
to the retinotectal problem, Philos. Trans. R. Soc. B, 287:203–243.

144

Part III: Articles

Backpropagation: General Principles
Michael A. Arbib
Introduction
Perceptrons are neural nets that use an error-correction rule to
change the weights of each unit that makes erroneous responses to
stimuli that are presented to the network. As already explained in
PERCEPTRONS, ADALINES, AND BACKPROPAGATION (q.v.) and Section I.3: “Dynamics and Adaptation in Neural Networks,” backpropagation is a family of methods for training a multilayer perceptron, a loop-free network that has its units arranged in layers,
with each unit providing input only to units in the next layer of the
sequence. The first layer comprises input units; there may then be
several layers of trainable “hidden units” carrying an internal representation, and finally there is the layer of output units, also with
trainable synaptic weights.
Rumelhart, Hinton, and Williams (1986) is the most influential
paper on the error backpropagation method, providing a formula
(see the Proposition below) for propagating back the gradient of
error evaluation from a unit to the units that provide its inputs.
Since the formulas involve derivatives, the input and output of each
unit must take continuous values in some range, here taken to be
[0, 1]. The response is a sigmoidal function of the weighted sum.
Werbos (1995) provides a historical perspective on precursors of
their paper. As a specific example of such a precursor, LEARNING
AND STATISTICAL INFERENCE (q.v.) presents a general stochastic
descent on-line learning procedure (Amari, 1967), which, when
applied to the multilayer perceptron, yields the error backpropagation method.
Proposition. Consider a layered loop-free net with error E ⳱
	k(tk ⳮ ok)2, where k ranges over designated output units, and let
the weights wij be changed according to the gradient descent rule
E
Dwij ⳱ ⳮ
⳱ 2
wij

o

兺k (tk ⳮ ok) wkij

Then the weights may be changed inductively, working back from
the output units, by the rule Dwij is proportional to di oj, where
Basis Step: di ⳱ (ti ⳮ oi) f ⬘i for an output unit.
Induction Step: If i is a hidden unit, and if dk is known for all
units that receive unit i’s output, then di ⳱ (	k dkwki) f ⬘i, where k
runs over all units that receive unit i’s output. 䊐
Thus the “error signal” di propagates back layer by layer from
the output units. In 	k dkwki, unit i receives error propagated back
from a unit k to the extent to which i affects k.
The above proposition tells us how to compute Dwij for the online backpropagation algorithm that adjusts the weights in response
to each single input pattern, using the “local error” of the network
with its current weight settings for that input. It does not guarantee
that the above step size is appropriate to reach the minimum, nor
does it guarantee that the minimum, if reached, is global. The backpropagation rule defined by this proposition is, thus, a heuristic
rule, not one guaranteed to find a global minimum. The batch version of the algorithm cycles through a complete training set of
input-output pairs (x1, y1), (x2, y2), . . . , (xN, yN), with gradient
descent applied to the cumulative error of each cycle, until no further changes are required.
As the index to this Handbook attests, backpropagation has been
perhaps the most diversely used adaptive architecture, especially
in technological applications. The purpose of this article is neither
to introduce the basics of backpropagation (again, see PERCEPTRONS, ADALINES, AND BACKPROPAGATION and Section I.3: Dy-

namics and Adaptation in Neural Networks) nor to survey its applications (see SPEECH RECOGNITION TECHNOLOGY for one
example of a careful analysis of the pros and cons of using multilayer perceptrons), but instead to place backpropagation in a
broader context by providing a road map for a number of contributions elsewhere in the Handbook that enrich our basic understanding of this adaptive architecture. The article also assesses the
biological plausibility of backpropagation.

Auto-Encoding
A basic application for backpropagation networks has been to find
compressed representations. In this case, a network with one hidden
layer is trained to become an auto-encoder or auto-associator by
learning the identity function: making the desired states of the N
output units identical to the states of the N input units for each
input-output pair in the training sample. Data compression is
achieved by making the number of hidden units M  N. Moreover,
the features discovered by the hidden units may be useful for processing tasks, such as classification of the input patterns. However,
as shown in UNSUPERVISED LEARNING WITH GLOBAL OBEJCTIVE
FUNCTIONS, it may not be possible to relate the activities of individual hidden units to specific features that may be found by other
means to characterize complicated input patterns. One way to constrain the hidden unit representation is to add extra penalty terms
to the error function. For example, a penalty term on hidden unit
activations can be chosen that causes these units to represent highdimensional data as localized bumps of activity in a lowerdimensional constraint surface. This encourages the hidden units
to form a map-like representation that best characterizes the input.
Other penalty terms lead to other encodings, such as sparse or combinatorial representations (see MINIMUM DESCRIPTION LENGTH
ANALYSIS).
RAAM networks (Pollack, 1990) are three-layer backpropagation networks whose input and output layers are each divided into
regions. The network is trained to “auto-associate,” i.e., to reproduce a given pattern of input on the output layer. The purpose of
this training is to permit condensed, distributed encodings of Ktuples of information (i.e., the subpatterns presented to the K regions of the input layer) to be developed on the hidden layer. Once
such a distributed encoding has been developed for a given K-tuple
of information, that encoding may later be presented as input to a
single region of the input layer, while the remaining input regions
receive similarly derived distributed encodings, so that the network
then develops codes for K-tuples of information. The network may
then be trained to auto-associate on this more complex set of input
information. Iterating the procedure yields codes for K-tuples of Ktuples, and so on, thus making possible condensed distributed encodings for entire tree structures in the RAAM’s hidden layer. SYSTEMATICITY OF GENERALIZATIONS IN CONNECTIONIST NETWORKS
(q.v.) discusses the implications of such techniques for the ability,
or otherwise, of connectionist models to capture human abilities
for symbol processing.

Stochasticity and Plateaus
STOCHASTIC APPROXIMATION AND EFFICIENT LEARNING (q.v.)
notes that both on-line and batch backpropagation seek a weight
vector w that minimizes the error function, but stresses the statistical notion that inputs must follow some probability distribution
so that what we really seek to minimize is the error as averaged

Backpropagation: General Principles
over all examples. But should we average over the few examples
available in the training set, or over all the complete probability
distribution “given by Nature”? The first average is named empirical risk and measures only the training set performance. The second average is called the expected risk and measures the much more
interesting generalization performance (Vapnik, 1998). The Handbook introduces a stochastic gradient descent algorithm in which
each iteration consists of picking a single random example and
updating the weight vector w accordingly. This stochastic gradient
descent does not need to remember which examples were visited
during the previous iterations, making this algorithm suitable for
the on-line adaptation of deployed systems. In such a situation, the
stochastic gradient descent directly optimizes the expected risk,
since the examples are randomly drawn from the “ground truth”
distribution. The stochastic gradient descent can also pick examples
from a finite training set. This procedure optimizes the empirical
risk. The number of iterations is usually larger than the size of the
training set. The examples are therefore presented multiple times
to the network.
LEARNING AND STATISTICAL INFERENCE (q.v.) offers a general
method, called Fisher efficiency, of assessing the success of an
estimator relating input and output patterns. It then notes that, although backpropagation learning has been used widely, it is not
Fisher efficient. Moreover, the method may converge to one of the
local minima of the error landscape, which might be different from
the global minimum. Intriguingly, convergence may be drastically
slow because of “plateaus.” The error decreases quickly at the beginning of learning, but its rate of decrease becomes extremely
slow. After surprisingly many steps, the error again decreases rapidly. This is understood as showing that weights are trapped in a
“plateau” that is not a local minimum but nonetheless provides a
region of weight space that learning takes very long to escape from.
Saad and Solla (1995) used statistical mechanics to show that
plateaus exist because of the “symmetry” in the hidden units: the
output and hence the error measure is invariant under permutations
of hidden units in the multilayer perceptron. Whereas this property
leads to phase transitions in equilibrium batch training (see STATISTICAL MECHANICS OF GENERALIZATION), the effect in on-line
training is that the system approaches a symmetric state from generic initial conditions. STATISTICAL MECHANICS OF ON-LINE
LEARNING AND GENERALIZATION (q.v.) discusses how the properties of such plateaus can be investigated in detail by linearizing
the dynamics close to the fixed point (Biehl, Riegler, and Wöhler,
1996). Figure 1 in STATISTICAL MECHANICS OF ON-LINE LEARNING
AND GENERALIZATION (q.v.) provides a simple example of the
breaking of permutation symmetry during learning, showing a typical learning curve in which the learning process is dominated by
a pronounced plateau state in which hardly any progress is made
while the number of examples increases. Only after an extended
period of time does the system leave the plateau and approach its
asymptotic state exponentially fast. In the case displayed in the
figure, the system is very close to a perfectly symmetric
configuration.
There are various acceleration methods for the backpropagation
learning rule, but they cannot eliminate plateaus. NEUROMANIFOLDS AND INFORMATION GEOMETRY (q.v.) shows that the natural
gradient method (Amari, 1998), based on the Riemannian structure
of a neuromanifold, not only eliminates plateaus but is Fisher
efficient.

Recurrent Neural Networks
A feedforward network is just a static mapping of input vectors to
output vectors, whereas our brain is a high-dimensional nonlinear
dynamical system, replete with loops. This provides one motivation
(another is technological) for the study of learning algorithms for
recurrent neural networks, which have feedback connections and

145

time delays. In a recurrent network, the state of the system can be
encoded in the activity pattern of the units, and a wide variety of
dynamical behaviors can be encoded by the connection weights.
Network dynamics that converge to a minimum of an “energy”
function (see COMPUTING WITH ATTRACTORS) have proved important for associative memory tasks and optimization networks. However, steady-state solutions (fixed-point attractors) are only a limited portion of the capabilities of recurrent networks. A recurrent
network can serve as a sequence recognition system or as a sequential pattern generator. RECURRENT NETWORKS: LEARNING ALGORITHMS (q.v.) reviews the learning algorithms for training recurrent networks, focusing on supervised learning algorithms for
recurrent networks, with only a brief overview of reinforcement
and unsupervised learning algorithms.
Recurrent neural networks use the additional degree of freedom
provided by a priori unlimited processing time in order to map the
information appropriately. For example, simple recurrent networks
(SRNs; Elman, 1990) augments the three-layer backpropagation
network with a supplementary context layer of the same size as the
hidden layer. Reciprocal links between the hidden layer and the
context layer create a loop enabling any activation pattern currently
present on the hidden layer to be merged with the activation pattern
currently present in the context layer, and vice versa. An extension
of the backpropagation algorithm trains these connections as well.
Essentially, the activity in the context and hidden layers may be
seen as an internal state, so that training serves to update both the
definition of a “next-state function” as well as the reading of the
output from the internal state in such a way as to enable the system
to better and better approximate a training set, which now consists
of pairs of input and output sequences, rather than one-shot input
vectors and output vectors CONSTITUENCY AND RECURSION IN
LANGUAGE (q.v.) exemplifies the use of SRNs in connectionist
linguistics.

Other Perspectives
To create a neural network, a designer typically fixes a network
topology and uses training data to tune its parameters, such as connection weights. The designer, however, often does not have
enough knowledge to specify the ideal topology. In the case of a
multilayer perceptron, the only free parameter in “topology space”
is the number of hidden units. Too few hidden units and the current
task is unlearnable; too many units and the network learns the noise
as well as the task relationships. It is thus desirable to learn the
topology from training data as well. LEARNING NETWORK TOPOLOGY (q.v.) looks at learning as a search in the space of topologies
as well as in weight space. In particular, it provides a general measure of the “goodness” of a topology and some search strategies
over the space of topologies to find the best one. This framework
is applied to learning the topologies of both feedforward neural
networks and Bayesian belief nets (see BAYESIAN NETWORKS).
A basic strategy to avoid false minima is Boltzmann learning
(see SIMULATED ANNEALING AND BOLTZMANN MACHINES). Here
the units respond in stochastic fashion to their inputs. The degree
of “stochasticity” is controlled by a parameter T. As T r ⳮ, the
unit becomes deterministic; as T r , the unit becomes very noisy.
T is often referred to as “temperature,” as part of the comparison
of large neural networks with the systems treated by statistical mechanics (see STATISTICAL MECHANICS OF NEURAL NETWORKS).
Convergence to the global optimum is aided by starting at high T
and gradually lowering it—this is the process of “simulated
annealing”—with the intuition being that the initial high noise
“bumps the system out of the high valleys” of the error landscape,
while the eventual low noise allows it to settle in the “low valleys.”
MODULAR AND HIERARCHICAL LEARNING SYSTEMS (q.v.) replaces the training of a single network by the training of a set of
networks that forms a “mixture of experts,” the idea being that each

146

Part III: Articles

network will become expert at processing inputs from a region of
the input space, while a gating network will learn which experts to
rely on for processing a given input. As an alternative to gradient
methods, Jordan and Jacobs (1994) developed an ExpectationMaximization (EM) algorithm (McLachlan and Krishnan, 1997,
give a general treatment of the EM algorithm) that is particularly
useful for models in which the expert networks and gating networks
have simple parametric forms. Each iteration of the algorithm consists of two phases: (1) a recursive propagation upward and downward in the tree of modules to compute posterior probabilities (the
“E step”), and (2) solution of a set of local weighted maximum
likelihood problems at the nonterminals and terminals of the tree
(the “M step”). Jordan and Jacobs (1994) tested this algorithm on
a nonlinear system identification problem (the forward dynamics
of a 4-degrees-of-freedom robot arm) and reported that it converged
nearly two orders of magnitude faster than backpropagation in a
comparable multilayer perceptron network.
GRAPHICAL MODELS: PROBABILISTIC INFERENCE (q.v.) tells us
that many neural network architectures are special cases of the
general graphical model formalism that the article presents. Special
cases of graphical models include essentially all of the models developed under the rubric of “unsupervised learning,” as well as
Boltzmann machines, mixtures of experts, and radial basis function
networks. It is argued that many other neural networks, including
the classical multilayer perceptron of the present article, can be
analyzed profitably from the point of view of graphical models.

Biological Considerations
REINFORCEMENT LEARNING IN MOTOR CONTROL (q.v.) notes the
importance of supervised learning in motor control, but stresses
that reinforcement learning (in which positive reinforcement signals success on a task and increasing negative reinforcement gives
a measure of increasingly poor performance, but where no explicit
error signal for the network’s output units is available) is more
plausible in many situations involving motor learning; and, indeed,
DOPAMINE, ROLES OF (q.v.) shows that certain reinforcement
learning methods seem to fit well with the action of dopamine in
the brain.
However, RECURRENT NETWORKS: NEUROPHYSIOLOGICAL
MODELING (q.v.) argues for the utility of backpropagation as a tool
for studying actual networks in the brain. The argument here is that
backpropagation provides a means for the computational neuroscientist to adjust the parameters within a given neural network architecture to see whether there is indeed a parameter setting (whose
robustness can then be studied) that yields a given type of behavior.
The article presents backpropagation not as a model for biological
learning, simply as an effective method of obtaining a solution.
Biologically plausible learning algorithms will also find similar
solutions, but usually take longer. For example, Mazzoni, Andersen, and Jordan (1991) argued that reinforcement learning gave a
more biologically plausible learning rule than backpropagation in
their study of a network model of cortical area 7a.
But what is the evidence that backpropagation is biologically
implausible? HEBBIAN SYNAPTIC PLASTICITY (q.v.) makes a partial case for biological plausibility. While conceding that there is
no evidence that the backpropagation formula represents actual
brain mechanisms, it summarizes new evidence suggesting that activity in one neuron may affect presynaptic neurons, and even neurons presynaptic to those. One might call this qualitative backpropagation to stress that the evidence says nothing about the
quantitative plausibility of the generalized delta rule. The ability to
patch (make local electrode recordings and current injections) at
different distances from the soma of a biological neuron has suggested that action potentials propagate back from the soma into the
dendrites as well as in the “conventional” direction, from dendrites

to soma (see DENDRITIC PROCESSING). HEBBIAN SYNAPTIC PLAS(q.v.) discusses three distinct mechanisms by which backpropagating spikes can be seen as the “binding signal” emitted by
the soma to modify differentially synapses that are active within a
precise temporal window. Moreover, the study of identified neurons and synapses in low-density hippocampal cultures has revealed extensive but selective spread of both long-term potentiation
(LTP) and long-term depression (LTD) from the site of induction
to other synapses in the network (see Bi and Poo, 2001, for a review). LTD induced at synapses between two glutamatergic neurons can spread to other synapses made by divergent outputs of the
same presynaptic neuron, to synapses made by other convergent
inputs on the same postsynaptic cell, and can even spread in a
retrograde direction to depress synapses afferent to the presynaptic
neuron (the evidence for qualitative backpropagation). In contrast,
LTP can exhibit only lateral spread and backpropagation to the
synapses associated with the presynaptic neuron.

TICITY

Discussion
Backpropagation has provided an effective and widely used architecture for the training of artificial neural networks. We recalled
the generalized delta rule for multilayer perceptrons, illustrated its
utility with two examples of auto-encoder networks, and showed
how the methodology could be extended to recurrent networks.
However, statistical analysis showed that backpropagation has
problems—a particular example being the likelihood of backpropagation training of multilayer perceptrons getting trapped in
plateaus—as well as advantages. We thus provided pointers to stochastic descent methods that avoided these pitfalls, as well as noting extensions of, and alternatives to, backpropagation that can
usefully be added to the repertoire of those who train artificial neural networks.
As for biology, we saw that backpropagation may serve as a
computational tool to estimate the parameters of a particular biological network even though it does not model the actual learning
processes within that network. On the other hand, evidence of
“spike backpropagation” provides inspiration for a family of subtle
new learning rules that allow the activity of a neuron to affect the
neurons presynaptic to its input neurons, but this offers no direct
support for the specific formulas of the generalized delta rule. Finally, it should be noted that the modeling and theory summarized
in this article are based on neurons with sigmoid outputs. Such
units are useful both in artificial neural networks and in connectionist modeling. They can also be considered biological models if
their real-valued output is seen to represent a moving-window
mean of spiking frequency of the biological neurons (see RATE
CODING AND SIGNAL PROCESSING). However, there are cases in
which it seems that a better fit to the biology can be obtained if the
local temporal structure of spikes in the output of each neuron is
taken into account (SPIKING NEURONS, COMPUTATION WITH). This
suggests the importance of seeking to define learning rules that do
take detailed spike placement, rather than local firing rates, into
account. The data reviewed in HEBBIAN SYNAPTIC PLASTICITY
(q.v.) may lead brain modelers in the right direction but, unfortunately, no efficient learning algorithm for networks of spiking neurons, whether biological or not, has yet gained wide acceptance.
Road Map: Learning in Artificial Networks
Background: I.3. Dynamics and Adaptation in Neural Networks; Perceptrons, Adalines, and Backpropagation
Related Reading: Computing with Attractors; Hebbian Synaptic Plasticity;
Learning and Statistical Inference; Recurrent Networks: Learning Algorithms; Statistical Mechanics of On-Line Learning and Generalization;
Stochastic Approximation and Efficient Learning

Basal Ganglia

References
Amari, S., 1967, Theory of adaptive pattern classifiers, IEEE Trans. Elec.
Comp., EC-16:299–307.
Amari, S., 1998, Natural gradient works efficiently in learning, Neural
Computat., 10:251–276.
Biehl, M., Riegler, P., and Wöhler, C., 1996, Transient dynamics of online learning in two-layered neural networks, J. Phys. A, 29:4769.
Bi, G., and Poo, M., 2001, Synaptic modification by correlated activity:
Hebb’s postulate revisited, Annu. Rev. Neurosci., 24:139–166. ⽧
Elman, J. L., 1990, Finding structure in time, Cogn. Sci., 14:179–212. ⽧
Jordan, M. I., and Jacobs, R. A., 1994, Hierarchical mixtures of experts and
the EM algorithm, Neural Computat., 6:181–214.
Mazzoni, P., Andersen, R. A., and Jordan, M. I., 1991, A more biologically
plausible learning rule than backpropagation applied to a network model
of cortical area 7a, Cereb. Cortex, 1:293–307.

147

McLachlan, G. J., and Krishnan, T., 1997, The EM Algorithm and Extensions, New York: Wiley-Interscience.
Pollack, J. B., 1990, Recursive distributed representations, Artif. Intell.,
46:77–105.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J., 1986, Learning internal representations by error propagation, in Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. 1, Foundations, (D. E. Rumelhart, J. L. McClelland, and PDP Research Group,
Eds.), Cambridge, MA: MIT Press, pp. 318–362. ⽧
Saad, D., and Solla, S. A., 1995, On-line learning in soft committee machines, Phys. Rev. E, 52:4225–4243.
Vapnik, V. N., 1998, Statistical Learning Theory, New York: Wiley.
Werbos, P., 1995, Backpropagation: Basics and new developments, in The
Handbook of Brain Theory and Neural Networks (M. A. Arbib, Ed.),
Cambridge, MA: MIT Press, pp. 134–139. ⽧

Basal Ganglia
Tony J. Prescott, Kevin Gurney, and Peter Redgrave
Introduction
Lying on either side of the forebrain/midbrain boundary, at the hub
of the mammalian brain, the basal ganglia are a group of highly
interconnected brain structures with a critical influence over movement and cognition. The importance of these nuclei for a cluster of
human brain disorders, including Parkinson’s disease, Huntington’s disease, and schizophrenia, has produced a century or more
of strong clinical interest, and a prodigious volume of neurobiological research. Given the wealth of relevant data, and a pressing
need for a better functional understanding of these structures, the
basal ganglia provide one of the most exciting prospects for computational modeling of brain function.
This article will begin by summarizing aspects of the functional
architecture of the mammalian basal ganglia and will then describe
the computational approaches that have been developed over the
course of the past decade (see also Houk, Davis, and Beiser, 1995;
Wickens, 1997; Gillies and Arbuthnott, 2000). An important task
for an appraisal of computational models is to provide a framework
for comparing pieces of work that can differ radically in their
breadth of focus, level of analysis, computational premises, and
methodology, and whose relative merits can consequently be difficult to ascertain (I.2. Levels and Styles of Analysis). Here, we
first distinguish between models that attempt to incorporate appropriate biological data (anatomical and/or physiological) and those
that attempt an explanation of function using generic neural network architectures. This review will discuss only those models that
incorporate known neurobiological constraints and will consider
some of the implications for these models of recent biological data.
The models can be divided in two main categories: (1) those that
work at a comparatively low level of detail (membrane properties
of individual neurons and micro-anatomical features) and that restrict themselves to a single component of the basal ganglia nucleus; and (2) those that deal at the “system level” with the basal
ganglia as a whole and/or with their interactions with related structures (e.g., thalamus and cortex). In this article we will also seek
to classify system level models in terms of the primary computational role that is being addressed by the neural substrate.
The neuromodulator dopamine is known to play a vital role in
regulating basal ganglia processing and also in mediating learning
within the basal ganglia. Although some of the likely regulatory
functions of dopamine will be considered in this article, a fuller
discussion of this topic, including hypotheses and models con-

cerned with the role of dopamine in learning from reinforcement,
are the subject of a separate article (DOPAMINE, ROLES OF).

Key Architectural Features
There have been many excellent summaries of the functional anatomy of the basal ganglia (e.g., Mink, 1996; Smith et al., 1998), the
following therefore focuses on those aspects most relevant to understanding the models discussed in this article.
The principle structures of the rodent basal ganglia (Figure 1)
are the striatum (consisting of the caudate, the putamen, and the
ventral striatum), the subthalamic nucleus (STN), the globus pallidus (GP), the substantia nigra (SN), and the entopeduncular nucleus (EP) (homologous to the globus pallidus internal segment in
primates). These structures are massively interconnected and form
a functional subsystem within the wider brain architecture. There
is a growing consensus that the basal ganglia nuclei can be regionally subdivided on the basis of their topographically organized connectivity with each other and with cortical and thalamic regions.
Current views of information processing within the basal ganglia
are heavily influenced by this suggestion of multiple parallel loops
or channels.
The principle input components of the basal ganglia are the striatum and the STN. Afferent connections to both of these structures
originate from virtually the entire brain, including cerebral cortex,
many parts of the brainstem (via the thalamus), and the limbic
system. Input connections provide phasic (intermittent) excitatory
input.
The main output nuclei of the basal ganglia are the substantia
nigra pars reticulata (SNr) and the entopeduncular nucleus (EP).
Output structures provide extensively branched efferents to the
thalamus (which project back to the cerebral cortex), and to premotor areas of the midbrain and brainstem. Most output projections
are normally (tonically) active and inhibitory.
To make sense of the intrinsic connectivity of the basal ganglia
it is important to recognize that the main projection neurons from
the striatum (medium spiny cells) form two widely distributed
populations differentiated by their efferent connectivity and
neurochemistry.
One population comprises neurons with mainly D1-type dopamine receptors and projects to the output nuclei (SNr and EP). In
the prevailing informal model of the basal ganglia (Albin, Young,
and Penney, 1989) this projection constitutes the so-called direct

148

Part III: Articles

Figure 1. Basal ganglia anatomy of the rat: A. Internal pathways. B. External pathways. Excitatory and inhibitory pathways are denoted by solid and gray
lines, respectively; not all connections are shown. See text for key to abbreviations.

pathway to the output nuclei (see Figure 2A). Efferent activity from
these neurons suppresses the tonic inhibitory firing in the output
structures, which in turn disinhibits targets in the thalamus and
brainstem.
A second population of striatal output neurons has predominantly D2-type dopamine receptors. This group projects primarily
to the globus pallidus (GP) whose tonic inhibitory outputs are directed both to the output nuclei (SNr and EP) and to the STN. The
inhibitory projection from D2 striatal neurons constitutes the first
leg of an indirect pathway to the output nuclei. Since this pathway
has two inhibitory links (Striatum-GP, GP-STN), followed by an
excitatory one (STN-EP/SNr), the net effect of striatal activity is

to activate output nuclei, which increases inhibitory control of the
thalamus and brainstem.
The main source of dopamine innervation to the striatum is the
substantia nigra pars compacta (SNc). Interestingly, the D1 and
D2 striatal populations respond differently to dopaminergic transmission, activation of D1 receptors having a predominantly excitatory effect while D2 receptor activation appears to be mainly
inhibitory. This arrangement seems to provide dopaminergic control of a “push/pull” mechanism subserved by the direct (inhibitory) and indirect (net excitatory) basal ganglia pathways. Importantly, a key input to the SNc is from striatal areas known as
striosomes (areas that project to EP/SNr are known as matri-

Figure 2. Functional interpretations of the basal ganglia: (a) Informal models stress the “direct” and “indirect” pathways and leave the functional
consequences of their interactions ill-defined. Other pathways (indicated by
dotted lines) have received less emphasis. (b) An alternative interpretation

arising from computational modeling by Gurney et al. (2001) specifies
specific functional roles for the various intrinsic basal ganglia connections
summarized by the concept of “selection” and “control” pathways. See text
for further explanation.

Basal Ganglia
somes), thus the striatum is a major player in modulating its own
dopaminergic input.
Although the preceding description focuses on pathways originating from the striatum, the STN, though much smaller in size, is
increasingly recognized as a second important input structure
within the basal ganglia functional architecture (see Mink, 1996).
STN’s excitatory outputs project to both the output nuclei (SNr and
EP) and to the intermediary structure GP.
Recent anatomical data by Wu, Richard, and Parent (2000) has
suggested that the “direct pathway” is actually branched with a
significant output going to GP. Other new data shows the existence
of additional inhibitory projections from GP to EP/SNr implying a
multiplicity of indirect pathways (Smith et al., 1998). The proliferation of intrinsic basal ganglia circuitry in recent literature has
highlighted the need for: (1) a radical reinterpretation of basal ganglia functional anatomy; and (2) an increasingly important role for
computational modeling in interpreting the functional properties of
the multiple interconnections and loops within the basal ganglia.

Low-Level Models of Individual Basal Ganglia Nuclei
In contrast to the diverse nature of the brain regions projecting to
the mammalian striatum, its internal organization appears surprisingly homogeneous. This finding offers hope that an understanding
of striatal functioning in one local area could generalize across
much of the entire structure. At any given moment the majority of
striatal cells are in an inactive “down state,” and can only be triggered into an active “up state” (in which they can fire action potentials) by a significant amount of coincident input. Since each
neuron has a wide dendritic fan-in (with up to 30,000 synapses),
but only a few synapses with any single source neuron, it must
receive coincident signals from a large population of inputs to become active (see Wilson in Houk et al., 1995). This organization
suggests that striatal spiny neurons may act as “context-specific
filters,” each one configured to match a specific pattern of activity
distributed across multiple loci in one or more brain areas (Mink,
1996).
Recent studies have provided evidence for local inhibition within
the striatum mediated either via local interneurones or by reciprocal
inhibitory networks among the output cells themselves (see Oorschot et al. in Nicholson and Faull, 2002). Wickens and colleagues
(see Wickens, 1997) have investigated the dynamics of such local
neighborhoods of striatal neurons using network models. Under
varying assumptions of topology and size, they concluded that reciprocal inhibition will usually lead to a network dynamic of competition, that is, the most active neurons will tend to suppress activity in their less active neighbors. This research also examined
the effects of simulated dopamine inputs, showing that under circumstances of low dopamine, the dynamic of the network changes
from competition to coactivation (where activity is uniformly distributed within the local population of neurons), a pattern that could
provide a model for the muscular rigidity seen in dopamine-deficient Parkinson’s patients. Using another variant of this model,
Wickens explored the implications of dendritic asymmetries based
on those observed in the early stages of Huntington’s disease.
Simulation of asymmetric interconnectivity generated slow traveling waves of activity (where normal symmetric configurations
produce stationary activity patterns), suggesting that a similar abnormal network dynamic may underlie the sudden involuntary
movements seen in Huntington’s patients.
Apart from the striatum, relatively little attention has been given
to modeling intrinsic processing within basal ganglia nuclei. One
interesting exception is the work by Gillies and Willshaw (see Gillies and Arbuthnott, 2000) on a model of the STN. Having incorporated key physiological and anatomical properties, they showed
that the widespread excitatory interconnectivity between STN neurons allows focused input to produce a widely distributed pulse of

149

excitation to SNR and EP. Given that the output of the basal ganglia
is largely inhibitory, phasic STN activity could serve to break established patterns of activity in basal ganglia targets thereby acting
as a form of interrupt or “reset” mechanism.
The preceding models have as their starting point a wealth of
low-level biological constraints with the rationale that the resulting
model behavior must approximate observed biological data. Nevertheless, since the phenomena discussed are related to the ability
to resolve localized competitions (in striatum) and to interrupt ongoing behaviors (STN), we would argue that these models may be
thought of as addressing components of the overall computational
problem of action selection (see further discussion later in this
article).

System Level Models of Basal Ganglia Circuits
and External Functional Loops
Most of the effort so far directed at basal ganglia modeling has
been concerned with simulating interactions between the various
basal ganglia structures, and between the basal ganglia and other
key brain regions such as cortex, thalamus, and brainstem. A comparatively high level of abstraction is usually adopted in this work,
in which components of the basal ganglia are decomposed into
functional units (e.g., multiple parallel channels). Most work to
date has focused on a number of related computational hypotheses—that the basal ganglia function to (1) regulate the degree of
action gating, (2) select between competing actions, (3) sustain
working memory representations, and (4) store and enact sequences
of behavior. These ideas will be the main focus of the remaining
discussion.

Action Gating
A key function of the striatum is to provide intermittent, focused
inhibition (via the “direct pathway”) within output structures that
otherwise maintain inhibitory control over motor/cognitive systems
throughout the brain. This architecture strongly suggests that a core
function of the basal ganglia is to gate the activity of these target
systems via the mechanism of disinhibition. Many basal ganglia
models employ selective gating, however, that of Contreras-Vidal
and Stelmach (1995) is particularly interesting as it explores gating
operations in both normal and dysfunctional model variants. These
authors coupled a simulation of basal ganglia intrinsic circuitry to
a neural network that computed arm movements. Excitatory striatal
input resulted in a smoothly varying signal to thalamic targets that
provided the “Go” signal for the motor command, and also set its
overall velocity. The time taken to execute movements decreased
with increasing basal ganglia input thereby matching the results of
striatal microstimulation studies. A “dopamine depleted” version
of the model exhibited akinesia and bradykinesia similar to that
observed in Parkinson’s disease.

Selecting Between Competing Actions
The proposal that the basal ganglia act to resolve action selection
competitions is based on a growing consensus that a key function
of these structures is to arbitrate between sensorimotor systems
competing for access to the final common motor path. A computational hypothesis developed from this idea relies on the premise
that afferent signals to the striatum encode the salience of “requests
for access” to the motor system (Redgrave, Prescott, and Gurney,
1999). Multiple selection mechanisms embedded in the basal ganglia could resolve conflict between competitors and provide clean
and rapid switching between winners. First, the up/down states of
the striatal neurones may act as a first pass filter to exclude weakly
supported “requests.” Second, local inhibition within the striatum
could selectively enhance the activity of the most salient channels.

150

Part III: Articles

Third, the combination of focused inhibition from striatum with
diffuse (divergent) excitation from STN could operate as a feedforward, off-center/on-surround network across the basal ganglia
as a whole (see Mink, 1996). Last, local reciprocal inhibition within
the output nuclei could sharpen up the final selections.
Using the action selection hypothesis as an organizing principle,
Gurney, Prescott, and Redgrave (2001) have proposed a reinterpretation of basal ganglia functional anatomy in which the direct/
indirect classification is replaced by a new functional grouping
based on selection and control circuits (Figure 2B). Specifically,
the focused D1 inhibitory pathway from striatum to EP/SNr (originally the “direct pathway”), together with a diffuse excitatory pathway from STN to EP/SNr, form a primary feedforward selection
circuit. A second group of intrinsic connections centered on the GP
acts as a control circuit to regulate the performance of the main
selection mechanism. Analytical and simulation studies of this
model suggest two likely functional roles for this control circuit.
First, the inhibition of STN by GP constitutes a negative feedback
path that automatically scales the excitatory output of the STN with
the number of channels. Second, GP inhibition of EP/SNr forms
part of a mechanism that supports dopaminergic regulation of selection. Specifically, increased dopamine in these circuits promotes
“promiscuous” selection in which channels are more easily disinhibited, while reduced dopamine results in a “stiffer” competition
in which there are fewer winners and higher levels of general target
inhibition. The adequacy of this model has been tested by embedding it in the control architecture of a mobile robot equipped with
a small repertoire of animal-like behaviors (see Prescott, Gurney
et al. in Nicholson and Faull, 2002). This work confirmed that the
simulated basal ganglia can provide effective action selection in a
real-world context requiring appropriate and timely behavioral
switching. The robot model also provided an insight into the emergent consequences of abnormal dopamine modulation of action
selection. For instance, and reminiscent of some motor symptoms
of Parkinson’s disease, reduced dopamine was found to cause failures to select appropriate behavior or to complete behaviors once
selected.
An earlier model of the basal ganglia proposed by Berns and
Sejnowski (1995) shared the “action selection” premise of Gurney
et al., but emphasized possible timing differences between the direct and indirect pathways in a model that included just the feedforward intrinsic basal ganglia connections. An interesting feature
of this model is that it incorporated a version of the dopamine
hypothesis for reinforcement learning (DOPAMINE, ROLES OF) as a
means for adaptively tuning the selection mechanism.

targets so that the target location was maintained in the corticothalamic circuits until the saccade was made.
In our view, the selection and maintenance of specific working
memory items can be viewed as an extension of action selection
by the basal ganglia to the domain of cognition (selecting from a
range of potential cognitive representations those which are to be
sustained as working memory). It is interesting to speculate that
deficits in this system may underlie the disorders of thought associated with schizophrenia, attention-deficit disorder, and obsessivecompulsive disorder.

Sequence Processing
A plausible use for the working memory mechanism outlined previously would be to link successful selections during the development of behavioral/cognitive sequences. This idea has therefore
become a central theme in a number of basal ganglia models. According to Beiser and Houk (1998), sequence encoding can be
viewed as the task of translating a temporal ordering into a spatial
pattern of neural activity. They propose that the initial item in a
sequence selects the basal ganglia loop whose striatal neurons are
most attuned to that context. When this channel is disinhibited, the
item then becomes encoded as a self-sustaining pattern of corticothalamic activity. Later sequence elements are recorded in an identical way, except that, as each new item is added, the cortical activity triggered by its predecessors becomes part of its context
(thereby implicitly encoding its position in the temporal order).
Rather than recording sequences as spatially distributed patterns,
Fukai (1999) has suggested a form of cortical short-term memory
for sequences that uses patterns of fast (gamma) and slow (theta)
oscillatory activity. Reciprocal inhibition between striatal neurons
would allow the basal ganglia to select the first item in such a
sequence, while other striatal neurons (and their corresponding
basal ganglia outputs) would be recruited to maintain the selection
of that item for as long as required. Finally, an excitatory burst
from STN terminates the movement and signals the transition to
the next item in the sequence. Although differing considerably in
detail, these two models share the premise that the basal ganglia is
specialized to “unpack” a cortical representation of sequential behavior by selectively gating each of the component movements.
Sequence learning is another important theme in basal ganglia
modeling. For instance, Dominey, Arbib, and Joseph (1995) have
extended their model of delayed saccade control (described previously) to include a mechanism for associative and SEQUENCE
LEARNING (q.v.) based, again, on the hypothesis that dopamine
provides a reinforcement learning signal.

Sustaining Working Memory Representations

Discussion

The relationship between basal ganglia and cortex is characterized
by relatively segregated parallel loops, in which cortical projections
to the striatum are channeled through basal ganglia outputs to the
thalamus and then back to their cortical areas of origin. The thalamic nuclei in this circuit also have reciprocal, net-excitatory, connections to their cortical targets. This architecture suggests a pattern
of cortical-thalamic activity which, once initiated by disinhibitory
signals from the basal ganglia, could be sustained indefinitely. Several authors have proposed that this circuit could act as a working
memory store (see Houk et al., 1995). An example of this is provided by Arbib and Dominey’s model of basal ganglia control of
the primate saccadic eye movement system (see article in Houk et
al., 1995). These authors modeled an experimental task in which a
monkey is required to make a saccade to a remembered target location. They simulated circuits in which cortical cells in the frontal
eye fields were activated by the target, which, in turn, excited a
population of striatal neurons specialized for delayed saccades. The
basal ganglia loops involving these cells disinhibited their thalamic

The preceding summary demonstrates that basal ganglia modeling
is still at the stage of exploring the space of alternative hypotheses,
seeking to operationalize theoretical proposals while trying to
match known neurobiological constraints. As a result, there is now
a candidate set of “global” basal ganglia functions whose computational requirements we are beginning to understand. It remains
to be seen to what extent proposed functions are mutually exclusive
and to what extent one may be subsumed within another (for instance, action gating can be viewed as an essential component of
action selection). Similar considerations apply when appraising
models directed at different levels of basal ganglia function. For
example, lower-level models of the striatum or STN may, in the
future, be imported as fully functional components into higherlevel models. However, some system level models are clearly in
direct competition with each other as they ascribe different functional roles to local pathways and nuclei. We anticipate that models
based on correct computational assumptions will find it comparatively easy to incorporate new biological constraints, which in most

Bayesian Methods and Neural Networks
cases will improve their accuracy. In contrast, models making mistaken functional assignments will find it increasingly difficult to
incorporate additional biological data while maintaining their functionality. Future work will therefore require ever-closer links between neurobiologists and modelers to refine the models, to formulate questions based on function, and to test the interesting and
unforeseen predictions that can emerge from modeling studies.
Road Maps: Mammalian Brain Regions; Mammalian Motor Control
Related Reading: Action Monitoring and Forward Control of Movements;
Arm and Hand Movement Control; Dopamine, Roles of; Motor Control,
Biological and Theoretical; Reinforcement Learning in Motor Control

References
Albin, R. L., Young, A. B., and Penney, J. B., 1989, The functional anatomy
of basal ganglia disorders, Trends Neurosci., 12(10):366–375.
Beiser, D. G., and Houk, J. C., 1998, Model of cortical-basal ganglionic
processing: Encoding the serial order of sensory events, J. Neurophysiol., 79(6):3168–3188.
Berns, G. S., and Sejnowski, T. J., 1995, How the basal ganglia make
decisions, in The Neurobiology of Decision Making (A. Damasio, H.
Damasio, and Y. Christen, Eds.), Berlin: Springer-Verlag, pp. 101–113.
Contreras-Vidal, J. L., and Stelmach, G. E., 1995, A neural model of basal
ganglia-thalamocortical relations in normal and Parkinsonian movement,
Biol. Cybernetics, 73(5):467–476.

151

Dominey, P., Arbib, M., and Joseph, J.-P., 1995, A model of corticostriatal
plasticity for learning oculomotor associations and sequences, J. Cognit.
Neurosci., 7(3):311–336.
Fukai, T., 1999, Sequence generation in arbitrary temporal patterns from
theta-nested gamma oscillations: A model of the basal ganglia-thalamocortical loops, Neural Networks, 12(7–8):975–987.
Gillies, A., and Arbuthnott, G., 2000, Computational models of the basal
ganglia, Movement Disorders, 15(5):762–770. ⽧
Gurney, K., Prescott, T. J., and Redgrave, P., 2001, A computational model
of action selection in the basal ganglia. I, II, Biological Cybernetics,
84(6):401–423.
Houk, J. C., Davis, J. L., and Beiser, D. G., 1995, Models of Information
Processing in the Basal Ganglia, Cambridge, MA: MIT Press. ⽧
Mink, J. W., 1996, The basal ganglia: Focused selection and inhibition of
competing motor programs, Progr. Neurobiol., 50(4):381–425. ⽧
Nicholson, L. F. B., and Faull, R. L. M., 2002, Basal Ganglia VII, New
York: Plenum Press.
Redgrave, P., Prescott, T., and Gurney, K., 1999, The basal ganglia: A
vertebrate solution to the selection problem? Neuroscience, 89:1009–
1023.
Smith, Y., Bevan, M. D., Shink, E., and Bolam, J. P., 1998, Microcircuitry
of the direct and indirect pathways of the basal ganglia, Neuroscience,
86:353–387. ⽧
Wickens, J., 1997, Basal ganglia: Structure and computations. NetworkComputation in Neural Systems, 8(4):R77–R109. ⽧
Wu, Y., Richard, S., and Parent, A., 2000, The organization of the striatal
output system: a single-cell juxtacellular labeling study in the rat, Neurosci. Res., 38:49–62.

Bayesian Methods and Neural Networks
David Barber
Introduction
An attractive feature of artificial neural networks is their ability to
model highly complex, nonlinear relationships in data. However,
choosing an appropriate neural network model for data is compounded by the difficulty of assessing the network’s complexity.
Since we are rarely certain about either our data measurements or
model beliefs, a natural framework is to use probabilities to account
for these uncertainties. How can we combine our data observations
with these modeling uncertainties in a consistent and meaningful
manner? The Bayesian approach provides a consistent framework
for formulating a response to these difficulties, and is noteworthy
for its conceptual elegance (Box and Tiao, 1973; Berger, 1985;
MacKay, 1992). The fundamental probabilistic relationship required for inference is the celebrated Bayes rule, which, for general
events A, B, C, is
p(B|A, C)p(A|C)
p(A|B, C) ⳱
p(B|C)

(1)

It is convenient to think of different levels of uncertainty in formulating a model. At the lowest level, we may assume that we
have the correct model but are uncertain as to the parameter settings
h for this model. This assumption details how observed data are
generated, p(data|h, model). The task of inference at this level is to
calculate the posterior distribution of the model parameter. Using
Bayes’s rule, this is
p(h|data, model) ⳱

p(data|h, model) p(h|model)
p(data|model)

(2)

Thus, if we wish to infer model parameters from data, we need
two assumptions: (1) how the observed data are generated under
the assumed model, or the likelihood p(data|h, model), and (2) beliefs about which parameter values are appropriate before the data

have been observed, or the prior p(h|model). (The denominator in
Equation 2 is the normalizing constant for the posterior and plays
a role in uncertainty at the higher model level.) That these two
assumptions are required is an inescapable consequence of Bayes’s
rule, and forces the Bayesian to lay bare all necessary assumptions
underlying the model.

Coin Tossing Example
Let h be the probability that a coin will land heads up. An experiment yields the data, D ⳱ {h, h, t, h, t, h, . . . }, which contains
H heads and T tails in H Ⳮ T flips of the coin. What can we infer
about h from these data? Assuming that each coin is flipped independently, the likelihood of the observed data is
p(D|h, model) ⳱ hH(1 ⳮ h)T

(3)

A standard approach in the statistical sciences is to estimate h by
maximizing the likelihood, hML ⳱ arg maxh p(D|h, model). This
approach is non-Bayesian, since it does not require the specification
of a prior. Consequently, theories that deal with uncertainty in ML
estimators are primarily concerned with the data likelihood, and
not directly with posterior parameter uncertainty (see LEARNING
AND GENERALIZATION: THEORETICAL BOUNDS). In the Bayesian
approach, however, we need to be explicit about our prior beliefs
p(h|model). These are updated by the observed data to yield the
posterior distribution
p(h|D, model)  hH(1 ⳮ h)Tp(h|model)

(4)

The Bayesian approach is more flexible than the maximum likelihood approach since it allows (indeed, instructs) the user to calculate the effect that the data have in modifying prior assumptions
about which parameter values are appropriate. For example, if we
believe that the coin is heavily biased, we may express this using

152

Part III: Articles

the prior distribution in Figure 1A. The likelihood as a function of
h is plotted in Figure 1B for data containing 13 tails and 12 heads.
The resulting posterior (Figure 1C) is bimodal, but less extreme
than the prior. It is often convenient to summarize the posterior by
either the maximum a posteriori (MAP) value or the mean, h̄ ⳱

hp(h|D)dh. Such a summary is not strictly required by the Bayesian framework, and the best choice of how to summarize the posterior depends on other loss criteria (Berger, 1985).

Model Comparison and Hierarchical Models
The preceding discussion showed how we can use the Bayesian
framework to assess which parameters of a model are a posteriori
appropriate, given the data at hand. We can carry out a similar
procedure at a higher, model level to asses which models are more
appropriate fits to the data. In general, the model posterior is given
by
p(M|D) ⳱

p(D|M)
123

p(M) /p(D)
123

If the model is parameterized by some unknown variable h, we
need to integrate this out to calculate the model likelihood

冮 p(D|h, M)p(h|M)dh

Bayesian Regression
Neural networks are often applied to a regression in which we wish
to infer an unknown input-output mapping on the basis of observed
data D ⳱ {(x l, t l), l ⳱ 1, . . . P}, where (x l, t l) represents an
input-output pair. For example, fit a function to the data in Figure
2A. Since there is the possibility that each observed output t l has
been corrupted by noise, we would like to recover the underlying
clean input-output function. We assume that each (clean) output is
generated from the model f (x; w), where the parameters w of the
function f are unknown, and that the observed outputs t l are generated by the addition of noise g to the clean model output,
t ⳱ f (x; w) Ⳮ g

(5)

Model likelihood Model prior

p(D|M) ⳱

intuition would suggest. If we desired, we could continue in this
way, forming a hierarchy of models, each less constrained than the
submodels it contains.

(8)

If the noise is Gaussian distributed, g  N(0, r ), the model M
generates an output t for input x with probability
2

(6)

Comparing two competing model hypotheses, M1 and M2, is
straightforward:
p(M1|D)
p(D|M1) p(M1)
⳱
p(M2|D)
p(D|M2) (M2)
123

(7)

Bayes factor

In the coin example, we can use this to compare the biased coin
hypothesis (model M1 with prior given in Figure 1A) with a less
unbiased hypothesis formed by using a Gaussian prior p(h|M2) with
mean 0.5 and variance 0.12 (model M2). This gives a Bayes factor
p(D|M1)/p(D|M2)  0.00018. If we have no prior preference for
either model M1 or M2, the data more strongly favor model M2, as

Figure 1. Coin tossing. A, The prior: this indicates our belief that the coin
is heavily biased. B, The likelihood after 13 tails and 12 heads are recorded,
hML ⳱ 0.48. C, The posterior: the data have moderated the strong prior
beliefs, resulting in a posterior less certain that the coin is biased. hMAP ⳱
0.25, h̄ ⳱ 0.39.

Figure 2. Along the horizontal axis we plot the input x and along the
vertical axis the output t. A, The raw input-output training data. B, Prediction using regularized training and fixed hyperparameters. C, Prediction
with error bars, using ML-II optimized hyperparameters.

Bayesian Methods and Neural Networks

冦

冧冫冪2pr

1
p(t|w, x, M) ⳱ exp ⳮ 2 (t ⳮ f (x; w))2
2r

2

(9)

If we assume that each data input-output pair is generated identically and independently from the others, the data likelihood is
P

p(D|w, M) ⳱

兿

p(t l|w, x l, M)

(10)

l⳱1
1

P

1

b
log p(w|D, M) ⳱ ⳮ
2

兺l (t l ⳮ f (x l; w))2

Ⳮ log p(w|M) Ⳮ

P
logb Ⳮ const.
2

(11)

where b ⳱ 1/r2. Note the similarity between Equation 11 and the
sum square regularized training error used in standard approaches
to training neural networks (see GENERALIZATION AND REGULARIZATION IN NONLINEAR LEARNING SYSTEMS and Bishop, 1995). In
the Bayesian framework, we can motivate the choice of a sum
square error measure as equivalent to the assumption of additive
Gaussian noise. Typically, we wish to encourage smoother functions so that the phenomenon of overfitting is avoided. One approach to solving this problem is to use a regularizer penalty term
to the training error. In the Bayesian framework, we use a prior to
achieve a similar effect. In principle, however, the Bayesian should
make use of the full posterior distribution, not just a single weight
value. In standard neural network training, it is good practice to
use committees of networks, rather than relying on the prediction
of a single network (Bishop, 1995). In the Bayesian framework,
the posterior automatically specifies a committee (indeed, a distribution) of networks, and the importance attached to each committee
member’s prediction is simply the posterior probability of that network’s weight.

Radial Basis Functions and Generalized Linear Models
Generalized linear models have the form
f (x; w) ⳱

兺i wi␾i(x)  wTU(x)

(12)

Such models have a linear parameter dependence, but nevertheless
represent a nonlinear input-output mapping if the basis functions
␾(x), i ⳱ 1, . . . , k are nonlinear. Radial basis functions (see
RADIAL BASIS FUNCTION NETWORKS) are an example of such a
network (Bishop, 1995). A popular choice is to use Gaussian basis
functions ␾i(x) ⳱ exp(ⳮ(x ⳮ li)2/(2k2)). In this discussion, we
will assume that the centers li are fixed, but that the width of the
basis functions k is a hyperparameter that can be adapted. Since
the output is linearly dependent on w, we can discourage extreme
output values by penalizing large weight values. A sensible weight
prior is thus

␣
k
log p(w|␣) ⳱ ⳮ wTw Ⳮ log ␣ Ⳮ const.
2
2

(13)

Under the Gaussian noise assumption, the posterior distribution
is
P

b
log p(w|C, D) ⳱ ⳮ 兺 (t l ⳮ wTU(x))2
2 l⳱1
␣
ⳮ wTw Ⳮ const.
2

where C represents the hyperparameter set {␣, b, k}. (We drop the
fixed model dependency wherever convenient.) The weight posterior is therefore a Gaussian, p(w|C, D) ⳱ N(w̄, S), where

(14)

P

冢

S ⳱ ␣I Ⳮ b

ⳮ1

兺 U(x l)UT(x l)冣
l⳱1

P

w̄ ⳱ bS

兺

t lU(x l)

(15)

l⳱1

P

(Strictly speaking, we should write p(t , . . . , t |w, x , . . . , x , M)
on the left-hand side of Equation 10. However, since we assume
that the training inputs are fixed and non-noisy, it is convenient
and conventional to write p(D|w, M).) The posterior distribution
p(w|D, M)  p(D|w, M)p(w|M) is

153

The mean predictor is straightforward to calculate: f (x)  
f (x;
w)p(w|D, C)dw ⳱ w̄TU(x). Similarly, error bars are straightforward, var( f (x)) ⳱ U(x)TSU(x) (predictive standard errors are
given by 冪var( f ) Ⳮ r2). In Figure 2B, we show the mean prediction on the data in Figure 2A using 15 Gaussian basis functions
with width k ⳱ 0.03 spread out evenly over the input space. We
set the other hyperparameters to be b ⳱ 100 and ␣ ⳱ 1. The
prediction severely overfits the data, a result of poor choice of
hyperparameters.

Determining Hyperparameters: ML-II
How would the mean predictor be calculated if we were to include
the hyperparameters C as part of a hierarchical model? Formally,
this becomes

冮 f (x; w)p(w, C|D)dwdC
⳱ 冮 冦冮 f (x; w)p(w|C, D)dw冧p(C|D)dC

f̄ (x) ⳱

(16)

The term in curly brackets is the mean predictor for fixed hyperparameters. We therefore weigh each mean predictor by the posterior probability of the hyperparameter p(C|D). Equation 16 shows
how to combine different models in an ensemble—each model prediction is weighted by the posterior probability of the model. There
are other non-Bayesian approaches to model combinations in which
the determination of the combination coefficients is motivated heuristically (see ENSEMBLE LEARNING).
Provided the hyperparameters are well determined by the data,
we may instead approximate the above hyperparameter integral by
finding the MAP hyperparameters C* ⳱ arg maxC p(C|D). Since
p(C|D) ⳱ p(D|C)p(C)/p(D), if the prior belief about the hyperparameters is weak (p(C)  const.), we can estimate the optimal hyperparameters by optimizing the hyperparameter likelihood
p(D|C) ⳱

冮 p(D|C, w)p(w|C)dw

(17)

This approach to setting hyperparameters is called ML-II (Berger, 1985; Bishop, 1995) and assumes that we can calculate the
integral in Equation 17. In the case of GLMs, this involves only
Gaussian integration, giving
P

2 log p(D|C) ⳱ ⳮb

兺

(t l)2 Ⳮ dTSⳮ1d ⳮ log|S|

l⳱1

Ⳮ k log ␣ Ⳮ P log b Ⳮ const.

(18)

where d ⳱ b	lU(x l)t l. Using the hyperparameters ␣, b, k that
optimize the above expression gives the results in Figure 2C, where
we plot both the mean predictions and standard predictive error
bars. This solution is more acceptable than the previous one in
which the hyperparameters were not optimized, and demonstrates
that overfitting is avoided automatically. A non-Bayesian approach
to model fitting based on minimizing a regularized training error
would typically use a procedure such as cross-validation to determine the regularization parameters (hyperparameters). Such approaches require the use of validation data (Bishop, 1995). An advantage of the Bayesian approach is that hyperparameters can be

154

Part III: Articles

set without the need for validation data, and thus all the data can
be used directly for training.

Relation to Gaussian Processes
The use of GLMs can be difficult in cases where the input dimension is high, since the number of basis functions required to cover
the input space fairly well grows exponentially with the input dimension—the so-called curse of dimensionality (Bishop, 1995). If
we specify n points of interest xi, i 僆 1, . . . n in the input space,
the GLM specifies an n-dimensional Gaussian distribution on the
function values f1, . . . , fn with mean f i ⳱ w̄TU(xi) and covariance
matrix with elements cij ⳱ c(xi, xj) ⳱ U(xi)TRU(xj) (see GAUSSIAN
PROCESSES). The idea behind a GP is that we can free ourselves
from the restriction of choosing a covariance function c(xi, xj) of
the form provided by the GLM prior; any valid covariance function
can be used instead. Similarly, we are free to choose the mean
function f i ⳱ m(xi). A common choice for the covariance function
is c(xi, xj) ⳱ exp (ⳮ|xi ⳮ xj|2). The motivation is that the function
space distribution will have the property that for inputs xi and xj,
which are close together, the outputs f (xi) and f (xj) will be highly
correlated, ensuring smoothness. This is one way of avoiding the
curse of dimensionality, since the matrix dimensions depend on the
number of training points, and not on the number of basis functions
used. However, for problems with a large number of training
points, computational difficulties can arise, and approximations
again need to be considered.

Multilayer Perceptrons
Consider the case of a single hidden layer neural network
H

f (x; w) ⳱

兺 vig(xTui Ⳮ bi)
i⳱1

(19)

where g(x) is a nonlinear sigmoidal transfer function, for example
g(x) ⳱ 1/(1 Ⳮ exp(ⳮx)). The set of all weights (parameters), including input-hidden weights ui, biases bi, and hidden-output
weights vi, is represented by the vector w. If the weights are small,
the network function f will be smooth, since only the near linear
regime of the transfer function g will be accessed. An appropriate
prior to control complexity is therefore

␣
k
log p(w|␣) ⳱ ⳮ wTw Ⳮ log␣ Ⳮ const.
2
2

(20)

where k ⳱ dim(w). For the moment, we will assume that we know
the value of the parameter ␣. This gives the weight posterior as
P

b
log p(w|␣, b, D) ⳱ ⳮ 兺 (t l ⳮ f (x l; w))2
2 l⳱1
␣
ⳮ wTw Ⳮ const.
2

Figure 3. The raw input-output training data, with mean Bayesian MLP
predictions (solid curve) and standard error bars (dashed curves). Note how
the error bars increase away from the data.

f̄ (x) ⳱

冮 f (x; w)p(w|C, D)dw

(22)

is difficult. An approximate solution is provided by Monte Carlo
sampling (Bishop, 1995; Neal, 1996):

冮

f (x; w)p(w|C, D)dw 

1
L

L

兺

f (x; wi)

(23)

i⳱1

where the sample weights wi are drawn from the posterior distribution. In principle, this procedure is exact in the limit L r . The
great difficulty, however, is in constructing a finite, representative
set of samples {wi}, and it is easy to remain trapped in unrepresentative parts of the posterior distribution (Neal, 1996).
Consider the problem of drawing samples from a general distribution p(x)  w(x) (Figure 4). Let xold be a sample point from p(x).
We propose a new sample point xnew ⳱ xold Ⳮ g where each
element gi is sampled from a zero-mean Gaussian distribution with
variance s2. We accept xnew if w(xnew)  w(xold), since the new
candidate sample point is more likely than the old sample point.
However, this does not constitute a valid sampling scheme since
we only accept increasingly likely points, targeting therefore only
the modes of the distribution. To correct for this, we accept a less
likely candidate with probability w(xnew)/w(xold). This valid sampling scheme is called the Metropolis method and forms the basis
for many generalizations (Neal, 1993, 1996).
In high dimensions, Metropolis sampling can be inefficient,
since it is unlikely that testing a new point a long way from the
current sample point will result in a more likely point (if you stand
on a mountain and jump, it is more likely that you will end up at
a point lower than at your current point). Thus, only very small

(21)

where b ⳱ 1/r2. In Figure 3 we show the result of using a sixhidden-unit network to fit the training data in Figure 3. With ␣ ⳱
0.1 and b ⳱ 1,000, we drew a number of weight vectors wl, l ⳱
1, . . . , 15, from the weight posterior p(w|D), Equation 21 and
considered the corresponding functions f (x; wl). The mean and
standard error bars calculated from these samples are plotted in
Figure 3. How these samples are obtained is discussed later. Note
how the error bars automatically increase in regions of low data
density.

Monte Carlo Sampling
In general, the posterior distribution p(w|C, D) is non-Gaussian,
and the integration required over the weight space to find, for example, the mean predictor

Figure 4. Metropolis Sampling from p(x)  w(x). Let xold be a sample from
the distribution p(x). We propose a new candidate xnew by sampling from
a Gaussian around xold with width s. More likely candidates such as xa are
accepted. Less likely candidates such as xb are accepted with probability
w(xb)/w(xold).

Bayesian Methods and Neural Networks

155

jumps will be accepted in high-dimensional spaces, and many samples are required to form a good representation of the distribution.
The hybrid Monte Carlo scheme attempts to improve sampling
efficiency and allow larger jumps by exploiting gradient information about the distribution and has been successfully employed in
Bayesian neural networks (Neal, 1996).

The Kullback Leibler divergence is a measure of the difference
between two probability distributions p(x) and q(x) (Cover and
Thomas, 1991)

Laplace’s Method

This has the advantageous properties KL ⱖ 0 and KL ⳱ 0 if and
only if p  q. Consider the KL divergence

Although sampling techniques can be attractive, convergence to a
representative set of samples is difficult to assess and can be very
slow. Laplace’s method is a perturbation technique motivated by
the fact that as the number P of training data points is increased,
the posterior distribution typically approaches a Gaussian (Walker,
1969) whose variance goes to zero in the limit P r  (we leave
aside here the issues of inherent network symmetries). In order to
calculate this Gaussian approximation, we consider the posterior
distribution, Equation 21:
p(w|D, C)  exp (ⳮ␾(w))

(24)

and expand ␾ around a mode of the distribution, w* ⳱ arg min
␾(w),

␾(w)  ␾(w*) Ⳮ 1⁄2 (w ⳮ w*)TH(w ⳮ w*)

(25)

H ⳱ 䉮䉮␾(w)|w*

(26)

where

is the local Hessian matrix. This local expansion defines a Gaussian
approximation
p(w|D, C) 

|H|1/2
exp {1⁄2 (w ⳮ w*)TH(w ⳮ w*)}
(2p)k/2

(27)

The expected value of f (x; w) as required in Equation 22 can be
evaluated by making a further local linearization of the function
f (•, w) around the point w*. In a practical implementation, a standard nonlinear optimization algorithm such as conjugate gradients
is used to find a mode w* of the log posterior distribution (Bishop,
1995).

Determining Hyperparameters
So far we have assumed that the hyperparameters of the MLP are
fixed. In a fully Bayesian treatment we would define prior distributions of the hyperparameters, and then integrate them out. Since
exact integration is analytically intractable, we can use ML-II to
estimate specific values for the hyperparameters by maximizing the
marginal likelihood P(D|C) (Equation 17) with respect to C. Using
MLPs, the integrand in Equation 17 is non-Gaussian and p(D|C)
needs to be approximated. This can be achieved using Laplace’s
method by locally expanding the integral to second order in the
weights. This leads to simple reestimation formulas for the hyperparameters expressed in terms of the eigenvalue/eigenvector decomposition of the Hessian matrix. This treatment of hyperparameters is called the evidence framework (MacKay, 1995) and
involves alternating the optimization of w (mode finding) for fixed
hyperparameters with reestimation of the hyperparameters by reevaluating the Hessian matrix for the new value of w. The various
approximations involved in this approach improve as the number
of data points P r . However, for a finite data set it can be difficult
to assess the accuracy of the method. One obvious limitation is that
it only takes account of the behavior of the posterior distribution
at the mode.

The KL Variational Approach

KL(q, p) ⳱

冮 {q(x) log q(x) ⳮ q(x) log p(x)}dx
KL(q(w), p(w|C, D)) ⱖ 0

(28)

(29)

Finding the best distribution q(w) in a restricted set of possible
distributions by minimizing KL(q, p) gives the best estimate (in the
KL sense) to the posterior distribution. From Equation 29 we immediately have the bound
log p(D|C) ⱖ

冮 ⳮq(w) log q(w)dw
Ⳮ 冮 q(w) log p(D|C, w)p(w)dw

(30)

We can make use of this lower bound to carry out an approximate ML-II hyperparameter optimization by the following two-step
procedure: First fix the hyperparameters C and optimize the bound,
Equation 30, with respect to q(w). Then, for fixed q(w), optimize
the bound with respect to C. This scheme is a generalization of the
Expectation-Maximization procedure (see Neal and Hinton in Jordan, 1998) and is also called ensemble learning (Barber and
Bishop, 1997).

Bayesian Pruning
To this point we discussed the idea of using a prior that encourages
smoothness of the input-output mapping. Insofar as neural networks are nonlinear functions of a linear combination of inputs, it
is reasonable to use a prior that encourages small weights, p(w) 
exp (ⳮwTAw/2). Typically, only diagonal matrices A are considered. We can group weights into clusters containing one or more
weights and associate with each cluster c a common hyperparameter ␣c. The Bayesian approach results in a posterior distribution
over these hyperparameters ␣c. Alternatively, we can optimize the
hyperparameters using ML-II. If the posterior distribution favors
large ␣c values, then effectively the weight cluster c is not contributing to the network and may be pruned. A useful choice of clustering is to group all the weights from a single input xi into the
hidden units (note that these weights are different from the weights
that fan in to a hidden node). If the hyperparameter ␣i (after MLII optimization) associated with the weights fanning out from input
xi is large, the contribution of input xi is negligible and can be
excluded. This is called automatic relevance determination
(MacKay, 1995).

The Relevance Vector Machine
In the discussion regarding GLMs, f (x) ⳱ 	iwi␾i(x), we fixed the
centers of the basis functions ␾i. Similarly, in the relevance vector
machine we use fixed basis functions (Tipping, 2001). By associating with each weight wi a regularizing prior p(wi)  exp(ⳮ
␣iw2i /2), we can perform ML-II to optimize the hyperparameters ␣i.
After optimization, typically many of the ␣i will become very large,
effectively removing the basis function ␾i from the model. This
pruning procedure often results in a much sparser representation of
the data in terms of only the “relevant” basis functions; this scheme
is therefore particularly useful for compression. This sparseness
effect is similar, although not equivalent, to the support vector machine (see SUPPORT VECTOR MACHINES), in which training points

156

Part III: Articles

are effectively removed if they do not affect the prediction of the
model.

Classification
The previously described methods can be applied to classification,
usually with only minor modification. For convenience, we consider here only problems with two classes. The data set is D ⳱
{(x l, t l), l ⳱ 1, . . . , P}, where t l 僆 {0, 1}. In a probabilistic
framework, we use the output of the network f (x; w) to represent
the probability that the input is in class 1. In this case, the likelihood
is
P

p(D|w) ⳱

兿 f (x l; w)t (1 ⳮ f (x l; w))1ⳮt
l⳱1
l

l

(31)

For example, we could take f (x) ⳱ g(wTx), where g(x) ⳱ 1/(1 Ⳮ
exp(ⳮx)) (Bishop, 1995). In the Bayesian approach, we need to
specify a prior belief about the weights. As before, a sensible choice
is p(w)  exp(ⳮ␣wTw/2), since smaller weights will give less certain predictions. This results in a posterior distribution p(w|D) 
p(D|w)p(w). For a novel input x the probability that it belongs to
class 1 is

冮 p(t ⳱ 1|x, w)p(w|D)dw
⳱ 冮 g(w x)p(w|D)dw

p(t ⳱ 1|x, D) ⳱

T

Consider, for example, fitting the data in Figure 5A. The posterior distribution is given in Figure 5D. The decision boundary
(p(t ⳱ 1|x, w, D) ⳱ 0.5) for the MAP solution is given in Figure
5B along with the 0.1 and 0.9 decision contours. Another decision
boundary associated with the posterior weights wA is plotted in
Figure 5B. Because the decision boundaries are linear, the predictions of these single networks away from the data remain overly
confident. The Bayesian prediction, Equation 32, is plotted in Figure 5C and has decision boundaries that properly account for the
uncertainty in the predictions away from the training data.
Since the final integrand in Equation 32 depends only on the
weight vector through the “activation” a ⳱ wTx, we need only
know the distribution of this one-dimensional quantity. A reasonable assumption is that the activation will be Gaussian distributed
p(a) ⳱ N(ā, var(a)), and the resulting one-dimensional integration
p(t ⳱ 1|x, D) ⳱ 
g(a)p(a)da can be efficiently performed using
quadrature. The statistics of the activation are
ā ⳱ w̄Tx,

var(a) ⳱ xTRx

(33)

where w̄ and 	 are the mean and covariance of the weight posterior
p(w|D). It is convenient to approximate these statistics using Laplace’s method.

Discussion
(32)

The Bayesian framework deals with uncertainty in a natural, consistent manner by combining prior beliefs about which models are

Figure 5. A, The decision boundary and
0.1, 0.9 decision contours for the most
likely predictor wMAP. B, The predictions
for wA. C, The posterior averaged predictors. D, The weight posterior distribution.

Bayesian Networks
appropriate with how likely each model would be to have generated
the data. This results in an elegant, general framework for fitting
models to data, which, however, may be compromised by computational difficulties in carrying out the ideal procedure. There are
many approximate Bayesian implementations, using methods such
as sampling, perturbation techniques, and variational methods. Often these enable the successful approximate realization of practical
Bayesian schemes. An attractive, built-in effect of the Bayesian
approach is an automatic procedure for combining predictions from
several different models, the combination strength of a model being
given by the posterior likelihood of the model. In the case of models linear in their parameters, Bayesian neural networks are closely
related to Gaussian processes, and many of the computational difficulties of dealing with more general stochastic nonlinear systems
can be avoided.
Bayesian methods are readily extendable to other areas, in particular density estimation, and the benefits of dealing with uncertainty are again to be found (see Bishop in Jordan, 1998). Traditionally, neural networks are graphical representations of functions,
in which the computations at each node are deterministic. In the
classification discussion, however, the final output represents a stochastic variable. We can consider such stochastic variables elsewhere in the network, and the sigmoid belief network is an early
example of a stochastic network (Neal, 1992). There is a major
conceptual difference between such models and conventional neural networks. Networks in which nodes represent stochastic variables are called graphical models (see BAYESIAN NETWORKS) and
are graphical representations of distributions (GRAPHICAL MODELS: PROBABILISTIC INFERENCE). Such models evolve naturally
from the desire of incorporating uncertainty and nonlinearity in
networked systems.

157

Road Map: Learning in Artificial Networks
Related Reading: Bayesian Networks; Gaussian Processes; Graphical
Models: Probabilistic Inference; Support Vector Machines

References
Barber, D., and Bishop, C., 1997, Ensemble learning in Bayesian neural
networks, in Neural Networks and Machine Learning (C. Bishop, Ed.),
NATO ASI Series, New York: Springer-Verlag.
Berger, J. O., 1985, Statistical Decision Theory and Bayesian Analysis, 2nd
ed., New York: Springer-Verlag. ⽧
Bishop, C. M., 1995, Neural Networks for Pattern Recognition, Oxford,
Engl.: Oxford University Press. ⽧
Box, G., and Tiao, G., 1973, Bayesian Inference in Statistical Analysis,
Reading, MA: Addison-Wesley.
Cover, M., and Thomas, J., 1991, Elements of Information Theory, New
York: Wiley.
Jordan, M., Ed., 1998, Learning in Graphical Models, Cambridge, MA:
MIT Press.
MacKay, D. J. C., 1992, Bayesian interpolation, Neural Computat., 4(3):
415–447.
MacKay, D. J. C., 1995, Probable networks and plausible predictions: A
review of practical Bayesian methods for supervised neural networks,
Netw. Computat. Neural Syst., 6(3). ⽧
Neal, R. M., 1992, Connectionist learning of belief networks, Artif. Intell.,
56:71–113.
Neal, R. M., 1993, Probabilistic Inference Using Markov Chain Monte
Carlo Methods, Technical Report CRG-TR-93-1, Department of Computer Science, University of Toronto, Toronto, Canada.
Neal, R. M., 1996, Bayesian Learning for Neural Networks, Lecture Notes
in Statistics 118, New York: Springer-Verlag.
Tipping, M. E., 2001, Sparse Bayesian learning and the relevance vector
machine, J. Machine Learn. Res., no. 1, 211–244. ⽧
Walker, A. M., 1969, On the asymptotic behaviour of posterior distributions, J. R. Statist. Soc. B, 31:80–88.

Bayesian Networks
Judea Pearl and Stuart Russell
Introduction
Probabilistic models based on directed acyclic graphs have a long
and rich tradition, beginning with work by the geneticist Sewall
Wright in the 1920s. Variants have appeared in many fields. Within
statistics, such models are known as directed graphical models;
within cognitive science and artificial intelligence (AI), they are
known as Bayesian networks. The name honors the Reverend
Thomas Bayes (1702–1761), whose rule for updating probabilities
in light of new evidence is the foundation of the approach. The
initial development of Bayesian networks in the late 1970s was
motivated by the need to model the top-down (semantic) and
bottom-up (perceptual) combination of evidence in reading. The
capability for bidirectional inferences, combined with a rigorous
probabilistic foundation, led to the rapid emergence of Bayesian
networks as the method of choice for uncertain reasoning in AI and
expert systems, replacing earlier, ad hoc rule-based schemes (Pearl,
1988; Shafer and Pearl, 1990; Jensen, 1996).
The nodes in a Bayesian network represent propositional variables of interest (e.g., the temperature of a device, the sex of a
patient, a feature of an object, the occurrence of an event) and the
links represent informational or causal dependencies among the
variables. The dependencies are quantified by conditional probabilities for each node, given its parents in the network. The network
supports the computation of the probabilities of any subset of variables given evidence about any other subset.

Figure 1 illustrates a simple yet typical Bayesian network. It
describes the causal relationships among five variables: the season
of the year (X1), whether it’s raining or not (X2), whether the sprinkler is on or off (X3), whether the pavement is wet or dry (X4), and
whether the pavement is slippery or not (X5). Here, the absence of
a direct link between X1 and X5, for example, captures our understanding that there is no direct influence of season on slipperiness;

Figure 1. A Bayesian network representing causal influences among five
variables. Each arc indicates a causal influence of the “parent” node on the
“child” node.

158

Part III: Articles

the influence is mediated by the wetness of the pavement. (If freezing is a possibility, then a direct link could be added.)
Perhaps the most important aspect of Bayesian networks is that
they are direct representations of the world, not of reasoning processes. The arrows in the diagram represent real causal connections
and not the flow of information during reasoning (as in rule-based
systems and neural networks). Reasoning processes can operate on
Bayesian networks by propagating information in any direction.
For example, if the sprinkler is on, then the pavement is probably
wet (prediction); if someone slips on the pavement, that also provides evidence that it is wet (abduction, or reasoning to a probable
cause). On the other hand, if we see that the pavement is wet, that
makes it more likely that the sprinkler is on or that it is raining
(abduction); but if we then observe that the sprinkler is on, that
reduces the likelihood that it is raining (explaining away). It is this
last form of reasoning, explaining away, that is especially difficult
to model in rule-based systems and neural networks in any natural
way, because it seems to require the propagation of information in
two directions.

Probabilistic Semantics
Any complete probabilistic model of a domain must, either explicitly or implicitly, represent the joint distribution—the probability
of every possible event as defined by the values of all the variables.
There are exponentially many such events, yet Bayesian networks
achieve compactness by factoring the joint distribution into local,
conditional distributions for each variable given its parents. If xi
denotes some value of the variable Xi and pai denotes some set of
values for Xi’s parents, then P(xi | pai) denotes this conditional
distribution. For example, P(x4 | x2, x3) is the probability of wetness
given the values of sprinkler and rain. The global semantics of
Bayesian networks specifies that the full joint distribution is given
by the product
P(x1, . . . , xn) ⳱

兿i P(xi | pai)

(1)

In our example network, we have
P(x1, x2, x3, x4, x5)
⳱ P(x1)P(x2 | x1)P(x3 | x1)P(x4 | x2, x3)P(x5 | x4)

(2)

Provided that the number of parents of each node is bounded, it is
easy to see that the number of parameters required grows only
linearly with the size of the network, whereas the joint distribution
itself grows exponentially. Further savings can be achieved using
compact parametric representations, such as noisy-OR models, decision trees, or neural networks, for the conditional distributions.
For example, in sigmoid networks (see Jordan, 1999), the conditional distribution associated with each variable is represented as a
sigmoid function of a linear combination of the parent variables;
in this way, the number of parameters required is proportional to,
rather than exponential in, the number of parents.
There is also an entirely equivalent local semantics that asserts
that each variable is independent of its nondescendants in the network given its parents. For example, the parents of X4 in Figure 1
are X2 and X3, and they render X4 independent of the remaining
nondescendant, X1. That is,
P(x4 | x1, x2, x3) ⳱ P(x4 | x2, x3)
The collection of independence assertions formed in this way suffices to derive the global assertion in Equation 1, and vice versa.
The local semantics is most useful in constructing Bayesian networks, because selecting as parents all the direct causes of a given
variable invariably satisfies the local conditional independence
conditions (Pearl, 2000, p. 30). The global semantics leads directly
to a variety of algorithms for reasoning.

Evidential Reasoning
From the product specification in Equation 1, one can express the
probability of any desired proposition in terms of the conditional
probabilities specified in the network. For example, the probability
that the sprinkler is on, given that the pavement is slippery, is
P(X3⳱on | X5⳱true) ⳱

P(X3⳱on, X5⳱true)
P(X5⳱true)

兺 P(x1, x2, X3⳱on, x4, X5⳱true)
x ,x ,x
⳱
兺 P(x1, x2, x3, x4, X5⳱true)
x ,x ,x ,x
兺 P(x1)P(x2 | x1)P(X3⳱on|x1)P(x4 | x2, X3⳱on)P(X5⳱true | x4)
x ,x ,x
⳱
兺 P(x1)P(x2 | x1)P(x3 | x1)P(x4 | x2, x3)P(X5⳱true | x4)
x ,x ,x ,x
1 2 4

1 2 3 4

1 2 4

1 2 3 4

These expressions can often be simplified in ways that reflect the
structure of the network itself. The first algorithms proposed for
probabilistic calculations in Bayesian networks used a local, distributed message-passing architecture, typical of many cognitive
activities (Kim and Pearl, 1983). Initially this approach was limited
to tree-structured networks, but it was later extended to general
networks in Lauritzen and Spiegelhalter’s (1988) method of jointree propagation. A number of other exact methods have been developed and can be found in recent textbooks (Jensen, 1996; Jordan, 1999).
It is easy to show that reasoning in Bayesian networks subsumes
the satisfiability problem in propositional logic and, hence, is NPhard. Monte Carlo simulation methods can be used for approximate
inference (Pearl, 1988), giving gradually improving estimates as
sampling proceeds. (These methods use local message propagation
on the original network structure, unlike join-tree methods.) Alternatively, variational methods provide bounds on the true probability (Jordan, 1999).

Uncertainty over Time
Entities that live in a changing environment must keep track of
variables whose values change over time. Dynamic Bayesian networks, or DBNs, capture this process by representing multiple copies of the state variables, one for each time step (Dean and Kanazawa, 1989). A set of variables Xt denotes the world state at time
t and a set of sensor variables Et denotes the observations available
at time t. The sensor model P(Et | Xt) is encoded in the conditional
probability distributions for the observable variables, given the
state variables. The transition model P(XtⳭ1 | Xt) relates the state
at time t to the state at time t Ⳮ 1. Keeping track of the world,
known as filtering, means computing the current probability distribution over world states given all past observations, i.e., P(Xt |
E1, . . . , Et). Dynamic Bayesian networks include as special cases
other temporal probability models, such as hidden Markov models
(DBNs with a single discrete state variable) and Kalman filters
(DBNs with continuous state and sensor variables and linear Gaussian transition and sensor models). For the general case, exact filtering is intractable, and a variety of approximation algorithms have
been developed. The most popular and flexible of these is the family of particle filtering algorithms (see Doucet, de Freitas, and Gordan, 2001).

Learning in Bayesian Networks
The conditional probabilities P(xi | pai) can be updated continuously from observational data using gradient-based or ExpectationMaximization (EM) methods that use just local information derived
from inference (Binder et al., 1997; Jordan, 1999), in much the
same way as weights are adjusted in neural networks. It is also
possible to learn the structure of the network, using methods that

Bayesian Networks
trade off network complexity against degree of fit to the data (Friedman, 1998). As a substrate for learning, Bayesian networks have
the advantage that it is relatively easy to encode prior knowledge
in network form, either by fixing portions of the structure or by
using prior distributions over the network parameters. Such prior
knowledge can allow a system to learn accurate models from much
less data than are required by tabula rasa approaches.

Causal Networks
Most probabilistic models, including general Bayesian networks,
describe a distribution over possible observed events, as in Equation 1, but say nothing about what will happen if a certain intervention occurs. For example, what if I turn the sprinkler on? What
effect does that have on the season, or on the connection between
wetness and slipperiness? A causal network, intuitively speaking,
is a Bayesian network with the added property that the parents of
each node are its direct causes, as in Figure 1. In such a network,
the result of an intervention is obvious: the sprinkler node is set to
X3 ⳱ on, and the causal link between the season X1 and the sprinkler X3 is removed. All other causal links and conditional probabilities remain intact, so the new model is
P(x1, x2, x3, x4, x5) ⳱
P(x1)P(x2 | x1)P(x4 | x2, X3 ⳱ on)P(x5 | x4)
Notice that this differs from observing that X3 ⳱ on, which would
result in a new model that included the term P(X3 ⳱ on|x1). This
mirrors the difference between seeing and doing: after observing
that the sprinkler is on, we wish to infer that the season is dry, that
it probably did not rain, and so on; an arbitrary decision to turn the
sprinkler on should not result in any such beliefs.
Causal networks are more properly defined, then, as Bayesian
networks in which the correct probability model after intervening
to fix any node’s value is given simply by deleting links from the
node’s parents. For example, fire r smoke is a causal network,
whereas smoke r fire is not, even though both networks are equally
capable of representing any joint distribution on the two variables.
Causal networks model the environment as a collection of stable
component mechanisms. These mechanisms may be reconfigured
locally by interventions, with correspondingly local changes in the
model. This, in turn, allows causal networks to be used very naturally for prediction by an agent that is considering various courses
of action (Pearl, 2000).

Functional Bayesian Networks
The networks discussed so far are capable of supporting reasoning
about evidence and about actions. Additional refinement is necessary in order to process counterfactual information. For example,
the probability that “the pavement would not have been slippery
had the sprinkler been OFF, given that the sprinkler is in fact ON
and that the pavement is in fact slippery” cannot be computed from
the information provided in Figure 1 and Equation 1. Such counterfactual probabilities require a specification in the form of functional networks, where each conditional probability P(xi| pai) is replaced by a functional relationship xi ⳱ fi( pai, ⑀i), where ⑀i is a
stochastic (unobserved) error term. When the functions fi and the
distributions of ⑀i are known, all counterfactual statements can be
assigned unique probabilities, using evidence propagation in a
structure called a “twin network.” When only partial knowledge
about the functional form of fi is available, bounds can be computed
on the probabilities of counterfactual sentences (Pearl, 2000).

Causal Discovery
One of the most exciting prospects in recent years has been the
possibility of using Bayesian networks to discover causal structures

159

in raw statistical data (Pearl, 2000)—a task previously considered
impossible without controlled experiments. Consider, for example,
the following intransitive pattern of dependencies among three
events: A and B are dependent, B and C are dependent, yet A and
C are independent. If you ask a person to supply an example of
three such events, the example would invariably portray A and C
as two independent causes and B as their common effect, namely,
A r B R C. (For instance, A and C could be the outcomes of tossing
two fair coins, and B could represent a bell that rings whenever
either coin comes up heads.) Fitting this dependence pattern with
a scenario in which B is the cause and A and C are the effects is
mathematically feasible but very unnatural, because it must entail
fine tuning of the probabilities involved; the desired dependence
pattern will be destroyed as soon as the probabilities undergo a
slight change.
Such thought experiments tell us that certain patterns of dependency, which are totally void of temporal information, are conceptually characteristic of certain causal directionalities and not others.
When put together systematically, such patterns can be used to infer
causal structures from raw data and to guarantee that any alternative
structure compatible with the data must be less stable than the
one(s) inferred; namely, slight fluctuations in parameters will render that structure incompatible with the data.

Plain Beliefs
In mundane decision making, beliefs are revised not by adjusting
numerical probabilities but by tentatively accepting some sentences
as “true for all practical purposes.” Such sentences, called plain
beliefs, exhibit both logical and probabilistic character. As in classical logic, they are propositional and deductively closed; as in
probability, they are subject to retraction and can be held with
varying degrees of strength. Bayesian networks can be adopted to
model the dynamics of plain beliefs by replacing ordinary probabilities with nonstandard probabilities, that is, probabilities that are
infinitesimally close to either zero or one (Goldszmidt and Pearl,
1996).

Discussion
Bayesian networks may be viewed as normative cognitive models
of propositional reasoning under uncertainty. They handle noise
and partial information using local, distributed algorithms for inference and learning. Unlike feedforward neural networks, they facilitate local representations in which nodes correspond to propositions of interest. Recent experiments suggest that they accurately
capture the causal inferences made by both children and adults
(Tenenbaum and Griffiths, 2001). Moreover, they capture patterns
of reasoning, such as explaining away, that are not easily handled
by any competing computational model. They appear to have many
of the advantages of both the “symbolic” and the “subsymbolic”
approaches to cognitive modeling, and are now an essential part of
the foundations of computational neuroscience (Jordan and Sejnowski, 2001).
Two major questions arise when we postulate Bayesian networks
as potential models of actual human cognition. First, does an architecture resembling that of Bayesian networks exist anywhere in
the human brain? At the time of writing, no specific work has been
done to design neurally plausible models that implement the required functionality, although no obvious obstacles exist. Second,
how could Bayesian networks, which are purely propositional in
their expressive power, handle the kinds of reasoning about individuals, relations, properties, and universals that pervade human
thought? One plausible answer is that Bayesian networks containing propositions relevant to the current context are constantly being
assembled, as needed, from a more permanent store of knowledge.
For example, the network in Figure 1 may be assembled to help

160

Part III: Articles

explain why this particular pavement is slippery right now, and to
decide whether this can be prevented. The background store of
knowledge includes general models of pavements, sprinklers, slipping, rain, and so on; these must be accessed and supplied with
instance data to construct the specific Bayesian network structure.
The store of background knowledge must utilize some representation that combines the expressive power of first-order logical languages (such as semantic networks) with the ability to handle
uncertain information. Substantial progress has been made on constructing systems of this kind (Koller and Pfeffer, 1998), but as yet
no overall cognitive architecture has been proposed.
Road Maps: Artificial Intelligence; Learning in Artificial Networks
Related Reading: Bayesian Methods and Neural Networks; Decision Support Systems and Expert Systems; Graphical Models: Probabilistic
Inference

References
Binder, J., Koller, D., Russell, S., and Kanazawa, K., 1997, Adaptive probabilistic networks with hidden variables, Machine Learn., 29:213–244.
Dean, T., and Kanazawa, K., 1989, A model for reasoning about persistence
and causation, Computat. Intell., 5:142–150.
Doucet, A., de Freitas, J., and Gordon, N., 2001, Sequential Monte Carlo
Methods in Practice, Berlin: Springer-Verlag.
Friedman, N., 1998, The Bayesian structural EM algorithm, in Uncertainty
in Artificial Intelligence: Proceedings of the Fourteenth Conference

(G. F. Cooper and S. Moral, Eds.), San Mateo, CA: Morgan Kaufmann,
pp. 129–138.
Goldszmidt, M., and Pearl, J., 1996, Qualitative probabilities for default
reasoning, belief revision, and causal modeling, Artif. Intell., 84:57–112.
Jensen, F. V., 1996, An Introduction to Bayesian Networks, New York:
Springer-Verlag. ⽧
Jordan, M. I., Ed., 1999, Learning in Graphical Models, Cambridge, MA:
MIT Press. ⽧
Jordan, M. I., and Sejnowski, T. J., Eds., 2001, Graphical Models: Foundations of Neural Computation, Cambridge, MA: MIT Press.
Kim, J. H., and Pearl, J., 1983, A computational model for combined causal
and diagnostic reasoning in inference systems, in Proceedings of the
Eighth International Joint Conference on Artificial Intelligence (IJCAI83), San Mateo, CA: Morgan Kaufmann, pp. 190–193.
Koller, D., and Pfeffer, A., 1998, Probabilistic frame-based systems, in
Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98), Menlo Park, CA: AAAI Press, pp. 580–587.
Lauritzen, S. L., and Spiegelhalter, D. J., 1988, Local computations with
probabilities on graphical structures and their application to expert systems (with discussion), J. R. Statist. Soc., series B, 50:157–224.
Pearl, J., 1988, Probabilistic Reasoning in Intelligent Systems, San Mateo,
CA: Morgan Kaufmann. ⽧
Pearl, J., 2000, Causality: Models, Reasoning, and Inference, New York:
Cambridge University Press. ⽧
Shafer, G., and Pearl, J., Eds., 1990, Readings in Uncertain Reasoning, San
Mateo, CA: Morgan Kaufmann.
Tenenbaum, J. B., and Griffiths, T. L., 2001, Structure learning in human
causal induction, in Advances in Neural Information Processing Systems
13, Cambridge, MA: MIT Press.

Biologically Inspired Robotics
Noel E. Sharkey
Introduction

The Roots of Biologically Inspired Robotics

At the beginning of the twenty-first century, living organisms have
still not been successfully replicated by machines. Computers are
much faster at number crunching than humans and can even beat
the greatest at chess, and other machines can perform routine physical tasks faster than us and with a precision that we cannot approach. However, animals exhibit such remarkable capacities for
flexible adaptation to novel circumstances that roboticists can only
gaze in wonder. It is thus an important goal of modern robotics to
learn from the way organisms are constructed biologically, and how
this creates adaptive behaviors.
Biologically inspired robotics, also known as biomimetic robotics or biorobotics, refers to robotics research in which the life sciences, including biology, psychology, ethology, neuroscience, and
evolutionary theory, play a key role in motivating the research. It
is necessarily broad because the field is just beginning to emerge
as a unified discipline, and so it still has fuzzy boundaries. Biorobotics research ranges from modeling animal sensors in hardware
for guiding robots in target environments to investigating the interaction between neural learning and evolution in a variety of robot
tasks. There are, however, common themes that will be explored
here.
In the following sections, some of the major issues in biorobotics
research and the aims of this approach are examined. First we
briefly consider the historical roots of the core ideas. The seminal
work of Grey Walter (1953) sets the scene and introduces some of
the key elements of biologically inspired robotics. The re-introduction and development of the ideas in the 1980s occurred with
Braitenberg’s synthetic psychology and Brook’s behavior-based robotics. In summarizing the breadth of the current work, we attempt
a threefold classification of biologically inspired robotics.

The roots of biologically inspired robotics date back to the early
twentieth century, when Hammond constructed a heliotrope based
on the biologist Loeb’s tropism theory of animal behavior. Loeb
proposed that animals are attracted and repelled by stimuli in the
environment in a way similar to the phototropic responses of plants.
Although Hammond’s heliotrope did not model an animal, its
mechanized movement toward light was sufficient to satisfy Loeb
that his theory has physical plausibility (cf. Sharkey and Ziemke
in Ziemke and Sharkey, 1998, pp. 361–392, for an account).
There were a number of robot learning studies during the first
half of the twentieth century, before the birth of artificial intelligence (AI). However, the prototypical biorobotics work was conducted by Grey Walter (1953). He went far beyond Hammond in
testing the mechanistic plausibility of animal tropism. His aim was
to create a self-sustaining artificial life form that could adapt. This
required the development of a robot that could seek out a source
to recharge its batteries on demand.
Grey Walter used electromechanical robots equipped with two
input “receptors”: a photo-electric cell for sensitivity to light, and
an electrical contact as a touch receptor. The controller, between
sensors and motors, was a small artificial nervous system built from
miniature valves, relays, condensers, batteries, and small electric
motors—no computer. There was a hutch where a robot could drive
in to have the battery automatically recharged.
Behavior resulted from the interaction of the internal states of
the robot (battery level) and the intensity of light sources, as well
as other environmental factors such as obstacles. When the battery
levels were high, the robot was repelled by the bright light of the
hutch and attracted by the moderate light in the room, where it

Biologically Inspired Robotics
“explored.” With low battery levels, the robot was attracted to the
bright light of the hutch for an automatic recharge. In this way
Grey Walter demonstrated that mechanical tropism could work as
a means of exploration and maintaining energy.
Grey Walter (1953) also investigated adaptation and showed
how a simple learning mechanism could extend the behavior of a
robot using the conditioned reflex analog (CORA) with a microphone for auditory input.
Biorobotics more or less died when Grey Walter moved on to
other research in the 1950s. With the rise of AI and computing, the
focus was on providing robots with human-inspired perception and
cognition. The new robots had a series of modules, such as visual
processing, planning, and reasoning, through which sensory information passed serially. Typically, a decision-making module controlled the output to the actuators. This was in contrast to the more
direct control approach of Grey Walter, in which the only mediation between sensing and moving was provided by an artificial
neural net consisting of two hardware neurons. Another difference
was that whereas AI robotics focused on human cognition, Grey
Walter focused on the question of how seemingly complex animallike behavior could arise from simple mechanisms such as tropisms
and reflexes.
Today the term taxis is used instead of tropism to refer to the
movement of an animal directed by a stimulus, either negatively
or positively. Examples of such stimulus-directed activity include
chemotaxis (chemical taxis), geotaxis (gravity), phototaxis (light),
and phonotaxis (auditory). Although Grey Walter worked only on
individual taxes, biologists at the time (e.g., Fraenkel and Gunn;
cf. Sharkey and Ziemke in Ziemke and Sharkey, 1998, pp. 361–
392) proposed that the behavior of many organisms could be explained by a combination of taxes working together and in opposition. They cited Fraenkel’s study of the coastal slug, Littorina
neritoides. Littorina combines positive and negative phototaxis
with negative geotaxis to feed and survive. Subsequently, combinations of taxes have been used as powerful explanations of many
animal behaviors, from bacteria feeding to insect pheromone trailing to fish breeding and feeding.
These ideas began to emerge in a new wave of robotics during
the 1980s as a result of two major influences. First, the neuroanatomist Valentino Braitenberg showed how a number of complex
behaviors could emerge from a combination of very simple neural
networks encoding different taxes (Braitenberg, 1984). Second,
Rodney Brooks’s development of subsumption architecture allowed autonomous control by a combination of taxes, and drove
home the effectiveness of behavior-based robotics. His major papers from this period are reprinted in Brooks (1999). In this style
of robotics, each behavior-producing module, such as avoid obstacles or move toward light, is encoded as a separate program module
such that each is directly under the control of environmental circumstances rather than a central controller. For example, when
there is light on the sensors, the move toward light module will be
active until the light is occluded by an obstacle, at which point the
avoid obstacles module takes over.

Current Directions in Biorobotics
A large emerging body of research in robotics is making the connection between sensing and moving simple, and the relationship
between robot and world tightly coupled. It was the dramatic increase in robotics research, riding on the back of the new behaviorbased approach, that enabled biologically inspired robotics to flourish. Since the behavior-based approach grew directly from ideas in
the life sciences, it was only natural that once the tools and techniques of the approach had been developed, they would be turned
back to work on the source of inspiration.

161

In this article, biorobotics is divided into three main classes.
Although these classes are mutually supportive and their paths often cross, the distinctions between them are nonetheless useful.
• The generalized approach follows from the lineage of ideas that
inspired Grey Walter to use robots to investigate and extend general mechanistic theories of animal behavior and adaptation. This
includes research using neural network adaptation through learning and/or evolutionary methods (see REACTIVE ROBOTIC SYSTEMS).
• The specific approach uses methods from the generalized approach to investigate specific species or organisms. The research
can range from studies of the physical plausibility of a simple
neural explanation for some target behavior pattern to the physical modeling of a particular animal or some of its senses. Models
can be evaluated by observing the target behavior of the robot
interacting with the environment through sensing and moving.
One of the main goals of specific biorobotics is to develop new
methods for scientific modeling.
• The theoretical division is a mixed bag that provides an examination of the implications of the research for a number of disciplines. The issues range widely, from discussions of robot embodiment to the nature of life. Although all biorobotics has a
theoretical component, the theoretical approach is distinct in not
requiring empirical work.
The idea was to include only work that at least touched base with
the life sciences with respect to the type of controllers and the
method of adaptation used. Each of the classes is dealt with in more
depth in the following three subsections.

Generalized Biorobotics
The research impetus is to use broad notions derived from the life
sciences for robot control. Many of these notions are in the form
of implicit assumptions, such as deriving complex behavior from
the simplest possible mechanisms or using the ideas of taxis or
tropism for automated control. In this sense, Grey Walter’s research
was prototypical generalized biorobotics. His work on classical
conditioning with the CORA architecture also foresaw the modern
focus on adaptive techniques in robotics. The biological currency
in the generalized biorobotics community mostly consists of abstract models of neural network learning, animal learning, or evolutionary processes, or a mixture. In the next two subsections, the
main trends will be discussed.
Evolution. Evolutionary methods have been used for many applications since the 1950s when the first Genetic Algorithm (GA;
see EVOLUTION OF ARTIFICIAL NEURAL NETWORKS) was developed by Friedman for his master’s thesis on evolving control circuits for autonomous robots. These methods are particularly useful
for constraining search in very large search spaces. However, from
the perspective of biorobotics, the most important reason for employing evolutionary methods is that they are abstractly related to
the Darwinian principle of natural selection and may be seen as
analogous to real evolutionary theory; i.e., there is a fitness function
to decide how fit a particular program is in the context of the problem it is to solve, and there are mutation and crossover to operate
on the computer equivalent of gene strings. Given the intended
relationship between the behavior of biorobots and natural biological behavior, the development of an evolutionary robotics is a
very important step.
A fairly typical example of evolutionary methods for single robots is Nolfi’s garbage collector (in Sharkey, 1997, pp. 187–198).
The connection weights were evolved to control a miniature robot
equipped with distance sensors and a gripper. The task was to

162

Part III: Articles

“clean” an arena by picking up objects and dropping them off outside. To do this, the robot had to move around the arena, avoid
obstacles, locate an object, pick it up, move toward the walls, and
release the object outside the arena. After 1,000 generations, robot
controllers were evolved that performed the cleaning task to a high
degree of accuracy.
Most evolutionary robotics research relies on using a fixed neural
network architecture on which the weights are evolved. Another
interesting approach is to let the evolutionary method decide on
the type of connectivity between the units in the net; i.e., the pattern
of connectivity is “genetically” represented (see Husbands et al. in
Ziemke and Sharkey, 1998, pp. 185–210).
An important reasearch area in biorobotics is concerned with
how the environment and other species co-evolve with a given
organism, resulting in an evolutionary arms race. This issue has
been taken up in the simple form of evolving two competing robot
controllers at the same time. For example, Floreano and Nolfi
(1997) co-evolved the controllers for predator and prey behavior
for two different “species” as part of each other’s environment. One
of the main problems was that in one generation the predators
would win but in the next generation the prey would win because
a counterstrategy was evolved. This instability has been overcome
by introducing neural network learning during the lifetime of the
individuals. In this way the predators were able to adapt to the new
evolved strategies of the prey.
The approach of combining the two adaptive techniques of evolutionary methods and neural network learning is proving to be a
very effective adaptation technique that has a naturalistic flavor.
Much of the research on combining has focused on how learning
can help guide evolution—the Baldwin effect. The idea is that if
the genotype of an individual is close to an optimal combination
of genes, learning can allow that individual to increase its suitability for its environment, thereby increasing its probability of
survival and reproduction. This could lead to a larger “basin” of
fitness around optimal genotypes, channeling evolution toward optimal solutions (see Nolfi and Floreano, 1999, for a review).
Learning. One of the most widely used learning techniques in
biorobotics is reinforcement or reward learning (RL) (see, e.g.,
Krose, 1995). RL has been studied psychologically since the beginning of the twentieth century. An advantage of RL techniques
in robotics is that the learner needs only occasional reinforcement.
RL is therefore unlike supervised learning, which requires a trainer
to provide the learner with an exact target action in every time step,
suited for use in unknown environments or tasks (but see Sharkey,
1998, on the use of innate controllers for training supervised
learning).
More recently there has been a move toward using the operant
conditioning techniques developed in the 1940s for studies of animal learning. Operant conditioning involves the shaping of pregiven behaviors. In particular, animals can be trained to produce
an experimenter-required behavior when they are rewarded for successive approximations to that behavior. For example, to begin
training a rat to press a bar for food, rewards are given for any
reaching movement. Then successive approximations to the goal
are rewarded until the target behavior is observed. In robotics, this
has also been called behavior editing by Dorigo and Colombetti
(1998), who have conducted most of the experimental work on this
technique. An extension of this work to include incremental shaping is discussed by Urzelai et al. (in Zimke and Sharkey, 1998, pp.
341–360).
In a realistic approach, Saksida et al. (in Sharkey, 1997, pp. 231–
249) successfully used operant conditioning to modify the interaction between behaviors that had been preprogrammed into a
robot. This departure from using reinforcement learning as a trialand-error approach to modify existing behaviors is a step toward

real animal training. Furthermore, unlike most RL work, the training was conducted by a human trainer rather than a programmed
reinforcer. Initially, the robot has three categories of objects: a
bright orange jacket, green and pink plastic dog toys, and blue
plastic recycling bins. One of its innate behaviors was to approach
the plastic dog toys and pick them up. Successful (fast) shaping
was shown for a number of new behaviors, including Follow the
Trainer, Recycling, and Playing Fetch.

Specific Biorobotics
One of the attractions of robotics is that there is strong potential
for testing the relationship between a model and some hypothesized
behavioral consequences in the physical world. When Hammond
built his heliotrope in the early twentieth century to test Loebian
theory, it was essentially the physical plausibility of the theory that
was under scrutiny. This was generalized biorobotics in that the
hypotheses were about all animals. One of the goals of specific
biorobotics is to extend such physical testing to test specific hypotheses about specific species. The motivation is that mathematical specification and computer simulation provide only a weak test
of a model in that the inputs are typically chosen by the researcher
and the outputs are designed to be interpretable as data points or
graphs. The central idea of specific biorobotics is to test the model
by situating the robot in a physical environment that provides the
main features of the world of the target species.
There are a number of dangers with this approach, and a number
of wrinkles will have to be ironed out before such modeling reaches
maturity as a test methodology in biology and psychology. For
example, with complex neural networks such as brains, it is not
always possible to isolate a mechanism and test its behavioral consequences. Although robotics can offer a window on the possible
behaviors resulting from particular models, a model cannot generally be used directly as a robot controller; a number of “gaps”
between the sensors, the model, and motor output have to be filled
in. This can be advantageous in forcing the theorist to extend the
theoretical mechanisms, but care must be taken to ensure that
mechanisms outside the theoretical framework do not play a causal
role in the robot behavior.
Robotic modeling of living systems has taken a number of different forms, from behavioral modeling (see, e.g., Webb in Gaussier, 1996, pp. 117–134, on cricket phonotaxis; Grasso et al. in
Chang and Guadiano, 2000, pp. 115–131 on lobster chemotaxis)
to neuroscientific modeling (e.g., Burgess et al. in Ziemke and
Sharkey, 1998, pp. 291–300, and Recce et al. in Sharkey, 1997,
pp. 393–406, on the rat hippocampus; van der Smagt in Ziemke
and Sharkey, 1998, pp. 301–320, on the human cerebellum for arm
control) to modeling animal sensing (e.g., Lambrinos et al. in
Chang and Guadiano, 2000, pp. 39–64, on ant solar compass sensing; Blanchard et al. in Chang and Guadiano, 2000, pp. 17–38, on
locust sensing of approach; Rucci in Chang and Guadiano, 2000,
pp. 181–193, on localization of auditory and visual structures in the
barn owl) to biomechanics (e.g., Delcomyn and Nelson in Chang
and Guadiano, 2000, pp. 5–15, and Quinn and Ritzman in Ziemke
and Sharkey, 1998, pp. 239–254, on hexapod walking in the
cockroach).
One of the most successful attempts at behavioral modeling has
been the work of Webb and her associates (e.g., Webb in Gaussier,
1996, on mate selection in the female cricket). A wheeled robot
was used to physically model a female cricket locating a conspecific male by following its calls. The robot was equipped with an
auditory system capable of selectively localizing the sound of a
male cricket stridulating (rubbing its wings together rapidly to produce a sound that attracts potential mates).
A similar approach has been taken by Lambrinos et al. (in Chang
and Gaudiano, 2000, pp. 39–64) for modeling the sensors of the

Biologically Inspired Robotics
desert ant Cataglyphus, which maintains its heading across a
largely featureless desert using polarized light sensing. Lambrinos
et al. built special-purpose polarized light sensors based on what
is known about the neural mechanisms of polarization that the
honey bee Apis mellifera, the field cricket Gyrllus campestris, and
the desert ant Cataglyphus bicolor use to determine the position of
the sun. The sensors were mounted on a wheeled robot and used
to test different models of how Cataglyphus maintains its heading
with polarized light. The research has been successfully conducted
on a mobile robot in the ant’s natural habitat with a homing performance similar to that of the ant.
As in Webb’s work, the “ant robot” was used to model only a
small part of the whole process of finding the direction to the nest.
It did not, for example, accommodate the movement of the sun
across the sky during the day (although this information was used
to make corrections to the data). The sun moves relative to Earth
at an average of 15 per hour (this figure varies greatly according
to the time of day). In the early part of the twentieth century, this
fact was used to show that ants both memorized the position of the
sun and compensated for its movement. When the ants are imprisoned in a dark box for 21⁄2 hours and released, they deviate from
their original bearing by approximately the same number of degrees
as the sun moved during their imprisonment. These findings reveal
that Cataglyphus keeps track of the azimuth during the day and
uses this information in maintaining a course.
Another important aspect of robotics used for modeling concerns
legged locomotion. This leads to a two-way interaction between
model testing and engineering. A number of researchers have
turned to insect locomotion as a way to find a type of gait for a
legged robot. Quinn and Ritzmann (in Ziemke and Sharkey, 1998,
pp. 239–254) have designed and built a hexapod robot based on
detailed neurobiological and kinematic observations of the locomotion of the death’s head cockroach, Blaberus discoidalis. As a
result, the robot’s kinematics are remarkably similar to those of the
real cockroach, and issues addressed in controlling the artificial
cockroach have actually lead to new understanding of its natural
counterpart.
Moving onto the mammalian nervous system, Burgess, Donnett,
and O’Keefe (in Ziemke and Sharkey, 1998, pp. 291–300) used a
miniature mobile robot equipped with a camera to test a neuronal
model of how internal and external sensory information contribute
to the firing of place cells in the rat hippocampus, and how these
cells contribute to rat navigation behavior. They tested hypotheses
based on their earlier neurophysiological work on the rat hippocampus using single-cell recording techniques. The robot experiments showed that the information provided by the robot’s onboard video, odometry, and proximity sensors was sufficient to
allow reasonably accurate return to an unmarked goal location.
Similar robot modeling work has also been carried out by Recce
et al. (in Sharkey, 1997, pp. 393–406) using the hippocampus as a
method of absolute localization.
Research in specific biorobotics is gathering momentum as robot
and sensing technology continues to improve. There are still many
modeling issues to be worked out in conjunction with biology. The
next step would be to get the morphology of robots to more accurately model the bodies and movement of the target species and
to work continuously toward the goal of modeling whole animals,
rather than installing patches to cover the missing bits. Like computational modeling, great care must be taken to ensure that the
patches do not have a causal role in the target behavior.

Theoretical Biorobotics
Theoretical biorobotics is the most abstract level of biologically
inspired robotics. Essentially, theoretical biorobotics is an all-

163

encompassing category for work that does not involve implementation on a robot but rather addresses metaquestions about robotics.
Although wide-ranging, the main theoretical focus of biologically
inspired robotics concerns biological and psychological issues.
Many of these issues draw on detailed philosophical reasoning.
Here we will confine ourselves to setting out some of the main
points and referencing more detailed works in the literature.
A strong impetus for the new wave in biologically inspired robotics was the way it differed from traditional AI. Rodney Brooks,
one of the prime movers in the mid-1980s, was concerned with the
inadequacy of the prevailing methods used in AI for robotics (see,
e.g., Brooks, 1999). Based mainly on the cognitivist conception of
human intelligence, the sensory input to robots went through a
number of strategic stages such as perception, planning, and reasoning before each move. All of the information was presented to
a central controller, which decided how to act. This slowed performance to a single small move about every 15 minutes.
Rejecting cognitivism, theoretical biorobotics views intelligence
as embodied in the machine and in its interactions with the world
in which it is situated. Extreme cognitivists hold that mind is essentially a computer program, a language of thought, that could be
run on any machine capable of running it. Mind is simply linked
to the machine running it and the external world through transducers. Extreme bioroboticists might claim that mind is inseparable
from the individual machine and the more encompassing environmental machine of which the individual machine is a part. That is,
the robot is situated in the world and is an embodied or physically
grounded intelligence.
Varela, Thompson, and Rosch (1991) provide an insightful discussion of the details of embodiment in robots and its relation to
life and mind. These authors are primarily interested in how living
systems are embodied and how they are situated in their interactions with the world. Their purpose is to urge cognitive science to
reject the vacuity of ungrounded thought. However, Sharkey and
Ziemke (in Ziemke and Sharkey, 1998), while going along with
some of the account by Varela et al. of living systems, argue for a
weak embodiment in robotics, i.e., that robots can be used to model
embodiment without themselves being embodied (see PHILOSOPHICAL ISSUES IN BRAIN THEORY AND CONNECTIONISM).
Another idea that has received considerable attention is that of
emergence or emergent behavior. This is the notion that we can
get something for nothing (or very little). One analogy is that from
a collection of many molecules of water a cloud emerges that is
greater than the sum of the parts. Perhaps a better example is the
emergence of collective behavior in insects when each insect carries out very simple behaviors. For example, it is argued that the
extraordinary structures that termites build in the desert emerge
from very simple behaviors. Clark (1997) provides an in-depth discussion of emergent behavior and describes the two rules required
by the termites: “If not carrying anything and you bump into a
wood chip, pick it up”; and “If carrying a wood chip and you bump
into another one, put it down.” The resultant piling behavior
emerges from the interplay between simple rules and the constraints of the environment.
The idea, then, is that coherent behavior emerges from a collection of simple taxes working together at the same time. This was
an outright rejection of the notion of a central controller for action
that was prevalent in AI. The idea in AI was to provide the robot
with a model of the world, whereas one of the favorite slogans of
the new roboticists is “the world is its own model.” Nonetheless,
Sharkey and Ziemke (2001) caution that even the taxes are emergent in the sense that they are in the eye of the beholder; i.e., they
are distal descriptions of behavior.
One of the healthiest signs in the field is that some mainstream
biologists and psychologists have begun to write about the relationship between specific biological findings and robotics. Navi-

164

Part III: Articles

gation, for example, is an important topic in both biology and robotics, and biologists (e.g., Collett in Ziemke and Sharkey, 1998,
pp. 255–270; Etienne in Ziemke and Sharkey, 1998, pp. 271–290;
Franz and Mallot in Chang and Guadiano, 2000, pp. 133–153) have
discussed the relation between different aspects of navigation from
an insect and mammalian perspective. Moreover, psychologists are
beginning to take robot studies using animal learning techniques
seriously enough to write detailed discussions of the relationship
between the natural and the metallic (e.g., Savage in Ziemke and
Sharkey, 1998, pp. 321–340).

Conclusions
The field of biologically inspired robotics has been classified into
the three separate subfields of generalized, specific, and theoretical.
General and theoretical biorobotics has a long but patchy history
that is now a considerable and growing field. Specific biorobotics
has gradually emerged from the other two and is fast making headway toward the goal of accurately modeling specific animal species. With the ever-increasing improvements in materials, sensors,
and computing equipment, we can look forward to many exciting
new developments over the coming decade and the transfer of the
findings into engineering.
Road Map: Robotics and Control Theory
Related Reading: Arm and Hand Movement Control; Neuroethology,
Computational; Potential Fields and Neural Networks; Reactive Robotic
Systems

References
Braitenberg, V., 1984, Vehicles: Experiments in Synthetic Psychology,
Cambridge, MA: MIT Press. ⽧
Brooks, R., 1999, Cambrian Intelligence: The Early History of the New AI,
Cambridge, MA: MIT Press.
Chang, C., and Gaudiano, P., Eds., 2000, Biomimetic Robotics, Robot. Auton. Syst., 31(1–2):1–218 (special issue).
Clark, A., 1997, Being There: Putting Brain, Body and World Together
Again, Cambridge, MA: MIT Press.
Dorigo, M., and Colombetti, M., 1998, Robot Shaping: An Experiment in
Behavior Engineering, Cambridge, MA: MIT Press.
Floreano, D., and Nolfi, S., 1997, Adaptive behaviour in competing coevolving species, in Proceedings of the Fourth European Conference on
Artificial Life (P. Husbands and I. Harvey, Eds.), Cambridge, MA: MIT
Press.
Gaussier, P., Ed., 1996, Moving the Frontiers Between Robotics and Biology, Robot. Auton. Syst., 16:107–362 (special issue).
Grey Walter, W., 1953, The Living Brain, New York: Norton. ⽧
Krose, B., Ed., 1995, Special issue on reinforcement learning and robotics.
Robot. Auton. Syst., 15:233–340.
Nolfi, S., and Floreano, D., 1999, Learning and evolution, Auton. Robots,
7:89–113.
Sharkey, N., Ed., 1997, Robot Learning: The New Wave, Robot. Auton.
Syst., 22(3–4):135–274 (special issue). ⽧
Sharkey, N., 1998, Learning from innate behaviors: A quantitative evaluation of neural network controllers, Auton. Robots, 5:317–334.
Sharkey, N., and Ziemke, T., 2001, Mechanistic vs. phenomenal embodiment: Can robot embodiment lead to strong AI? Cognit. Syst. Res.,
2:251–262.
Varela, F., Thompson, E., and Rosch, E., 1991, The Embodied Mind: Cognitive Science and Human Experience, Cambridge, MA: MIT Press.
Ziemke, T., and Sharkey, N., Eds., 1998, Biorobotics, Connect. Sci., 10(3–
4):161–360 (special issue).

Biophysical Mechanisms in Neuronal Modeling
Lyle J. Graham
Introduction
Models of single neurons span a wide range, with more or less
fidelity to biological facts (see PERSPECTIVE ON NEURON MODEL
COMPLEXITY; SINGLE-CELL MODELS; MECHANISMS IN NEURONAL
MODELING). So-called biophysically detailed compartmental models of single neurons typically aim to quantitatively reproduce
membrane voltages and currents in response to some sort of “synaptic” input. We may think of them as Hodgkin-Huxley-Rall models, based on the hypothesis of the neuron as a dynamical system
of nonlinear membrane channels (e.g., conductances described by
Hodgkin-Huxley kinetics; see ION CHANNELS: KEYS TO NEURONAL SPECIALIZATION; AXONAL MODELING) distributed over an
electrotonic cable skeleton (e.g., as described by Rall dendritic cable theory; see DENDRITIC PROCESSING).
Such models can incorporate as much biophysical detail as desired (or practical), but, in general, all include some explicit assortment of voltage-dependent and transmitter-gated (synaptic)
membrane channels. Many Hodgkin-Huxley-Rall models also include some system for describing intracellular Ca2Ⳮ dynamics, for
example, to account for the gating of Ca2Ⳮ-dependent KⳭ channels. Modeling these dynamics involves not only Ca2Ⳮ channels
but often associated buffer systems and membrane pumps as well.
This article summarizes the application of the more common
mathematical models of these basic biophysical mechanisms
(Borg-Graham, 1999; Koch, 1999). The models for each of these
mechanisms are at an intermediate level of biophysical detail, appropriate for describing macroscopic variables (e.g., membrane

currents, ionic concentrations) on the scale of the entire cell or
anatomical compartments thereof.
First, we will discuss general issues regarding model formulations, and data interpretation for contructing models of biophysical
mechanisms. We will then describe models for nonlinear channel
properties, including Hodgkin-Huxley and Markov kinetic descriptions of voltage and second-messenger-dependent ion channels.
Similar models aimed particularly for synaptic mechanisms are
covered in SYNAPTIC INTERACTIONS. We will then discuss concentration systems, including models of membrane pumps and concentration buffers. Finally, examples of model definitions are illustrated using the Surf-Hippo Neuron Simulation System
(Graham, 2002), pointing out an essential and minimal syntax that
facilitates model documentation and analysis.

General Issues for Constructing Biophysical Models
Phenomenological and Mechanistic Models
A first consideration in choosing a mathematical model for a given
cellular mechanism is whether the model is intended only to capture an empirical relationship between an independent variable (the
input or signal) and a dependent variable (the output or response),
or whether the model represents an explicit mechanistic hypothesis.
Phenomenological models may be instantiated by a function with
few (e.g., a low-dimensional polynomial fit) or many (e.g., look-up
table) degrees of freedom, depending on the nature of the problem.

Biophysical Mechanisms in Neuronal Modeling
Of course, a mechanistic model can also have few or many parameters, but explanatory power tends to dimish with the number of
parameters. The mechanistic and phenomenological model alternatives are not mutually exclusive, since the former may incorporate the latter, and in some ways the distinction between them is
rather ad hoc.

Static (Instantaneous) and Dynamic (Kinetic) Models
Another basic consideration is whether the relation between signal
and response is instantaneous on the time scale relevant to the entire
system at hand. In some cases an instantaneous mechanism may
permit analysis by exploiting separation of variables, for example
assuming instantaneous activation for NaⳭ currents relative to KⳭ
currents during spiking (see OSCILLATORY AND BURSTING PROPERTIES OF NEURONS). For cellular models that are solved by explicit integration over time, however, instantaneous relationships
between state variables can introduce troublesome numerical instabilities unless there are intervening kinetics with slow time constants (relative to the time scale of the integration) that serve to
“decouple” element dynamics at the faster time scale.

Deterministic and Stochastic Models
A stochastic component, or “noise,” in experimental measures is
ubiquitous, for example, in the trial-to-trial variability of spike responses to deterministic stimuli, or in membrane voltage or membrane current fluctuations (especially in vivo). There is accumulating experimental and theoretical evidence that noise places an
important constraint on information processing under some conditions while conversely serving a useful computational role in
others.
Some system noise can be traced to the inherent stochasticity of
molecular kinetics at the cellular level, and there is increasing interest in analyzing single-neuron models that explicitly consider
this contribution. For example, simulations with stochastic Hodgkin-Huxley-type channel models can show functional dynamics
that would be completely missed by deterministic models. A deterministic approximation should be valid when the number of
channels is very large, the usual assumption, but the actual number
in a local region of the neuron membrane may be rather low, considering both realistic estimates of channel densities and, especially, the small number of open channels near spike threshold
(Schneidman, Freedman, and Segev, 1998).

The Experimenter’s Model Versus the Theorist’s Model
Every model is based on some empirical data set, but an often
overlooked point is how the theorist’s model relates to, or rather is
constrained by, that of the experimentalist. It may be a bit surprising to discover there is such a thing as an experimentalist’s model
(which is not the same thing as an experimental model). However,
in reality, experimental data are never arbitrary but reflect the experimenter’s explicit or implicit notion of either the necessary and
sufficient parameters of a phenomenon, the functionally relevant
mappings between signal and response, what is experimentally
tractable (no one is able to do his or her “dream” experiment!), or
some combination of all three. The first issue, in particular, is essentially equivalent to assuming some hypothetical model, but importantly, the associated experiments are not normally designed for
testing that hypothesis (since it is taken as an a priori). Examples
include electrophysiological reports on whole-cell current kinetics,
which usually focus on voltage-dependent activation and inactivation characteristics according to the classical model of Hodgkin
and Huxley (described below). However, this paradigm, while
practical, may miss crucial functional characteristics, basically by

165

not sufficiently characterizing certain important dynamical trajectories. We shall return to this point later in discussing Markov
channel models.

Channel Models
Membrane channels underlie both intrinsic neuronal excitablility
and the direct postsynaptic action of synaptic transmission. The
channel current I, assuming some permeant ion X, may be expressed as the product of a conduction term f (V, D[X]) and a gating
term h(V, t, . . .):
I ⳱ f (V, D[X])h(V, t, . . .)
where V is the membrane voltage, t is time, and D[X] represents
the concentration gradient of X across the cell membrane. The ellipsis in the argument of h() stands for the various ligand-dependent
processes, for example, Ca2Ⳮ dependence or the action of synaptic
neurotransmitters.

Ohmic and Permeation Conduction Models
The two common models of the conduction term f () are the ohmic
model (thermodynamic equilibrium conduction) and the constantfield permeation model (nonequilibrium conduction). In the ohmic
model, current is proportional to the difference of the membrane
voltage and the reversal potential for I:
f (V, D[X]) ⳱ ḡX(V ⳮ EX)
where ḡX is the maximum conductance. The reversal potential EX
for the ion X is given by the Nernst equation:
EX ⳱

ⳮRT
[X]out
Log
zF
[X]in

where R is the gas constant, F is Faraday’s constant, and T is temperature in degrees Kelvin. [X]in and [X]out are the intracellular and
extracellular concentrations, and z is the valence of the permeant
ion X. For more than one permeant ion (all with the same valence)
and under some assumptions, the similar Goldman-Hodgkin-Katz
voltage equation (e.g., Hille, 2002) may be used. Note that if the
effect of channel current on [X] is considered (see section on concentration integration), then the ohmic f () is in fact implicitly
nonlinear.
As the conducting ion moves farther from equilibrium (specifically the case for Ca2Ⳮ), the ohmic model becomes less accurate.
A widely used nonequilibrium model is the constant field model,
described by the Goldman-Hodgkin-Katz current equation. In this
equation (Jack, Noble, and Tsien, 1983; Hille, 2002), the nonlinearity of permeation is explicit:
f (V, D[X]) ⳱ p̄X

Vz2F 2 [X]in ⳮ [X]out exp(ⳮzFV/RT)
RT
1 ⳮ exp(ⳮzFV/RT)

where p̄X is the permeability (not the conductance) of the channel
(typically in cm3/s). Note that at membrane potentials far from the
reversal point (e.g., ⳮ20 mV for Ca2Ⳮ channels) the GoldmanHodgkin-Katz current equation becomes linear, and thus the ohmic
model may suffice if the model voltages are appropriately bounded.

Channel Gating à la Hodgkin and Huxley: Independent
Voltage-Dependent Gating Particles
Hodgkin and Huxley (1952) described channel gating as an interaction between independent two-state (open and closed) elements
or “particles,” all of which must be in the open state for channel
conduction (see ION CHANNELS: KEYS TO NEURONAL SPECIALIZATION; AXONAL MODELING). The state dynamics of each particle
are described with first-order kinetics:

166

Part III: Articles
␣(V),b(V)

xC

s

xO

(1)

where xC and xO represent the closed and open states of gating
particle x, respectively. ␣(V) and b(V) are the forward and backward
rate constants of the particle as a function of voltage, respectively.

An Extended Hodgkin and Huxley Model
While Hodgkin and Huxley hypothesized that the steady-state behavior of each particle fit a Boltzmann distribution, their underlying
rate equations were essentially ad hoc fits to the experimental data.
Although taken as a canonical form by countless cell models, one
consequence is that there is not an obvious relationship between
the equations’ parameters and the more “observable” steady-state,
x(V), and time-constant, sx(V), functions associated with Equation
1.
The Hodgkin-Huxley model can be recast in more explicit form
by considering parameters of a single-barrier kinetic model for each
particle (Jack et al., 1983; Borg-Graham, 1991, 1999). In its basic
form this formulation has five parameters for each particle, compared to six parameters in the Hodgkin-Huxley model. Nevertheless, this formulation may be readily fitted to the original HodgkinHuxley equations of squid axon INa and IK; the error is comparable
to the error between the original equations and the data to which
they were fit (cf. Figures 4, 7, and 9 in Hodgkin and Huxley, 1952).
We first derive the expressions the forward, ␣x⬘(V), and backward,
b⬘x(V), rate constants of the single-barrier transition. The parameter
z (dimensionless) is the effective valence of the gating particle:
when positive (negative), the particle opens (closes) with depolarization; thus it is an “activation” (“inactivation”) particle. The effective valence is the product of the actual valence of the particle
and the proportion of the membrane thickness that the particle
moves through during state transitions. c (dimensionless, between
0 and 1) is the asymmetry of the gating particle voltage sensor
within the membrane (symmetric when c ⳱ 0.5). K is the leading
rate coefficient of both ␣x⬘(V) and b⬘x(V). This term can be described
in terms of Eyring rate theory, but here we just take K as a constant.
V1/2 is the voltage for which ␣x⬘(V) and b⬘x(V) are equal. The final
equations for ␣⬘x(V) and b⬘x(V) are then:

␣x⬘(V) ⳱ K exp

冢zc(V ⳮRTV )F冣

b⬘(V)
⳱ K exp
x

ⳮ V )F
冢ⳮz(1 ⳮ c)(V
冣
RT

1/2

1/2

An additional parameter, s0 (not the passive membrane time constant), is crucial for fitting the expressions to the original HodgkinHuxley equations. s0 represents a rate-limiting step in the state
transition, for example, “drag” on the particle conformation change
(similar considerations have been explored for other, more general,
kinetic schemes; e.g., Patlak, 1991), and may be incorporated directly into the expression for the time constant sx(V). x(V), however, is not affected by s0:
sx(V) ⳱

1
Ⳮ s0
␣x⬘(V) Ⳮ b⬘(V)
x

x(V) ⳱

␣x⬘(V)
␣x⬘(V) Ⳮ b⬘(V)
x

Two additional parameters, ␣0 and b0, may be considered in some
cases, although they are not necessary in reproducing the original
Hodgkin-Huxley equations. These parameters are voltage-independent forward and backward rate constants, respectively, of parallel
state transitions. If considered, these transitions will change the
final forms of sx(V) and x(V).

The parameters of this form have clear relationships to the corresponding x(V) and sx(V) functions. Thus, the V1/2 parameter
gives the midpoint and z sets the steepness of the x(V) sigmoid.
The symmetry parameter c determines the skew of sx(V): c ⳱ 0.5
gives a symmetric bell-shaped curve for sx(V), which otherwise
bends to one side or the other as c approaches 0 or 1. z sets the
width of sx(V), unless c is equal to either 0 or 1, in which case sx(V)
becomes sigmoidal and thus z sets the steepness as for x(V).
With this scheme a particle with a voltage-independent rate constant can be represented by setting 1/K K s0 (and ␣0 ⳱ b0 ⳱ 0),
thus making s0 the effective time constant. Likewise, both the time
constant and steady state become voltage independent by setting K
⳱ 0 and choosing the appropriate ␣0 and b0.

Determining the Number of Particles
in Hodgkin-Huxley Models
The Hodgkin-Huxley paradigm includes the possibility of multiple
gating particles of a given type associated with a given channel.
Some experimental papers report fitting integer powers of hypothetical gating particles to the observed kinetics, but more typically
steady-state activation or inactivation data are simply the observed
macroscopic behavior (that is, reflecting the steady state of the
ensemble of particles). Thus, gating particle powers for channel
models can often be considered as a free parameter.
Gating particle powers greater than 1 have several kinetic consequences, including a sigmoidal “delayed” time course of activation (Hodgkin and Huxley, 1952), a more rapid approach to 0 in
the steady-state characteristic as a function of voltage, and a shift
in either the peak (when 0  c  1) or inflection point (when c ⳱
0 or 1) of sx(V) in the direction of voltage for which x(V) tends to
0.

Channel Gating as Dynamical Systems
à la Markov Models
The independence and simplicity of the Hodgkin-Huxley gating
particle models have at least two advantages: model kinetics can
be predicted in an intuitive way, and their numerical evaluation is
efficient (Hines, 1984). In addition, as mentioned, electrophysiological measures of whole-cell currents are often guided by this
model. The two-state gating model can also be readily adapted to
include factors such as intracellular [Ca2Ⳮ], by using the appropriate functions for ␣ and b (see below).
On the other hand, the independence of the two-state HodgkinHuxley particles constrains the equivalent state space description
(e.g., allowed state transitions) given by the more general Markovian model (see SYNAPTIC INTERACTIONS). General Markov kinetic
models are standard for detailed biophysical analysis of singlechannel kinetics, but there have been relatively few applications in
the neural modeling literature. One practical limitation is that Markov models are often much more computationally expensive than
the Hodgkin-Huxley model. Nevertheless, the richer dynamics of
Markov models may prove necessary for capturing functional properties of some channel types, including subthreshold steady-state
NaⳭ channel rectification (Figure 4), delay of activation for NaⳭ
and KⳭ currents, and the coupling between opening of KⳭ channels by Ca2Ⳮ entering during the action potential and subsequent
inactivation (Borg-Graham, 1999).
Although the Markovian framework puts no restrictions on the
functions that define state transitions (other than the no-memory
condition), the form presented above of the ␣(V) and b(V) functions
for the extended Hodgkin-Huxley model is convenient and very
general. Another form is the following squeezed exponential formula for the transition rate ␣ij(V) from state i to state j:

Biophysical Mechanisms in Neuronal Modeling

冢

冢

␣ij(V) ⳱ smin Ⳮ (smax ⳮ smin)ⳮ1 Ⳮ exp

(V ⳮ V1/2)
k

冢

ⳮ1 ⳮ1

冣冣 冣

(2)
where the inverse of smin (analogous to s0 in the extended HodgkinHuxley model) and smax put upper and lower bounds, respectively,
on the rate constant ␣ij(V). Note that there is an implicit coefficient
of the exponential term of 1/ms (same units as either 1/smin or 1/
smax) in this equation.

where, e.g., ␣V(V) is the squeezed exponential function of voltage
in Equation 2, such that the forward reaction speeds up with
depolarization.
Moczydlowski and Latorre (1983) proposed a detailed Markovian kinetic scheme for the Ca2Ⳮ- and voltage-dependent gating of
the BK channel, which has been interpreted in several neuron models. The essential dynamics are captured by a two-state scheme as
in Equation 1, with rate constants dependent on both voltage and
Ca2Ⳮ, thus:

冢

k1(V)
[Ca2Ⳮ]in

冢

[Ca2Ⳮ]in
k4(V)

ⳮ1

冣

b(V, [Ca2Ⳮ]in) ⳱ b0 1 Ⳮ

Ca2Ⳮ-Dependent Gating
Neural models have used a variety of explicit relationships between
the concentration of some second messenger and the activation
state of the target mechanism. Here we consider a range of examples that have been used to describe Ca2Ⳮ-dependent KⳭ channels.
A simple instantaneous model for Ca2Ⳮ-dependent gating is
given by a static rectified power function of concentration with
some threshold hCa, reminiscent of firing rate models:

167

␣(V, [Ca2Ⳮ]in) ⳱ ␣0 1 Ⳮ

ⳮ1

冣

where
ki(V) ⳱ ki(0) ⳯ exp

ⳮVdiFZ
RT

冢

冣

w ⳱ K ⳯ r([Ca ] ⳮ hCa)
2Ⳮ n

where r(x) ⳱ 0 for x  0, and r(x) ⳱ x otherwise, for the gating
variable w.
A simple kinetic model for Ca2Ⳮ-dependent gating can be described by the following reaction. Assume that wC and wO represents the closed and open probabilities, respectively, of a Ca2Ⳮdependent gating particle w with forward and backward rate
constants ␣ and b, respectively:
␣,b

wC Ⳮ nCa2Ⳮ s wO
Note that the open state wO is bound to n Ca2Ⳮ ions. One can also
imagine a similar but reverse reaction for the description of Ca2Ⳮdependent inactivation, as has been reported for some Ca2Ⳮ channels. In the more general Markovian framework, wC and wO refer
to two adjacent states out of the entire state space. This scheme
assumes that binding with Ca2Ⳮ ions is cooperative: either all binding sites are occupied or none are. We may also consider a s0
parameter as in the extended Hodgkin-Huxley model. If we assume
that the binding of Ca2Ⳮ in this reaction does not appreciably
change [Ca2Ⳮ], then the steady-state value for w, w, and the time
constant for the kinetics, sw, are given by:
w ⳱

␣
␣ Ⳮ b[Ca2Ⳮ]inⳮn

sw ⳱

1
Ⳮ s0
␣[Ca2Ⳮ]inn Ⳮ b

An important distinction is whether or not a Ca2Ⳮ-dependent
channel is also dependent on voltage. For example, in recordings
of the large-conductance Ca2Ⳮ-dependent KⳭ (BK) channel, Barrett, Magleby, and Pallotta (1982) found an approximate thirdpower relationship between channel open times and [Ca2Ⳮ] that
was strongly facilitated by depolarization. At a membrane voltage
of 10 mV, channels became open with a [Ca2Ⳮ] threshold of about
1 lM.
If the dependences are separable, it may be convenient to consider a product of voltage-only and Ca2Ⳮ-only gating terms, for
example according to the formulations presented earlier. Otherwise, a single gating “particle” must take into account both voltage
and Ca2Ⳮ. A direct voltage dependence of the simple kinetic
scheme above can be added in a number of ways, for example by
adding a voltage-dependent term to the forward rate constant, now
defined as ␣(V, [Ca2Ⳮ]in):

␣(V, [Ca2Ⳮ]in) ⳱ ␣V (V) ⳯ ␣[Ca2Ⳮ]inn

Ionic Concentration Dynamics
An inevitable consequence of channel currents is that the concentrations on either side of the membrane will change as a function
of electrical activity. In addition to the negative feedback on channel currents already mentioned (as a result of a reduction in driving
force), such changes can have a variety of other functional consequences. These include the activation of intracellular or extracellular receptors, the most important being those that underlie the
myriad Ca2Ⳮ-dependent pathways (including the Ca2Ⳮ-dependent
channel gating just described). We may also consider the role of
the membrane pumps, which tend to maintain ionic gradients (see
MECHANISMS IN NEURONAL MODELING), and of the intracellular
buffer systems. In the following discussion we emphasize Ca2Ⳮ
dynamics, but similar considerations are relevant for other ions.

Concentration Integrators
Most neuron models that consider concentration changes rely on
some partition of the extracellular and intracellular space into a set
of well-mixed compartments (e.g., “shells”), with or without an
“inifinite” compartment with a fixed concentration. Simple diffusion is normally assumed between compartments, according to the
geometry of the partitioning and assumptions about the diffusion
coefficient for the free ion, D. Compartments adjacent to the cell
membrane also take into account ion flow across the membrane,
e.g., due to channels and pumps. The physical partitioning into
compartments depends on the question being addressed, with the
simplest system being a single intracellular compartment (extracellular concentration being assumed constant).
Any model of [Ca2Ⳮ] must take into account not only the influx
of Ca2Ⳮ but also some mechanism for the removal of Ca2Ⳮ. The
simplest method is to include a steady-state term in the differential
equation(s) describing [Ca2Ⳮ]. In the general case this value is
associated with a second parameter corresponding to the time constant for concentration decay.

Membrane Pump Models
More explicit models of ion removal includes mechanisms, such
as membrane-bound pumps, that transport Ca2Ⳮ, KⳭ, NaⳭ, and
other ions against their respective concentration gradients. A general pump model may be described with a Michaelis-Menton mechanism, assuming no appreciable change in the extracellular [Ca2Ⳮ]:

168

Part III: Articles
JCa2Ⳮ ⳱ Vmax

[Ca2Ⳮ]in
Kd Ⳮ [Ca2Ⳮ]in

ⳮ Jleak

where JCa2Ⳮ is the removal rate of Ca2Ⳮ per unit area, Vmax is the
maximum flux rate per unit area, and Kd is the half-maximal
[Ca2Ⳮ]in. Jleak compensates for the resting pump rate and is typically adjusted so that there is no net pump current at rest, given
some resting activation of Ca2Ⳮ channels.
For example, the spine model by Zador, Koch, and Brown (1990)
included two Ca2Ⳮ pumps with Michaelis-Menton kinetics: one
high-affinity, low-capacity, corresponding to a Ca ATPase-driven
mechanism, and the other low-affinity, high-capacity, corresponding to a Ca2Ⳮ/NaⳭ exchange mechanism (see also Koch, 1999).
These pumps were treated as separate currents in the [Ca2Ⳮ] differential equation. Other models have incorporated a pump that
binds to intra- and extracellular Ca2Ⳮ with various rate constants.
These reactions are then solved simultaneously with another binding reaction between [Ca2Ⳮ]in and a buffer.

bBu ⳱

bound
[Ca2Ⳮ]in
free
[Ca2Ⳮ] in

and thus is a function of [Bu]. This mechanism implies that the
measured [Ca2Ⳮ]in is equal to the total [Ca2Ⳮ]in divided by (bBu Ⳮ
1). For nondiffusional models of [Ca2Ⳮ]in (e.g., where there is one
Ca2Ⳮ compartment per electrical compartment), this is the only role
of the instantaneous buffer. For multiple-compartment systems, the
effective diffusion constant D⬘ applied to the difference in [Ca2Ⳮ]
between compartments must also be adjusted to take into account
the instantaneous buffer, by setting D⬘ equal to D/(bBu Ⳮ 1). A
variation on this scheme would be to assume that bBu is a function
of each compartment. In this case the diffusion equation between
compartments would reference the original D, with the concentration difference between any two compartments determined by the
difference of the total concentrations, weighted by the appropriate
bBus.

Practical Aspects of Coding Biophysical Models
Buffer Models
Endogenous intracellular Ca2Ⳮ buffers have a strong effect on free
intracellular [Ca2Ⳮ]in. Cell models that consider Ca2Ⳮ dynamics
have incorporated buffer mechanisms of varying complexities, including solving the dynamical equations for the buffer-Ca2Ⳮ reaction during the course of the simulation. Note that in some models an explicit (instantaneous) buffer mechanism is replaced by
adjusting Ca2Ⳮ sensitivities of Ca2Ⳮ-dependent mechanisms, such
as Ca2Ⳮ-dependent KⳭ channels.
A simple way to treat intracellular buffering of Ca2Ⳮ is to assume
a nonsaturated buffer (i.e. [Bu] k [Ca2Ⳮ]in, where [Bu] is the concentration of buffer binding sites) with instantaneous kinetics. The
key parameter, bBu, in this mechanism equals the ratio of the concentration of bound Ca2Ⳮ and free Ca2Ⳮ:

(CHANNEL-TYPE-DEF
‘(NA-HH
(GBAR-DENSITY . 1200) ; pS/um2
(E-REV . 50)
; mV
(V-PARTICLES . ((M-HH 3) (H-HH 1)))))
(PARTICLE-TYPE-DEF
‘(M-HH
(CLASS . :HH)
(ALPHA . (LAMBDA (VOLTAGE)
(/ (* -0.1 (- VOLTAGE -40))
(1- (EXP (/ (- VOLTAGE -40) -10)))))))
(BETA . (LAMBDA (VOLTAGE)
(* 4 (EXP (/ (- VOLTAGE -65) -18)))))))

Figure 1. The Surf-Hippo definitions of the classical Hodgkin-Huxley
model of the squid axon NaⳭ channel and the associated M-activation
gating particle. The plethora of parentheses may seem daunting; however,
all formatting (including indentation) is done automatically by Lisp-savvy
editors (such as Emacs). The last line in the CHANNEL-TYPE-DEF form
specifies three M-HH and one H-HH gating particles. Surf-Hippo model
definitions allow concise inclusion of arbitrary functions, in this case for
the ALPHA and BETA rate constants (refer to equations of the HodgkinHuxley NaⳭ channel in AXONAL MODELING, q.v.) in the PARTICLETYPE-DEF form. The LAMBDA symbol denotes the beginning of a function
definition. In the context of gating particle definitions, Surf-Hippo assumes
only that the rate functions take a single VOLTAGE argument (in millivolts),
and return a rate value (in msⳮ1). Comments are indicated with a semicolon.
Expression precedence is unambiguous with the prefix notation of Lisp.
The first element of each (parenthesized) list defines the operation applied
to the rest of the list, and in nested lists everything is evaluated from the
inside out: e.g., (Ⳮ A (* B C) D) is A Ⳮ BC Ⳮ D.

The translation of experimental data on some biophysical mechanism into simulator code, within the framework of a given mathematical model, and the reverse process (which is a necessary step
in formulating experimental predictions from a model) have received little attention. However, these steps have many practical
aspects, not the least of which is that as models become more and
more complex, the opportunity for errors becomes more and more
serious. For this reason it is useful to consider model syntax (see
NEUROSIMULATION: TOOLS AND RESOURCES; GENESIS SIMULATION SYSTEM; NEURON SIMULATION ENVIRONMENT; NSL NEURAL SIMULATION LANGUAGE).
In most situations, it is desirable that a simulator program act
essentially as a “black box,” so that model analysis concerns only
the input (some collection of model definition files) and the output
(numerical data, usually time sequences). Thus, when composing
the model definition, one should ideally be able to focus on the
model algorithms and their parameters, rather than on their implementation. Model syntax should therefore allow the expression of
mathematical (and symbolic, if appropriate) relationships in as
close to a “natural” syntax as possible. In other words, one should
be able to simply “write down the equations” defining the model.
Certainly a practical consequence of such a syntax is that the learning curve for the simulator is reduced, but more important over the
long term is simply that if model definitions are easier to read, they
are also easier to verify, document, and change.
To illustrate these ideas, here we present examples of biophysical
model definitions taken from the Surf-Hippo Neuron Simulation
System (Graham, 2002). This system is written in Lisp, an important point, since the system exploits many advantages of this truly
high-level language that are well known to the AI community (at
the same time having a numerical performance on par with languages such as C). In particular, Lisp supports an emphasis on more

(PARTICLE-TYPE-DEF
‘(M-HH-FIT
(CLASS . :HH-EXT)
(VALENCE . 2.7)
(GAMMA . 0.4)
(BASE-RATE . 1.2) ; 1/ms
(V-HALF . -40)
; mV
(TAU-0 . 0.07))) ; ms
Figure 2. The Surf-Hippo definition for the extended Hodgkin-Huxley
model for the M-activation particle type of the squid axon NaⳭ channel.

Biophysical Mechanisms in Neuronal Modeling
(PARTICLE-TYPE-DEF
‘(NA-X-HPC
(CLASS . :MARKOV)
(STATES . (0 I C1 C2))
(OPEN-STATES . (0)
(STATE-TRANSITIONS .
((O I 3)
(O C1 (SQUEEZED-EXPONENTIAL
(C1 O (SQUEEZED-EXPONENTIAL
(O C2 (SQUEEZED-EXPONENTIAL
(C2 O (SQUEEZED-EXPONENTIAL
(I C1 (SQUEEZED-EXPONENTIAL
(C1 C2 (SQUEEZED-EXPONENTIAL

VOLTAGE
VOLTAGE
VOLTAGE
VOLTAGE
VOLTAGE
VOLTAGE

:V-HALF
:V-HALF
:V-HALF
:V-HALF
:V-HALF
:V-HALF

-51
-42
-57
-51
-53
-60

:K
:K
:K
:K
:K
:K

-2
1
-2
1
-1
-1

:TAU-MIN
:TAU-MIN
:TAU-MIN
:TAU-MIN
:TAU-MAX
:TAU-MAX

169

1/3))
1/3))
1/3))
1/3))
100 :TAU-MIN 1))
100 :TAU-MIN 1)))))

Figure 3. The Surf-Hippo definition of a Markovian gating particle for a
hippocampal pyramidal cell NaⳭ channel model (Borg-Graham, 1999).
Transition rates between states, in msⳮ1, are defined with either constants
(for example, 3 msⳮ1 for the transition from state O to state I) or functions

of voltage (as indicated by the “dummy” variable VOLTAGE). The
SQUEEZED-EXPONENTIAL function (Equation 2) is built into SurfHippo (when :TAU-MAX is not specified, then the mininum rate is 0).

declarative descriptions (emphasizing what kind of model is desired), rather than imperative ones (emphasizing how to construct
a model). Thus, model syntax in Surf-Hippo is designed to minimize the actual code for mechanism specification: correspondingly,
these examples illustrate the necessary and sufficient parameters
for each mechanism, avoiding “overhead” code that would be simulator specific. Surf-Hippo also includes automatic generation of
mechanism definition code, for example, allowing capture of the
“state” of a given mechanism model that has been modified on-line
during automatic or manual parameter exploration. This capability,
which is facilitated by both the minimal requirements for model
specification and the relative ease with which Lisp programs may
be able to write Lisp code, is important for avoiding errors when
documenting model results.
Figure 1 illustrates the definitions of the classical HodgkinHuxley model of the squid axon NaⳭ channel and the associated
M-activation gating particle, showing in particular how arbitary
functions are represented. Once the basic syntax of Lisp is grasped,
the human readability of this format is enhanced because it includes
only the essential kinetic parameters. As a comparison, the equivalent source code in similar simulation systems such as GENESIS
and NEURON is about two to three times larger.

Several parameterized models of biophysical mechanisms are
included in Surf-Hippo, including those discussed in this article.
For example, Figure 2 shows the definition for the extended
Hodgkin-Huxley model of the NaⳭ channel M-gating particle.
Markov models for particle gating are also readily represented in
this system. As an example, Figure 3 illustrates the definition of a
Markovian gating particle for a hippocampal pyramidal cell NaⳭ
channel model (Borg-Graham, 1999); the state diagram is shown
in Figure 4.

Figure 4. State diagram of a hypothetical Markov gating model used for
INa in a model of hippocampal pyramidal cells (Borg-Graham, 1999). From
the single inactivated state I, the two closed states Ci are reached with
increasing hyperpolarization. The Ci r O transitions implement in effect
distinct thresholds, occuring at progressively lower potentials with increasing i. Likewise, the I r C1 and C1 r C2 transitions occur at voltages hyperpolarized to the associated C1 r O and C2 r O transitions, respectively,
somewhat like a rachet mechanism. The O r I transition is voltage independent. The arrows denote the dominant transitions during spike depolarization/repolarization. One important aspect of this model is that the inactivation state is reached only after channel opening, as reported from studies
of single NaⳭ channels (Patlak, 1991; Hille, 1992). Such coupling contradicts the central assumption of independent activation and inactivation kinetics in the Hodgkin-Huxley model.

Discussion
Biophysical details are likely to be crucial for understanding neural
computation (see MECHANISMS IN NEURONAL MODELING). This
process entails an informed trade-off between incorporating every
known experimental nuance of a given cellular mechanism and the
practical application of abstractions and simplifications that capture
essential dynamic relationships between biological molecules and
various neuronal signals. In this article we have presented some of
the more commonly used mathematical descriptions for these relationships. We note in closing that an increasingly important (and
unavoidable) problem with complicated neural models that rely on
these sorts of mechanisms is the lack of formal or analytic verification. This situation calls for alternative methods, in particular the
cross-validation of numerical results using several tools of similar
capability (e.g., NEURON, GENESIS, Surf-Hippo). Practical aspects of coding biophysical models, such as the minimal model
syntax discussed here, should facilitate such efforts.
Road Map: Biological Neurons and Synapses
Related Reading: Activity-Dependent Regulation of Neuronal Conductances; Axonal Modeling; GENESIS Simulation System; Ion Channels:
Keys to Neuronal Specialization; NEURON Simulation Environment;
Neurosimulation: Tools and Resources; NSL Neural Simulation Language; Oscillatory and Bursting Properties of Neurons; Perspective on
Neuron Model Complexity; Single-Cell Models; Synaptic Interactions

References
Barrett, J. N., Magleby, K. L., and Pallotta, B. S., 1982, Properties of single
calcium-activated potassium channels in cultured rat muscle, J. Physiol.,
331:211–230.
Borg-Graham, L., 1991, Modelling the non-linear conductances of excitable membranes, in Cellular Neurobiology: A Practical Approach (J.
Chad and H. Wheal, Eds.), New York: IRL/Oxford University Press,
chap. 13.

170

Part III: Articles

Borg-Graham, L., 1999, Interpretations of data and mechanisms for hippocampal pyramidal cell models, Cereb. Cortex, 13:19–138. ⽧
Graham, L., 2002, The Surf-Hippo Neuron Simulation System, available:
http: //www.cnrs-gif.fr/iaf/iaf9/surf-hippo.html, v3.0.
Hille, B., 2002, Ionic Channels of Excitable Membranes, 3rd ed., Sunderland, MA: Sinauer. ⽧
Hines, M., 1984, Efficient computation of branched nerve equations, Int.
J. Biomed. Comput., 15:69–76.
Hodgkin, A. L., and Huxley, A. F., 1952, A quantitative description of
membrane current and its application to conduction and excitation in
nerve, J. Physiol., 117:500–544.
Jack, J. J. B., Noble, D., and Tsien, R. W., 1983, Electric Current Flow in
Excitable Cells, Oxford, Engl.: Clarendon Press. ⽧

Koch, C., 1999, The Biophysics of Computation: Information Processing
in Single Neurons, Oxford, Engl.: Oxford University Press. ⽧
Moczydlowski, E., and Latorre, R., 1983, Gating kinetics of Ca2Ⳮ-activated
KⳭ channels from rat muscle incorporated into planar lipid bilayers, J.
Gen. Physiol., 82:511–542.
Patlak, J., 1991, Molecular kinetics of voltage-dependent NaⳭ channels,
Physiol. Rev., 71:1047–1080.
Schneidman, E., Freedman, B., and Segev, I., 1998, Ion-channel stochasticity may be critical in determining the reliability and precision of spike
timing, Neural Computat., 10:1679–1703.
Zador, A., Koch, C., and Brown, T. H., 1990, Biophysical model of a
Hebbian synapse, Proc. Natl. Acad. Sci. USA, 87:6718–6722.

Biophysical Mosaic of the Neuron
Lyle J. Graham and Raymond T. Kado
Introduction
In this article we broadly review the biophysical mechanisms of
neurons that are likely to be relevant to computational function
(Table 1). These mechanisms operate within the complex threedimensional anatomy of the single neuron and are manifested by
electrical and chemical interactions between ions on either side of
the cell membrane and the diverse proteins and other molecules

Table 1. Neuronal Biophysical Mechanisms Relevant to Computational
Function
Ion channels
Control by intrinsic signals (membrane voltage, intracellular molecules)
Control by extrinsic signals (extracellular molecules, e.g., released from
presynaptic terminals)
Receptors
Ionotropic: Direct control of ion channels
Metabotropic: Indirect control of ion channels and other internal systems
External binding sites (synaptic and pancrinic)
Internal binding sites associated with neuronal and organelle membranes
Control of internal biochemical systems
Enzymes (kinases and phosphatases, which determine the state of most
proteins; others)
Gap junctions
Pumps, transporters, exchangers (electrogenic and nonelectrogenic)
Organelles
Ca2Ⳮ sequestering and release (endoplasmic recticulum, mitochondria)
Protein synthesis and metabolism
Maintenance and modulation of three-dimensional structure (spines,
dendritic morphology)
Transmitter sequestering and release (synaptic vesicles)
Cytoplasmic biochemical systems
Transmitter synthesis and degradation
G proteins: Initiate second messenger release after receptor activation
Second messengers: Diffusible molecules linking various stages of
internal biochemical systems
Effectors: Targets of second messengers, including channels and
enzymes (kinases, phosphatases)
Three-dimensional structure
Macroscopic anatomy (soma, dendritic and axonal trees)
Microscopic anatomy (spines, synaptic junctions, variations in dendritic
or axonal dimensions)
“Electrotonic” anatomy (modulated by state of local membrane)
Geometrical synaptic and channel distribution
Functional synaptic localization (e.g., retinotopic, tonotropic)
Computational synaptic localization (e.g., on-the-path interactions,
coincidence detection)

embedded in the membrane and within the cytoplasm (Figure 1).
The signals mediating these interactions may be defined by the
voltage across the cell membrane or by concentrations of specific
molecules in specific conformational or metabolic states. The first
case relies on the voltage sensitivity of various membrane proteins;
the second relies on a vast multitude of receptor proteins that link
the functional state of neuronal proteins with the external or internal
concentrations of ions and molecules.
It may be noted that none of these cellular mechanisms is unique
to neurons. For example, essentially all the mechanisms discussed
in this article may be relevant when considering a possible computational role of the neuroglia network (Laming et al., 2000). By
the same token, neurons (and glial cells) include the essential mosaic of biochemical systems found in all cells required for metabolism, reproduction, growth, and repair. The complexity is daunting. Here we focus on the better-known elements most clearly
linked to the reception, processing, and transmission of neuronally
represented information. It may seem that there are so many such
elements, and an even larger number of unknown relationships, that
it would not be possible for a theory to take all of the actual dynamic behaviors into consideration. Nevertheless, it also seems
likely that an oversimplification of these interactions—for example,
in the extreme case by describing single neuron function as an
abstracted trigger device—may put fundamental limits to the explanatory and predictive power of any neural model. The challenge
remains, then, to develop a description of single neuron function
that can serve as the foundation for a practical yet sufficient neural
theory.
We start with a metaphor, the mosaic neuron. A mosaic is a
collection of discrete parts, each with unique properties, fitted together in such a way that an image emerges from the whole in a
nonobvious way. Similarly, the neuronal membrane is packed with
a diversity of receptors and ion channels and other proteins with a
recognizable distribution. In addition, the cytoplasm is not just water with ions, but a mosaic of interacting molecular systems that
can directly affect the functional properties of membrane proteins.
Whether for the developing or for the mature neuron, this mosaic
is not stationary. To begin with, neuronal proteins are constantly
recycled, as is the case for all cells. Furthermore, on both long and
short time scales, most mechanistic theories for learning and memory implicate physical changes in various cellular constituents. On
time scales of seconds or less, different signaling systems impinging on the neuron from the network or present in the cytoplasm
can modify the properties of the mosaic elements, and in some
cases their distribution within the cell (see ACTIVITY-DEPENDENT
REGULATION OF NEURONAL CONDUCTANCES). Thus, just as a mo-

Biophysical Mosaic of the Neuron

171

Figure 1. Sketch of the molecular circuit underlying the neuron’s biophysical mosaic. This caricature outlines the many interrelated control paths
among the molecular elements that determine the functional properties of
the neuron. For example, activation of an ionotropic synapse starts with the
binding of extracellular transmitter to the membrane receptor, which then
directly turns on an associated ion channel. Alternatively, activation of a
metabotropic synapse is initiated by transmitter binding to a receptor, which

then activates a G protein, which turns on an enzyme, which raises the
concentration of a second messenger, which, finally, activates a target ion
channel. Many control pathways are immediately bidirectional: current
through an ion channel changes the membrane voltage, which in turn can
control the gating of that same channel. Some elements can even control
themselves, for example autophosphorylating enzymes.

saic painting provokes perception of a complete image out of a
maze of individually diversified tiles, current thinking holds that a
given neuron performs a well-defined computational role that depends not only on the network of cells in which it is embedded but
also to a large extent on the dynamic distribution of macromolecules throughout the cell.

physiological measurements of single cells, when a change in intrinsic response properties or synaptic dynamics is seen after a
given chemical is added to the fluid bathing the nervous tissue.

The Minimal Essential Model
and the Biophysical Mosaic
It remains an open question as to what constitutes a minimal neuron
model for reproducing functional neuronal computation (Meunier
and Segev, 2000; see also CANONICAL NEURAL MODELS). This is
in part because to date, only a handful of neuron circuit models
come close to predicting known experimental data in any nontrivial
way. The question of finding a minimal model is hardly an academic one, as can be appreciated by reviewing the dimensionality
of the mosaic neuron (Table 2).
Whatever the minimal essential model turns out to be, a detailed
knowledge of neuronal biophysics is most likely necessary for understanding the system behavior (even if this understanding is not
sufficient). The clearest evidence for this point of view comes from
psychopharmacology: although we lack a clear understanding of
the mechanisms, we know that adding certain chemicals to the
brain parenchyma can qualitatively alter cognitive behavior. We
know that the direct action of psychotropic drugs is probably to
change one or more biophysical properties at the microscopic cellular level, such as blocking an ion channel, altering the binding
kinetics of a receptor, modulating a biochemical pathway, and so
on, rather than acting at a more macroscropic systems level, such
as cleanly disconnecting a circumscribed subcircuit from the entire
network. We know that physical access to the brain is necessary
for this action (preventing a drug from crossing the blood-brain
barrier eliminates its effect), and we also know, in many cases, that
some neurons have highly specific membrane receptors for a given
psychotropic molecule that are often localized in very restricted
areas at the level of brain substructures and even at the single-cell
level. Often there is direct evidence of a drug’s effect in electro-

The Mosaic’s Tiles
We will now review the major proteins that compose the neuron
mosaic and discuss some basic implications of their diversity and
complexity. These macromolecules include ion channels, receptors
(described along with the molecules that activate them), enzymes,
gap junctions, pumps, exchangers, and transporters. Note that these
classifications can sometimes overlap. For example, an ionotopic
receptor is a protein multimer that includes both a receptor part and
a channel part.
Several texts may be consulted for more detail on these mechanisms (Johnston and Wu, 1995; Weiss, 1996; Koch and Segev,
1998; Fain, 1999; Hille, 2002). In particular, the textbook by Koch
(1999) provides an explicit foundation for the computation/
algorithm/implementation trinity that is fundamental for understanding brain function.

Ion Channels
Ion channels are membrane-spanning proteins that, owing to their
conformational states which allow the passage of ionic current, are
the primary basis for the dynamical electrical behavior of neurons
(see ION CHANNELS: KEYS TO NEURONAL SPECIALIZATION). The
permeability of the conducting states and the kinetics governing
state transitions (generally referred to as channel gating) can be
affected by a variety of factors, principally the membrane voltage
and the intra- and extracellular concentrations of the permeable ions
and other specific molecules. Sensitivity to extracellular molecules
is generally mediated by either direct action on the channel or various receptor proteins (e.g., in response to neurotransmitters), as
discussed below. Molecules that affect channel gating from the
inside include second messengers. The kinetic relationship between
membrane voltage, the concentration of neurotransmitters, second
messengers, and a channel’s conductance state can be quite complex, a point we return to later.

172

Part III: Articles

Table 2. Quantitative Summary of the Neuron’s Biophysical Mosaic
Relevant to Computational Function
Spatial scales (voltage and concentration transients): 1 to thousands of
microns
Temporal scales
Kinetics of gating, binding, and diffusion: 1 ms to seconds
Gene expression: days to years?
Anatomy
Tens to hundreds of dendritic and axonal branches
Models of electrotonic structure can require thousands of compartments
Synapses and channels
Thousands of pre- and postsynaptic sites
Five major categories of charge carriers: NaⳭ, KⳭ, Ca2Ⳮ, Clⳮ, other
(e.g., “cationic,” proton, etc.)
Tens of types for each category, several of which may be expressed in
a single neuron
Receptors and associated agonists (neurotransmitters, second messengers,
etc.)
Approximately 30 major types, several of which may be expressed in a
single neuron
Possibly tens of identified subtypes for some major receptor types
A brute force map of a single neuron would be very large indeed. For
example, compartmental models of the dendritic tree can require
hundreds or thousands of coordinates, corresponding to spatial scales
relevant for representing gradients of membrane voltage or of the
concentration of intracellular molecules. A given neuron may have
thousands of synaptic inputs, each associated with one of several types of
receptors, and the cell membrane can include tens of types of ion
channels. Furthermore, the different synaptic receptor and channel types
are typically scattered inhomogeneously over the neuron. It is also
important to consider the stationarity of the map. For computations over
hundreds of milliseconds or less, the map may properly be thought to be
static, but at longer time scales it may be necessary to consider a
dynamic layout of the mosaic. For another carte du monde of
computational cellular mechanisms and their spatial and temporal scales,
see Figure 21.2 in Koch, 1999.

Since channels are the most direct mechanism determining the
basic firing properties of the cell (e.g., regular adapting, bursting,
fast spiking), and since channels are subject to functional modulation on a variety of time scales, it is not surprising that a given
neuron can exhibit more than one “stereotypical” firing behavior,
depending on the conditions (see NEOCORTEX: BASIC NEURON
TYPES).

Receptors and Their Agonists and Antagonists:
Neurotransmitters, Neuromodulators, Neurohormones,
and Second Messengers
Receptors are membrane proteins whose functional action is triggered by the reversible binding of specific molecules called ligands
(Cooper, Bloom, and Roth, 1996; see NMDA RECEPTORS: SYNAPTIC, CELLULAR, AND NETWORK MODELS). A given molecule
may be a ligand for more than one kind of receptor, with very
different or even opposite functional effects; likewise, a given receptor may be able to be activated by more than one endogenous
(or artificial, that is, experimental or pharmaceutical) ligand. A ligand that tends to upregulate the functional activity of a receptor
protein is called an agonist for that receptor. Conversely, antagonists are molecules that inhibit the activity of a receptor.
There are two basic types of receptors, ionotropic and metabotropic. Ionotropic receptors are directly associated with an ion
channel whose gating is controlled by the presence of the receptor
agonist. The action of metabotropic receptors is more complex:
upon binding to an agonist, these receptors activate a G protein (so
named because their action involves the conversion between guan-

osine diphosphate and guanosine triphosphate), which may directly
control channel gating or may initiate a biochemical cascade mediated by second messengers. The end point of this “chain reaction”
can be, for example, the opening of a channel, or the phosphorylation of a receptor by the activation of a kinase.
Agonists are properly called neurotransmitters when released by
the presynaptic terminal of an axon (or possibly a dendrite) arising
from another neuron (see NEOCORTEX: CHEMICAL AND ELECTRICAL SYNAPSES). Extracellular agonists also include neuromodulators and neurohormones, with the latter distributed through the vasculature as well as the perineuronal space (see NEUROMODULATION
IN MAMMALIAN NERVOUS SYSTEMS and NEUROMODULATION IN
INVERTEBRATE NERVOUS SYSTEMS). From a functional viewpoint,
the main difference between these agonists and neurotransmitters
is that neurotransmitters generally mediate synaptic communication between two specific pre- and postsynaptic cells, whereas the
release of a neuromodulator or neurohormone into the extracellular
space mediates pancrinic transmission, affecting a local region of
tissue rather than a single postsynaptic site. Another, somewhat
arbitrary, difference is that neuromodulators and neurohormones
tend not to overtly excite or inhibit their targets, but rather shape
the response of a neuron to classical synaptic transmitters in various
and subtle ways (Kaczmarek and Levitan, 1987). Note that a given
molecule can be assigned more than one of these roles (e.g., neurotransmitter versus neuromodulator), depending on the cell type
or region in the nervous system.
Intracellular second messengers are called such because their
concentration is often subsequent to the message delivered by neurotransmitters (e.g., after activation of a metabotopic receptor).
Second messengers may have direct actions or, as mentioned, may
participate in more complicated reaction schemes. Depending on
the complexity of the reaction, the functional action of second messengers can be quite delayed and last for minutes if not longer. In
addition, the more complicated the biochemical cascade, the more
opportunities there are for interactions with modulatory pathways.
The most well-known second messenger is the Ca2Ⳮ ion, which
modulates various membrane channels and biochemical cascades,
including many neurotransmitter release systems, and whose intracellular concentration is mediated by a variety of Ca2Ⳮ-permeable
channels, pumps, buffers, and intracellular stores (involving as well
the extensive endoplasmic recticulum network, which may support
regenerative intracellular Ca2Ⳮ waves [Berridge, 1998]).
There is a vast array of receptor types, some of which are associated with classical point-to-point synaptic transmission, others
that mediate pancrinic transmission, and still others that function
as links along intracellar pathways. Presynaptic membrane may
also express extracellular receptors whose agonist is either the
transmitter released by the same terminal (and thus implementing
an immediate feedback loop) or another substance, which then may
modulate the presynaptic terminal properties. A given neuron may
express many different types of receptors in response to the signaling molecules released from other cells, normally in a nonuniform distribution over its surface. In contrast, the number of neuroactive compounds that a single neuron releases itself is usually
one, probably (according to current knowledge) at most two or
three.

Enzymes
Among the wide variety of enzymes distributed in the neuron’s
cytoplasm, the most important types for signal processing include
kinases and phosphatases, as well as those involved in the metabolism of signaling molecules (e.g., synthases and lipases). The kinases and phosphatases respectively phosphorylate (add a phosphate group) and dephosphorylate specific target proteins, as a
result modifying the functional properties of the target. This is the

Biophysical Mosaic of the Neuron
most common mechanism of regulating the activity of neuronal
proteins, for example, by altering the responsiveness of a receptor
to an agonist, or the voltage dependency or conductance of an ion
channel.

Gap Junctions
Gap junctions are membrane proteins that form a direct electrical
path between two neurons, essentially as a nonlinear, nonselective
ion channel (see NEOCORTEX: CHEMICAL AND ELECTRICAL SYNAPSES). Thus, on the one hand, these connections are like conventional synapses in that they mediate information flow from cell to
cell, but on the other hand, they are quite unlike conventional synapses in that this flow is (more or less) reciprocal and instantaneous.
As with essentially all the other neuronal elements, gap junctions
can be functionally modulated, typically by Ca2Ⳮ or other second
messengers.

Pumps, Exchangers, and Transporters
Pumps, exchangers, and transporters are membrane proteins responsible for the active maintenance of concentration gradients of
different ions and molecules crucial for neural signal processing,
and thus are able to modify the membrane potential, either directly
or indirectly.
For example, the enzyme Na/K ATPase maintains the characteristic NaⳭ and KⳭ gradients across all cell membranes; related
proteins include the calcium and proton pumps. The action of these
pumps depends on the hydrolysis of adenosine triphosphate (ATP)
to adenosine diphosphate (ADP), and thus they are tightly coupled
to the metabolic machinery of the neuron. Since these cations directly or indirectly contribute to the membrane potential, and since
the kinetics of the pump can be modulated, a pump can set the
neuron’s long-term electrical behavior.
In addition to driving channel currents, the NaⳭ and KⳭ gradients across the cell membrane also provide the energy for exchangers and transporters. Exchanger proteins move ions such as Ca2Ⳮ
and protons out of the neuron, against their gradients, in exchange
for NaⳭ moving down its gradient. The exchangers react faster than
pumps and thus provide early protection against excessive accumulation of various ions. Transporter proteins move molecules
such as glutamate and GABA (respectively the principal excitatory
and inhibitory neurotransmitters in the central nervous system)
back into the neuron (and into surrounding glia as well) after being
released into the extracellular space during synaptic transmission.
The activity of some of these proteins is electrogenic. For example, the Na/K pump cycles two KⳭ ions in for three NaⳭ ions
out, and therefore directly generates a net outward current that can
cause a hyperpolarization of many millivolts, depending on conditions. Although not always inherently electrogenic, there is an
indirect link between the activity of exchangers and transporters
and the membrane potential. Since they are driven by the inward
movement of NaⳭ, an increase in exchanger or transporter activity
leads to an increase in the cytoplasmic concentration of NaⳭ, which
will then be countered by increased Na/K ATPase activity and its
attendent electrogenic effect.

Implications of Neuronal Macromolecule Diversity
and Complexity
Channels, receptors, pumps, enzymes, and so on are comprised of
one or several individual proteins, called subunits, each of which
is coded by a specific gene. For any given type of channel (etc.)
there may be many variations of the complete ensemble, or multimer, as one subunit substitutes for another, which often imparts
different peculiarities to the functional properties of the multimeric

173

protein (binding sites, effect on kinetics, etc.—in fact, the same
sort of properties that may be affected by protein phosphorylation).
Thus, a particular Ca2Ⳮ channel type, for example, may have ten
or so identified variants or subtypes (with the strong likelihood that
more remain to be discovered). There are as yet but few demonstrations, either by explicit functional studies or by model prediction, that these differences between subtypes are relevant for neural
computation. Nevertheless, correlations are increasingly being
found between particular disease states and subtle functional alterations of cellular elements, or, in the opposite sense, functional
(e.g., behavioral) expressions of genetic manipulation (e.g., knockout) protocols. Thus, the reality of subtype diversity suggests an
important limitation for models that employ a single stereotypical
kinetic model of a given type of neural protein.
Subunit substitution in a receptor, channel, or other neural protein can, among other things, determine different endogenous modulatory agonists or antagonists. Since there are many candidates
for pancrinic pathways at most neurons, this mechanism is important for understanding circuitry dynamics in the intact brain. This
functional diversity also has extremely important implications for
clinical pharmacology: different subunits can also impart sensitivities to different exogenous compounds, allowing the eventual possibility of targeting very specific synapses or other cellular elements with the appropriately chosen (or designed) drug.
Individual proteins are comprised of contorted chains of thousands of amino acids. This fundamental complexity allows for, in
principle, several mechanisms by which a protein may be influenced by its local environment. Thus, there may be an important
location dependence of the functional properties of a particular kind
of protein, reflecting subtle variations in the protein’s microenvironment. For the same reason, it is not surprising that the behavior
of a channel, for example, may be modified by the membrane voltage or by binding with a signaling molecule. In this context, we
may note that quantitative experimental measurements of a given
channel or receptor type in different cell types are inevitably different, beyond what would be expected from experimental variability. Sometimes such differences are seen even between different
locations of a single cell type (in particular somatic versus dendritic). Thus, there are at least two possible explanations for such
differences: they may be intrinsic to the neural protein under investigation (i.e., a difference in subunit composition), or they may
reflect how different local environments, specific to different cell
types or location within a single cell, can influence the protein’s
behavior.

Neuron Models and the Biophysical Mosaic
We now return to the question of neuron models and how they
might relate to cellular details. In the most general sense, a single
neuron provides a dynamic mapping from a spatiotemporal pattern
of pulsed inputs impinging on its dendrites and soma, into a single
sequence of output spikes at the axon hillock, which may then be
further altered by distinct mechanisms in the axonal tree and presynaptic boutons. Overall, the neuron models employed by theorists describe the time-varying three-dimensional biophysical mosaic underlying this complex signal processing to varying degrees
(see PERSPECTIVE ON NEURON MODEL COMPLEXITY and SINGLECELL MODELS).
At the simplest level, an extreme abstract model might be a point
integrator whose output is passed through a static sigmoid transfer
function, where the scalar output is analogous to the firing rate of
a spiking neuron. Here the biophysical basis is essentially limited
to the resistive nature of the neuron membrane and the spike threshold. As a next step, the basic temporal characteristics of neuronal
function may be represented by a leaky integrate-and-fire model
that captures the resistive-capacitive nature of the neuron mem-

174

Part III: Articles

brane and the action potential–based point process communication
between neurons (see INTEGRATE-AND-FIRE NEURONS AND NETWORKS). Among other things, this scheme allows for encoding by
both firing rate and higher-order statistics of spike trains, as well
as a more tractable analysis of generalized stochastic mechanisms
(see ADAPTIVE SPIKE CODING, RATE CODING AND SIGNAL PROCESSING, and SENSORY CODING AND INFORMATION TRANSMISSION).
A more explicit description of biophysical mechanisms might
start with the characteristics of membrane channels and dendritic
cables (see DENDRITIC PROCESSING). For example, a single neuron
model may include transmitter-gated synaptic conductance inputs
distributed on a linear (or “passive”) cable tree topology, with
conductance-based (i.e., voltage-dependent channels) spike generation at a central somatic node. An anatomically based dendritic
cable structure provides an explicit basis for synaptic weights via
different coupling impedances to the soma, as well as cabledependent (e.g., “on-the-path”) nonlinear synaptic interactions.
Simple channel models can capture basic spike firing properties
such as absolute and relative refractory period, adaptation, or nonzero minimum firing rates.
A model with increased biophysical realism could include
voltage-dependent membrane properties distributed throughout the
cell (Stuart, Spruston, and Häusser, 1999; see DENDRITIC PROCESSING). Intrinsic and synaptic mechanisms can be modeled with
less or more sophisticated kinetic descriptions, either deterministic
or stochastic (see BIOPHYSICAL MECHANISMS IN NEURONAL MODELING; TEMPORAL DYNAMICS OF BIOLOGICAL SYNAPSES, SYNAPTIC INTERACTIONS; and SYNAPTIC NOISE AND CHAOS IN VERTEBRATE NEURONS). Further details of functional properties may
require descriptions of the microphysiology of extra- and intracellular systems, and thus explicit modeling of biochemical dynamics,
including Ca2Ⳮ diffusion, buffering, sequestration, and release;
protein conformations; and enzyme activation/inactivation. Finally,
the most faithful cellular model would require a four-dimensional
construct whose biophysical properties vary with both space and
time, in particular depending on past activity, or “experience” (see
HEBBIAN SYNAPTIC PLASTICITY).

State Variables and Functional Compartments
of the Mosaic Neuron
The many cellular elements we have described suggest a similar
number of variables that characterize the functional state of a neuron as a signal processing device, each of which may be thought
of as representing information. The most classical variable, of
course, is the membrane voltage, which defines the immediate integration of synaptic input onto the dendritic tree and soma and,
eventually, the action potential output of the cell. However, it may
be argued that for predicting spike output, the first derivative of
the membrane voltage may be nearly as important as the actual
value of the voltage, a behavior that is easily predicted by HodgkinHuxley-type models (see AXONAL MODELING; BIOPHYSICAL
MECHANISMS IN NEURONAL MODELING; and ION CHANNELS:
KEYS TO NEURONAL SPECIALIZATION). Other variables that may
be important include the concentration of ions and various neuroactive molecules (e.g., transmitters and second messengers) both
inside and outside the cell, and the metabolic or conformational
state of various membrane and intracellular proteins. Finally, it may
be useful to consider structural or anatomical parameters of the
single neuron as functional state variables, such as number and
distribution of spines or postsynaptic sites.
All of these state variables are determined by complex relationships between the cellular constituents. For example, the membrane
voltage at any given point in the neuron is determined by the spatial
distribution of electrically conducting membrane channels and their

reversal potentials, the membrane capacitance, and the electrical
coupling to the rest of the cell as determined by the threedimensional branching cable structure and cytoplasmic resistivity.
In turn, the ion concentration gradients that underlie channel reversal potentials are determined by an interplay between the currents through the appropriate channels (which tend to reduce the
gradients) and membrane pumps, exchangers, transporters, and intracellular buffer and sequestering systems (which in general tend
to maintain the gradients). Finally, in some cases ions passing
through a given membrane channel can subsequently bind with and
then modulate the conduction state of either the same or possibly
other types of channels. Clearly, feedback pathways are the rule
rather than the exception in the mosaic’s interactions (see Figure
1).
The state variables of a neuron can be associated with a variety
of functional compartments, with spatial scales that range from less
than a micron to the entire cell. These compartments may correspond to the spatial gradients of voltage (e.g., as determined by
dendritic cable properties) or second messenger concentration (e.g.,
determined by diffusion constant and geometry of the intracellular
space), or to the localization of a given biochemical pathway, or to
an explicit subcellular structure (e.g., a dendritic spine, an organelle, the cell nucleus). In summary, for most state variables the
neuron is far from a classical “well-mixed” system. Rather, an extreme internal heterogeneity is usually the case: a single cell thus
becomes a cell of cells.

Emergent State Properties of Intracellular
Chemical Systems
Recent work has demonstrated the possibility of various stable arrangements between the concentrations of certain intracellular molecules or the metabolic states of certain proteins, all of which in
turn can participate in various biochemical pathways, including
those regulating membrane properties and plasticity (Weng, Bhalla,
and Iyengar, 1999). From an information processing viewpoint,
these combinations can be thought of as essentially distinct states
that partially define the functional input-output properties of the
neuron. It has also been proposed that changes in cellular properties
on even longer time scales may be due to self-stabilizing conformational states of proteins such as CAM kinase II or others. Of
course, any mechanism underlying a long-lasting modification of
the neuronal transfer function is a candidate for the molecular basis
of memory (see INVERTEBRATE MODELS OF LEARNING: APLYSIA
AND HERMISSENDA).

Discussion: How Much Biophysics Needs to Be Known
for a Compelling Brain Theory?
Where does the complexity of the mosaic neuron leave us in terms
of formulating a theory for the brain? In particular, how detailed
does a model of the neuron have to be, and how does the power of
current methods compare with the computational complexity inherent at various levels of biophysical description? These open
questions have a very practical importance, since there are few
opportunities for formal analyses of these nonlinear dynamical systems. Furthermore, the increasing experimental knowledge of neuronal cellular mechanisms is daunting. The details seem to have an
almost fractal quality; no matter what level is being examined,
underneath any given mechanism there is another Pandora’s box
of parameters waiting to be described. Today, so many biophysical
properties are known to be present in the neuron membrane that
the biggest risk is to choose to include only those that will give the
model the properties desired. This leads to a model that is unlikely
to fail, and therefore unlikely to teach us anything that we did not
know before.
The brute force strategy is to construct a bottom-up, biophysically detailed cell model in order to cover as much as possible the

Brain Signal Analysis
high-order interactions between various mechanisms. Once the
map has been laid out, sufficiently rich deterministic or stochastic
kinetic equations for the membrane elements and intracellular dynamics may then be assigned. Such maps can be evaluated, at least
in principle, since all known biophysical mechanisms can be described by nonlinear partial differential equations, and thus are
amenable to standard numerical integration techniques.
However, it may appear that at some point there will be too many
equations with too many free parameters for practical evaluation
or, more important, for true understanding. (On the other hand, the
rapid evolution of computing power suggests that the threshold for
“impractical” is hard to define.) An optimistic view is that once a
sufficiently elaborate biophysical cellular model is constructed, its
behavior may be well enough understood so that a more abstract
model that captures the functional essentials with considerably less
computational expense may be derived. But until there are some
formal criteria for establishing what exactly “essential” means, this
process will inevitably be an iterative one, moving back and forth
between an analysis of detailed cell models in isolation and in
nontrivial networks (limited most probably by the available tools)
and an analysis of more abstract neural networks. The fundamental
challenge to theorists, then, is to go beyond this sort of “reverse
engineering” approach and develop a method that is at once commensurate with the complexity of the brain, yet can produce a bona
fide theory whose detail avoids that of the actual biological reality.

Road Map: Biological Neurons and Synapses
Related Reading: Activity-Dependent Regulation of Neuronal Conductances; Adaptive Spike Coding; Axonal Modeling; Dendritic Processing;
Hebbian Synaptic Plasticity; Ion Channels: Keys to Neuronal Specialization; Neocortex: Chemical and Electrical Synapses; NMDA Receptors: Synaptic, Cellular, and Network Models; Rate Coding and Signal

175

Processing; Synaptic Interactions; Synaptic Noise and Chaos in Vertebrate Neurons; Temporal Dynamics of Biological Synapses

References
Berridge, M. J., 1998, Neuronal calcium signaling, Neuron, 21:13–26.
Borg-Graham, L., 1999, Interpretations of data and mechanisms for hippocampal pyramidal cell models, Cereb. Cortex, 13:19–138. ⽧
Cooper, J. R., Bloom, F. E., and Roth, R. H., 1996, The Biochemical Basis
of Neuropharmacology, 7th ed., Oxford, Engl.: Oxford University
Press. ⽧
Fain, G. L., 1999, Molecular and Cellular Physiology of Neurons, Cambridge, MA: Harvard University Press. ⽧
Hille, B., 2002, Ionic Channels of Excitable Membranes, 3rd ed., Sunderland, MA: Sinauer. ⽧
Johnston, D., and Wu, S. M.-S., 1995, Foundations of Cellular Neurophysiology, Cambridge, MA: MIT Press. ⽧
Kaczmarek, L. K., and Levitan, I. B., Eds., 1987, Neuromodulation: The
Biochemical Control of Neuronal Excitability, Oxford, Engl.: Oxford
University Press.
Koch, C., 1999, The Biophysics of Computation: Information Processing
in Single Neurons, Oxford, Engl.: Oxford University Press. ⽧
Koch, C., and Segev, I., Ed., 1998, Methods in Neuronal Modeling, 2nd
ed., Cambridge, MA: MIT Press. ⽧
Laming, P. R., Kimelberg, H., Robinson, S., Salm, A., Hawrylak, N.,
Müller, C., Roots, B., and Ng, K., 2000, Neuronal-glial interactions and
behaviour, Neurosci. Biobehav. Rev., 24:295–340.
Meunier, C., and Segev, I., 2000, Neurons as physical objects: Structure,
dynamics and function, in Handbook of Biological Physics (F. Moss and
Gielen S., Eds.), New York: Elsevier.
Stuart, G., Spruston, N., and Häusser, M., Eds., 1999, Dendrites, Oxford,
Engl.: Oxford University Press. ⽧
Weiss, T. F., 1996, Cellular Biophysics (2 vol.), Cambridge, MA: MIT
Press. ⽧
Weng, G., Bhalla, U. S., and Iyengar, R., 1999, Complexity in biological
signaling systems, Science, 284:92–96.

Brain Signal Analysis
Jeng-Ren Duann, Tzyy-Ping Jung, and Scott Makeig
Introduction
Artificial neural networks (ANNs) have now been applied to a wide
variety of real-world problems in many fields of application. The
attractive and flexible characteristics of ANNs, such as their parallel operation, learning by example, associative memory, multifactorial optimization, and extensibility, make them well suited to
the analysis of biological and medical signals. In this study, we
review applications of ANNs to brain signal analysis, for instance,
for analysis of the electroencephalogram (EEG) and magnetoencephalogram (MEG) or electromyogram (EMG), and as applied to
computed tomographic (CT) images and magnetic resonance (MR)
brain images, and to series of functional MR brain images (i.e.,
fMRI).
Most ANNs are implemented as sets of nonlinear summing elements interconnected by weighted links, forming a highly simplified model of brain connectivity. The basic operation of such artificial neurons is to pass a weighted sum of their inputs through a
nonlinear hard-limiting or soft “squashing” function. To form an
ANN, these basic calculating elements (artificial neurons) are most
often arranged in interconnected layers. Some neurons, usually
those in the layer farthest from the input, are designated as output
neurons. The initial weight values of the interconnections are usually assigned randomly.

The operation of most ANNs proceeds in two stages. Rules used
in the first stage, training (or learning), can be categorized as supervised, unsupervised, or reinforced. During training, the weight
values for each interconnection in the network are adjusted either
to minimize the error between desired and computed outputs (supervised learning) or to maximize differences (or minimize similarities) between the output categories (unsupervised or competitive
learning). In reinforced learning, an input-output mapping is
learned during continued interaction with the environment so as to
maximize a scalar index of performance (Haykin, 1999). The second stage is recall, in which the ANN generates output for the
problem the ANN is designed to solve, based on new input data
without (or sometimes with) further training signals.
Because of their multifactorial character, ANNs have proved
suitable for practical use in many medical applications. Because
most medical signals of interest usually are not produced by variations in a single variable or factor, many medical problems, particularly those involving decision making, must involve a multifactorial decision process. In these cases, changing one variable at
a time to find the best solution may never reach the desired objective (Dayhoff and DeLeo, 2001), whereas multifactorial ANN approaches may be more successful. In this article, we review recent
applications of ANNs to brain signal processing, organized ac-

176

Part III: Articles

cording to the nature of brain signals to be analyzed and the role
that ANNs play in the applications.

Roles of ANNs in Brain Signal Processing
To date, ANNs have been applied to brain data for the following
purposes:
• Feature extraction, classification, and pattern recognition.
ANNs in this application serve mainly as nonlinear classifiers.
The inputs are preprocessed so as to form a feature space. ANNs
are used to categorize the collected data into distinct classes. In
other cases, inputs are not subjected to preprocessing but are
given directly to an ANN to extract features of interest from the
data.
• Adaptive filtering and control. In this application, ANNs operate
within closed-loop systems to process changing inputs, adapting
their weights “on the fly” to filter out unwanted parts of the input
(adaptive filtering), or mapping their outputs to parameters used
in on-line control (adaptive control).
• Linear or nonlinear mapping. In this application, ANNs are used
to transform inputs to outputs of a desired form. For example,
an ANN might remap its rectangular input data coordinates to
circular or more general coordinate systems.
• Modeling. ANNs can be thought of as function generators that
generate an output data series based on a learned function or data
model. ANNs with two layers of trainable weights have proved
capable of approximating any nonlinear function.
• Signal separation and deconvolution. In this application, ANNs
separate their input signals into the weighted sum or convolution
of a number of underlying sources using assumptions about the
nature of the sources or of their interrelationships (e.g., their
independence).
• Texture analysis and image segmentation. Image texture analysis
is becoming increasingly important in image segmentation, recognition, and understanding. ANNs are being used to learn spatial or spatial-frequency texture features and, accordingly, to
categorize images or to separate an image into subimages (image
segmentation).
• Edge detection. In an image, an edge or boundary between two
objects can be mapped to a dark band between two lighter areas
(objects). By using the properties of intensity discontinuity,
ANNs can be trained to recognize these dark bands as edges, or
can learn to draw such edges based on contrast and other
information.

Application Areas
In this section, we describe some applications of ANNs to brain
signals by means of examples involving neurobiological time series
and brain images. Neurobiological signals of clinical interest recorded noninvasively from humans include electroencephalographic (EEG), magnetoencephalographic (MEG), and electromyographic (EMG) data. Research in brain imaging includes the
analysis of structural brain images, mainly focused on the extraction of three-dimensional (3D) structural information, from various
kinds of brain images (e.g., magnetic resonance images, or MRIs),
as well as the analysis of functional brain imaging series that
mainly reveal changes in the brain state during cognitive tasks using medical imaging techniques (e.g., functional MRI, or fMRI,
and positron emission tomography, or PET). These examples by
no means cover all the applications in the field.

Neurobiological Signals
EEG and MEG. EEG provides a noninvasive measure of brain
electrical activity recorded as changes in the potential difference

between two points on the scalp. MEG is the magnetic counterpart
of EEG. In accordance with the assumption that the ongoing EEG
can be altered by stimulus or event to form respectively the eventrelated potential (ERP) or the evoked potential (EP), these changes,
though tiny, can be recorded through the scalp. It is possible for
researchers to apply pattern recognition algorithms to search for
the differences in brain status while the brain is performing different tasks. Thus, Peters, Pfurtscheller, and Flyvbjerg (2001) applied
an autoregressive model to four-channel EEG potentials to obtain
features that were used to train an ANN using a backpropagation
algorithm to differentiate the subject’s intention to move the left
or right index finger or right foot. They suggested that the framework might be useful for designing a direct brain-computer interface. In the study of Zhang et al. (2001), ANNs were trained to
determine the stage of anesthesia based on features extracted from
the middle-latency auditory-evoked potential (MLAEP) plus other
physiological parameters. By combining power spectral estimation,
principal component analysis (PCA), and ANNs, Jung et al. (1997)
demonstrated that continuous, accurate, noninvasive, and near realtime estimation of an operator’s global level of alertness is feasible
using EEG measures recorded from as few as two scalp sites. The
results of their ANN-based estimation compared favorably with
results obtained using a linear regression model applied to the same
PCA-reduced EEG power spectral data.
Sun and Sclabassi (2000) employed an ANN as a linear mapping
device to transform the EEG topography obtained from a forward
solution in a simple spherical model to a more realistic spheroidal
model whose forward solution was difficult to compute directly. In
that study, a backpropagation learning algorithm was used to train
an ANN to convert spatial locations between spherical and spheroid
models. Instead of computing the infinite sums of the Legendre
functions required in the asymmetric spheroidal model, the calculations were carried out in the spherical model and then converted
by the ANN to the more realistic model for display and evaluation.
Recently, ANNs have made a substantial contribution to the
analysis of EEG/MEG by separating the problem of EEG/MEG
source identification from that of source localization, a mathematically underdetermined problem: any scalp potential distribution
can be produced by a limitless number of potential distributions
within the head. Because of volume conduction through cerebrospinal fluid, skull, and scalp, EEG and MEG data collected from
any point on the scalp may include activity arising in multiple
locally synchronous but relatively independent neural processes
within a large brain volume. This has made it difficult to relate
EEG measurements to underlying brain processes and to localize
the sources of EEG and MEG signals. Progress has been made by
several groups in separating and identifying the distinct brain
sources from their mixtures in scalp EEG or MEG recordings, assuming only their temporal independence and spatial stationarity
(Makeig et al., 1997; Jung et al., 2001), using a class of independent
component analysis (ICA) or blind source separation algorithms.
Muscle and movement signals. From recordings of muscle
stretching (mainly the EMG), it is possible to predict the intent of
subjects to perform actions such as hand or finger movements, or
to judge the disability of a specific bundle of muscle cells. For
example, Khalil and Duchene (2000) used wavelet coefficients obtained from uterine EMG to train ANNs to separate the inputs into
four labeled categories: uterine contractions, fetal movements, Alvarez waves, and long-duration low-frequency band waves. They
reported that the system was useful for maintaining preterm births.
On the other hand, Stites and Abbas (2000) used an ANN as a
pattern shaper to refine the output patterns of a functional neuromuscular stimulation system that served as a pattern generator of
control signals for cyclic movements to help the paraplegic patient
stand using functional neuromuscular stimulation.

Brain Signal Analysis

Brain Images
Structural images. In structural brain image analysis, ANNs may
play roles in image segmentation, image labeling, or edge detection. Image segmentation is the first and probably the most important step in digital image processing. Segmentation may be a labeling problem in which the goal is to assign, to each voxel in a
gray-level image, a unique label that represents its belonging to an
anatomical structure. The results of image segmentation can be
used for image understanding and recognition, 3D reconstruction,
visualization, and measurements, including brain volume changes
in developmental brain diseases such as Alzheimer’s disease and
autism. The rapid pace of development of medical imaging devices
such as MRI and computed tomography permits a better understanding of anatomical brain structure without, prior to, or even
during neurosurgery. However, the results are highly dependent on
the quality of the image segmentation processes.
Here we give some examples using ANNs in image segmentation: Dawant et al. (1991) presented a backpropagation neural network approach to the automatic characterization of brain tissues
from multimodal MR images. The ability of a three-layer backpropagation neural network to perform segmentation based on a
set of MR images (T1-weighted, T2-weighted, and proton densityweighted) acquired from a patient was studied. The results were
compared with those obtained using a maximum likelihood classifier. Dawant et al. found no significant difference in the results
obtained by the two methods, although the backpropagation neural
network gave cleaner segmentation images. Using the same analysis strategy, Reddick et al. (1997) first trained a self-organizing
map (SOM) on multimodal MR brain images to efficiently extract
and convert the 3D inputs (from T1-, T2- and proton densityweighted images) into a feature space and utilized a backpropagation neural network to separate them into classes of white matter,
gray matter, and cerebrospinal fluid. Their work demonstrated high
intraclass correlation between the automated segmentation and
classification of tissues and standard radiologist identification, as
well as high intrasubject reproducibility.
Functional images. Today, not only are medical imaging devices
able to provide impressive spatial resolution and details of the fine
structure of the human brain, they can also reveal changes in brain
status in awake subjects who are performing a task or even daydreaming, by measuring ongoing metabolic changes, including cerebral blood flow, cerebral blood volume (by PET), and blood oxygenation level–dependent signal levels (by fMRI). We will give
some examples, mainly from fMRI analysis.
Functional brain imaging emerged in the early 1990s, based on
the observation that increases in local neuronal activity are followed by local changes in oxygen concentration. Changing the
amount of oxygen carried by hemoglobin changes the degree to
which hemoglobin disturbs a magnetic field, as a result of which
in vivo changes in blood oxygenation could be detected by MRI
(Ogawa et al., 1992). The subsequent changes in the MRI signal
became known as the blood oxygenation level–dependent or BOLD
signal. This technique was soon applied to normal humans during
functional brain activation (the subjects performed cognitive tasks),
giving birth to the rapid growing field of fMRI.
Theoretically, the fMRI BOLD signal from a given brain voxel
can be interpreted as a linear combination of different sources with
distinguishable time courses and spatial distributions, including
use-dependent hemodynamic changes, blood, or CSF flows, plus
subject movement and machine artifacts. Recently, ANNs (especially those using ICA), applied to fMRI data, have proved to be
a powerful method for detecting and separating task-related activations with either known or unanticipated time courses (McKeown et al., 1998) that could not be detected using standard

177

hypothesis-driven analyses. Duann et al. (2002) have given further
details of applying ICA to the fMRI BOLD signal. They showed
that the hemodynamic response to even widely spaced stimulus
presentations may be dependent on the trial, site, stimulus, and
subject. Thus, the standard regression–based method of applying a
fixed hemodynamic response model to find stimulus- or task-related BOLD activations needs to be reconsidered.

Discussion
The use of ANNs as classifiers currently dominates their applications in the field of brain signal analysis. This includes classification of brain or related signals as exhibiting normal or abnormal
features or processes. Not surprisingly, published studies report
promising results.
If the measurements can be modeled as an additive mixture of
different sources, including task-related signals and artifacts, applying blind source separation prior to the further processing, visualization, or interpretation may better reveal the underlying
physical phenomena (such as different brain processes), which in
the raw data could be contaminated or overwhelmed by other processes of no interest.
A survey of relevant papers shows that the most popular architecture for ANNs is the multilayer perceptron (MLP). The MLP
architecture is both simple and straightforward to implement and
use. In MLPs, information flows in one direction except during
training, when error terms are backpropagated. Backpropagation
updates network weights in a supervised manner. Although it cannot guarantee a globally minimal solution, backpropagation at least
arrives at a local minimum through gradient descent. Various techniques have been derived in an attempt to avoid overfitting to a
local minimum. Once the network weights have been learned and
fixed, feedforward networks can be implemented in hardware and
made to run in real time. All of these characteristics make the
backpropagation algorithm most popular in biomedical applications.
In some applications, target outputs may not be available or may
be too expensive to acquire. In these cases, unsupervised learning
algorithms may be used. Among unsupervised learning algorithms,
self-organizing maps (SOMs) are the most popular for biomedical
applications. During training, SOMs attempt to assign their input
patterns to different output regions. Often SOMs may converge
after only few learning cycles.

Application Issues
Although most published papers have concluded that ANNs are
appropriate for their domain of interest, many issues still have to
be resolved before ANNs can be claimed to be the general method
of choice. Unfortunately, most published studies have not gone
beyond demonstrating application to a very limited amount of data.
As with any type of method, ANNs have their limitations that
should be carefully considered:
• Every study should provide a rationale for the data chosen as
input. For example, ANN-based computer-aided diagnostic systems may give misleading results if the ANNs are not given
adequately representative features and sufficient naturally occurring data variations in their training data. With ANNs, any input
may yield some sort of output, correct and useful or not (“garbage
in, garbage out”). Therefore, the keys to success of ANN applications are not only to pick an appropriate architecture or learning
algorithm, but also to choose the right data and data features to
train the network.
• Although methods of applying ANNs to biomedical signals have
already shown great promise, great care must be taken to examine

178

Part III: Articles

the results obtained. The issue of trust in the outputs of ANNs
always deserves informed as well as statistical consideration.
Since medical diagnosis is nearly always a multifactorial and
multidisciplinary problem, medical experts should always evaluate network outputs in light of other direct or indirect convergent evidence before making final decisions affecting the health
of patients.
• Before practical implementation is planned, ANN methods
should be compared to more direct ways of obtaining the same
answers, as these might sometimes prove more accurate or cost
effective.

Model Mining
Since the first wave of popularization of backpropagation networks
nearly two decades ago, an ever greater number and variety of ANN
models have been devised to tackle an ever greater variety of problems. The overall insight that ANNs both embody and exemplify
is perhaps that our human intelligence is multifactorial and highly
adaptable to using whatever forms of information are available to
us. In this spirit, we suggest that researchers always attempt to
interpret the physiological meaning both of the features of their
input data and of the data models that their trained ANNs represent.
Too often ANNs have been treated like black boxes. We believe it
is time to open the black boxes and interpret what is happening
inside them. Such interpretations might even yield new insights
into the nature of the biomedical signals, or suggest new or more
efficient ways to look at the input data. It is also possible that the
ANN models and methods might suggest more efficient methods
to collect input data. Such “model mining” might even prove to be
the most rewarding result of applying ANNs. Researchers who simply recount classification accuracy may ignore nuggets of novel
information about brain processes hidden in the ANN models that
they and the data have jointly constructed.
Road Map: Implementation and Analysis
Related Reading: EEG and MEG Analysis; Muscle Models; Neuroinformatics; Statistical Parametric Mapping of Cortical Activity Patterns

References
Dawant, B. M., Ozkan, M., Zijdenbos, A., and Margolin, R., 1991, A computer environment for 2D and 3D quantitation of MR images using neural networks, Magn. Reson. Imaging, 20:64–65.

Dayhoff, J. E., and DeLeo, J. M., 2001, Artificial neural networks: Opening
the black box, Cancer, 91:1615–1635. ⽧
Duann, J. R., Jung, T. P., Kuo, W. J., Yeh, T. C., Makeig, S., Hsieh, J. C.,
and Sejnowski, T. J., 2002, Single-trial variability in event-related
BOLD signals, NeuroImage, 15:823–835. ⽧
Haykin, S., 1999, Neural Network: A Comprehensive Foundation, Englewood Cliffs, NJ: Prentice Hall. ⽧
Jung, T.-P., Makeig, S., Stensmo, M., and Sejnowski, T. J., 1997, Estimating alertness from the EEG power spectrum, IEEE Trans. Biomed.
Eng., 44:60–69.
Jung, T.-P., Makeig, S., McKeown, M. J., Bell, A. J., Lee, T.-W. and Sejnowski, T. J., 2001, Imaging brain dynamics using independent component analysis, Proc. IEEE, 89:1107–1122. ⽧
Khalil, M., and Duchene, J., 2000, Uterine EMG analysis, a dynamic approach for change detection and classification, IEEE Trans. Biomed.
Eng., 47:748–756.
Makeig, S., Jung, T.-P., Bell, A. J., Ghahremani, D., and Sejnowski, T. J.,
1997, Blind separation of auditory event-related brain responses into
independent components, Proc. Natl. Acad. Sci. USA, 94:10979–
10984. ⽧
McKeown, M. J., Jung, T.-P., Makeig, S., Brown, G. G., Kindermann, S. S.,
Lee, T.-W., and Sejnowski, T. J., 1998, Spatially independent activity
patterns in functional MRI data during the Stroop color-naming task,
Proc. Natl. Acad. Sci. USA, 95:803–810. ⽧
Ogawa, S., Tank, D., Menon, R., Ellermann, J., Kim, S., Merkle, H., and
Ugurbil, K., 1992, Intrinsic signal changes accompanying sensory stimulation: Functional brain mapping with magnetic resonance imaging,
Proc. Natl. Acad. Sci. USA, 89:5951–5959. ⽧
Peters, B. O., Pfurtscheller, G., and Flyvbjerg, H., 2001, Automatic differentiation of multichannel EEG signals, IEEE Trans. Biomed. Eng.,
48:111–116.
Reddick, W. E., Glass, J. O., Cook, E. N., Elkin, T. D., and Deaton R. J.,
1997, Automated segmentation and classification of multispectral magnetic resonance images of brain using artificial neural networks, IEEE
Trans. Med. Imaging, 16:911–918.
Stites, E. C., and Abbas, J. J., 2000, Sensitivity and versatility of an adaptive
system for controlling cyclic movements using functional neuromuscular
stimulation, IEEE Trans. Biomed. Eng., 47:1287–1292. ⽧
Sun, M., and Sclabassi, R. J., 2000, The forward EEG solution can be
computed using artificial neural networks, IEEE Trans. Biomed. Eng.,
47:1044–1050. ⽧
Zhang, X.-S., Roy, R. J., Schwender, D., and Daunderer, M., 2001, Discrimination of anesthetic states using mid-latency auditory evoked potential and artificial neural networks, Ann. Biomed. Eng., 29:446–453.

Brain-Computer Interfaces
José del R. Millán
Introduction
There is a growing interest in the use of physiological signals for
communication and operation of devices for the severely motor
disabled as well as for able-bodied people. Over the last decade
evidence has accumulated to show the potential of analyzing brainwaves on-line to derive information about the subject’s mental state
that is then mapped into some external action such as selecting a
letter from a virtual keyboard or moving a robotics device. A braincomputer interface (BCI) is an alternative communication and control channel that does not depend on the brain’s normal output
pathway of peripheral nerves and muscles (Wolpaw et al., 2000).
Most BCI systems use electroencephalogram signals (EEG and
MEG ANALYSIS) measured from scalp electrodes that do not require invasive procedures. Although scalp EEG is a simple way to
record brainwaves, it does not provide detailed information on the

activity of single neurons (or small clusters of neurons) that could
be recorded by implanted electrodes in the cortex (PROSTHETICS,
NEURAL). Such a direct measurement of brain activity may, in principle, enable faster recognition of mental states and even more
complex interactions.
A BCI may monitor a variety of brainwave phenomena. Some
groups exploit evoked potentials generated in response to external
stimuli (see Wolpaw et al., 2000, for a review). Evoked potentials
are, in principle, easy to pick up but are constrained by the fact
that the subject must be synchronized to the external machinery. A
more natural and practical alternative is to rely on components
associated with spontaneous mental activity. Such spontaneous
components range from slow cortical potentials of the EEG (e.g.,
Birbaumer et al., 1999), to variations of EEG rhythms (e.g., Wolpaw and McFarland, 1994; Kalcher et al., 1996; Anderson, 1997;

Brain-Computer Interfaces
Roberts and Penny, 2000; Millán et al., 2002b), to the direct activity of neurons in the cortex (e.g., Kennedy et al., 2000; Wessberg
et al., 2000).

Direct Brain-Computer Interfaces
Direct BCIs involve invasive procedures to implant electrodes in
the brain (PROSTHETICS, NEURAL). Apart from ethical concerns, a
major difficulty is to obtain reliable long-term recordings of neural
activity. Recent advances have made it possible to develop direct
BCIs with animals and even human beings.
Kennedy and colleagues (2000) have implanted a special electrode into the motor cortex of several paralyzed patients. These
electrodes contain a neurotrophic factor that induces growth of neural tissue within the hollow electrode tip. With training, patients
learn to control the firing rates of the multiple recorded neurons to
some extent. One of them is able to drive a cursor and write
messages.
Wessberg et al. (2000) have recorded the activity of ensembles
of neurons with microwire arrays implanted in multiple cortical
regions involved in motor control, as monkeys performed arm
movements. From these signals they have obtained accurate realtime predictions of arm trajectories and have been able to reproduce
the trajectories with a robot arm. Although these experiments do
not describe an actual BCI, they support the feasibility of controlling complex prosthetic limbs directly by brain activity. In addition,
earlier work by Nicolelis and colleagues showed that neural predictors can be derived for rats implanted with the same kind of
microelectrodes (see Nicolelis, 2001, for details and reference). The
rats were trained to press a bar to move a simple device delivering
water, and later learned to operate this device through neural activity only.
For a more detailed analysis and prospects of this area, see Nicolelis (2001).

Noninvasive Brain-Computer Interfaces
Noninvasive BCIs are based on the analysis of EEG phenomena
associated with various aspects of brain function. Thus, Birbaumer
et al. (1999) measure slow cortical potentials (SCP) over the vertex
(top of the scalp). SCP are shifts in the depolarization level of the
upper cortical dendrites and indicate the overall preparatory excitation level of a cortical network. Other groups look at local variations of EEG rhythms. The most commonly used rhythms are
related to the imagination of movements and are recorded from the
central region of the scalp overlying the sensorimotor cortex. In
this respect, there exist two main paradigms. Pfurtscheller’s team
works with event-related desynchronization (ERD, EEG and MEG
ANALYSIS) computed at fixed time intervals after the subject is
commanded to imagine specific movements of the limbs (Kalcher
et al., 1996; Obermaier, Müller, and Pfurtscheller, 2001). Alternatively, Wolpaw and co-workers analyze continuous changes in
the amplitudes of the l (8–12 Hz) or b (13–28 Hz) rhythms (Wolpaw and McFarland, 1994). Finally, in addition to motor-related
rhythms, Anderson (1997) and Millán et al. (2002b) also analyze
continuous variations of EEG rhythms, but not only over the sensorimotor cortex and in specific frequency bands. The reason is that
a number of neurocognitive studies have found that different mental tasks—such as imagination of movements, arithmetic operations, or language—activate local cortical areas to different extents.
The insights gathered from these studies guide the placement of
electrodes to obtain more relevant signals for the different tasks to
be recognized. In this latter case, rather than looking for predefined
EEG phenomena as in the previous paradigms, the approach aims
at discovering EEG patterns embedded in the continuous EEG signal associated with different mental states.

179

Most of the existing BCIs are based on synchronous experimental protocols in which the subject must follow a fixed repetitive
scheme to switch from one mental task to the next (Wolpaw and
McFarland, 1994; Kalcher et al., 1996; Wolpaw and McFarland,
1994; Birbaumer et al., 1999; Obermaier et al., 2001). A trial consists of two parts. A first cue warns the subject to get ready and,
after a fixed period of several seconds, a second cue tells the subject
to undertake the desired mental task for a predefined time. The
EEG phenomena to be recognized are time locked to the last cue
and the BCI responds with the average decision over the second
period of time. In these synchronous BCI systems, the shortest trial
lengths that have been reported are 4 s (Birbaumer et al., 1999) and
5 s (Obermaier et al., 2001). This relatively long time is necessary
because the EEG phenomena of interest, either SCP or ERD, need
some seconds to recover. On the contrary, other BCIs rely on more
flexible asynchronous protocols where the subject makes self-paced
decisions on when to stop doing a mental task and start immediately the next one (Roberts and Penny, 2000; Millán et al., 2002b).
In this second case, the time of response of the BCI goes from 0.5
s (Millán et al., 2002b) to several seconds (Roberts and Penny,
2000).
EEG signals are characterized by a poor signal-to-noise ratio and
spatial resolution. Their quality is greatly improved by means of a
Surface Laplacian (SL) derivation, which requires a large number
of electrodes (normally 64–128). The SL estimate yields new potentials that represent better the cortical activity originated in radial
sources immediately below the electrodes (for details see McFarland et al., 1997; Babiloni et al., 2001; and references therein).
The superiority of SL-transformed signals over raw potentials for
the operation of a BCI has been demonstrated in different studies
(e.g., McFarland et al., 1997). Although significant progress has
been obtained (and will still continue) with studies using a high
number of EEG electrodes (from 26 to 128), today’s practical BCI
systems should have a few electrodes (no more than 10) to allow
their operation by laypersons, as the procedure of electrode positioning is time consuming and critical. Most groups have developed
BCI prototypes with a limited number of electrodes that, however,
do not benefit from SL transformations. On the contrary, Babiloni
et al. (2001) and Millán et al. (2002b) compute SL derivations from
a few electrodes, using global and local methods, respectively.
Wolpaw and McFarland (1994) as well as Birbaumer et al.
(1999) have demonstrated that some subjects can learn to control
their brain activity through appropriate, but lengthy, training in
order to generate fixed EEG patterns that the BCI transforms into
external actions. In both cases the subject is trained over several
months to modify the amplitude of either the SCP or l rhythm,
respectively. A few other groups follow machine learning approaches to train the classifier embedded in the BCI. These techniques range from linear classifiers (Babiloni et al., 2001; Obermaier et al., 2001), to compact multi-layer perceptrons and
Bayesian neural networks (Anderson, 1997; Roberts and Penny,
2000), to variations of LVQ (Kalcher et al., 1996), to local neural
classifiers (Millán, 2002; Millán et al., 2002b). Most of these works
deal with the recognition of just two mental tasks (Roberts and
Penny, 2000; Babiloni et al., 2001; Obermaier et al., 2001), or
report classification errors bigger than 15% for three or more tasks
(Kalcher et al., 1996; Anderson, 1997). An exception is Millán’s
approach that achieves error rates below 5% for three mental tasks,
but correct recognition is 70% (Millán, 2002; Millán et al., 2002b).
Obermaier et al. (2001) reports on a single disabled person who,
after several months of training, has reached a performance level
close to 100%. It is also worth noting that some of the subjects
who follow Wolpaw’s approach are able to control their l rhythm
amplitude at four different levels. These classification rates, together with the number of recognizable tasks and duration of the
trials, yield bit rates from approximately 0.15 to 2.0.

180

Part III: Articles

Some approaches are based on a mutual learning process in
which the user and the brain interface are coupled together and
adapt to each other (Roberts and Penny, 2000; Obermaier et al.,
2001; Millán, 2002; Millán et al., 2002b). This should accelerate
the training time. Thus, Millán’s approach has allowed subjects to
achieve good performances in just a few hours of training (Millán,
2002; Millán et al., 2002b). Analysis of learned EEG patterns confirms that for a subject to operate a personal BCI satisfactorily, the
BCI must fit the individual features of the user (Millán et al.,
2002a).
Another important concern in BCI is the incorporation of rejection criteria to avoid making risky decisions for uncertain samples.
This is extremely important from a practical point of view. Roberts
and Penny (2000) apply Bayesian techniques for this purpose,
while Millán et al. (2002b) use a confidence probability threshold.
In this latter case, more than ten subjects have experimented with
their BCI (Millán, 2002; Millán et al., 2002b). Most of them were
trained for a few consecutive days (from three to five). Training
time was moderate, around half an hour daily. Experimental results
show that, at the end of training, the correct recognition rates were
70% (or higher) for three mental tasks. This figure is more than
twice random classification. This modest rate is largely compensated by two properties: wrong responses were below 5% (in many
cases even below 2%) and decisions were made every half-second.
Some other subjects have undertaken consecutive training sessions
(from four to seven) in a single day. None of these subjects had
previous experience with BCIs and, in less than two hours, all of
them reached the same excellent performance as noted previously.
It is worth noting that one of the subjects was a physically impaired
person suffering from spinal muscular atrophy.

Brain-Actuated Applications
These different BCI systems are being used to operate a number
of brain-actuated applications that augment people’s communication capabilities, provide new forms of education and entertainment, and also enable the operation of physical devices. There exist
virtual keyboards for selecting letters from a computer screen and
writing a message (Birbaumer et al., 1999; Obermaier et al., 2001;
Millán, 2002). Using these three different approaches, subjects can
write a letter every 2 minutes, every 1 minute, and every 22 seconds, respectively. Wolpaw’s group has also its own virtual keyboard (Wolpaw, personal communication). A patient who has been
implanted with Kennedy and colleagues’ special electrode has
achieved a spelling rate of about three letters per minute using a
combination of neural and EMG signals (Kennedy et al., 2000).
On the other hand, it is also possible to make a brain-controlled
hand orthosis open and close (see references in Wolpaw et al.,
2000; Obermaier et al., 2001) and even guide in a continuous manner a motorized wheelchair with on-board sensory capabilities
(Millán, 2002). In this latter case, the key idea is that users’ mental
states are associated with high-level commands that the wheelchair
executes autonomously (ROBOT NAVIGATION). Another critical aspect for the control of the wheelchair is that subjects can issue highlevel commands at any moment, as the operation of the BCI is selfpaced and does not require waiting for specific events.
Finally, Millán (2002) illustrates the operation of a simple computer game, but other educational software could have been selected instead.

Discussion
Despite recent advancements, BCI is a field still in its infancy and
several issues must be addressed to improve the speed and performance of BCI. One of them is the exploration of local components
of brain activity with fast dynamics that subjects can consciously

control. For this we will need increased knowledge of the brain
(where and how cognitive and motor decisions are made) as well
as the application of more powerful digital signal processing (DSP)
methods than those commonly used currently. In addition, extraction of more relevant features, by means of these DSP methods,
together with the use of more appropriate classifiers, will improve
BCI performance in terms of classification rates and the number of
recognizable mental tasks. It may be possible to apply recurrent
neural networks to exploit temporal dynamics of brain activity.
However, a main limitation in scaling up the number of recognizable mental tasks is the quality—signal-to-noise ratio (SNR)—and
resolution of the measured brain signals. This is especially true in
the case of EEG-based BCIs, in which the SNR is very poor and
we cannot get detailed information on the activity of small cortical
areas unless we use a large number of electrodes (64, 128, or more).
It is thus crucial to develop better electrodes that are also easy to
position, thereby enabling the use of a large number of electrodes
even by laypersons. Finally, another key concern is to keep the BCI
constantly tuned to its owner. This requirement arises because, as
subjects gain experience, they develop new capabilities and change
their EEG patterns. In addition, brain activity changes from one
session (with which data the classifier is trained) to the next (where
the classifier is applied). The challenge here is to adapt the classifier
on-line while the subject operates a brain-actuated application, even
if the subject’s intention is not known until later. In this respect,
local neural networks are better suited for on-line learning (STATISTICAL MECHANICS OF ON-LINE LEARNING AND GENERALIZATION) than other methods, due to their robustness against catastrophic interference. This list of topics is not exhaustive, but space
limits prevent further discussion (see Wolpaw et al., 2000, for additional details on these and other issues).
Although the immediate application of BCI is to help physically
impaired people, its potentials are extensive. Ultimately, they may
lead to the development of truly adaptive interactive systems that,
on the one hand, augment human capabilities by giving the brain
the possibility to develop new skills and, on the other hand, make
computer systems fit the pace and individual features of their owners. Most probably, people will use BCI in combination with other
sensory interaction modalities (e.g., speech, gestures) and physiological signals (e.g., electromyogram, skin conductivity). Such a
multimodal interface will yield a higher bit rate of communication
with better reliability than would occur if only brainwaves were
utilized. On the other hand, the incorporation of other interaction
modalities highlights a critical issue in BCI, namely the importance
of filtering out from the recorded brain signals non-CNS artifacts
originated by movements of different parts of the body. INDEPENDENT COMPONENT ANALYSIS (q.v.) is a method of detecting and
removing such artifacts.
Road Map: Applications
Related Reading: Event-Related Potentials; Kalman Filtering: Neural Implications; Prosthetics, Motor Control; Prosthetics, Sensory Systems

References
Anderson, C. W., 1997, Effects of variations in neural network topology
and output averaging on the discrimination of mental tasks from spontaneous EEG, J. Intell. Syst., 7:165–190.
Babiloni, F., Cincotti, F., Bianchi, L., Pirri, G., Millán, J. del R., Mouriño,
J., Sallinari, S., and Marciani, M. B., 2001, Recognition of imagined
hand movements with low resolution surface Laplacian and linear classifiers, Med. Eng. & Physics, 23:323–328.
Birbaumer, N., Ghanayim, N., Hinterberger, T., Iversen, I., Kotchoubey,
B., Kübler, A., Perelmouter, J., Taub, E., and Flor, H., 1999, A spelling
device for the paralysed, Nature, 398:297–298.
Kalcher, J., Flotzinger, D., Neuper, C., Gölly, S., and Pfurtscheller, G.,
1996, Graz brain-computer interface II, Med. Biol. Eng. Comput.,
34:382–388.

Canonical Neural Models
Kennedy, P. R., Bakay, R. A. E., Moore, M. M., Adams, K., and Goldwaithe, J., 2000, Direct control of a computer from the human central
nervous system, IEEE Trans. Rehab. Eng., 8:198–202.
McFarland, D. J., McCane, L. M., David, S. V., and Wolpaw, J. R., 1997,
Spatial filter selection for EEG-based communication, Electroenceph.
Clin. Neurophysiol., 103:386–394.
Millán, J. del R., 2002, Adaptive brain interfaces, Comm. of the ACM, to
appear. ⽧
Millán, J. del R., Franzé, M., Mouriño, J., Cincotti, F., and Babiloni, F.,
2002a, Relevant EEG features for the classification of spontaneous
motor-related tasks, Biol. Cybern., 86:89–95.
Millán, J. del R., Mouriño, J., Franzé, M., Cincotti, F., Varsta, M., Heikkonen, J., and Babiloni, F., 2002b, A local neural classifier for the recognition of EEG patterns associated to mental tasks, IEEE Trans. on
Neural Networks, 11:678–686.
Nicolelis, M. A. L., 2001, Actions from thoughts, Nature, 409:403–407. ⽧
Obermaier, B., Müller, G., and Pfurtscheller, G., 2001, “Virtual Keyboard”
controlled by spontaneous EEG activity, in Proceedings of the Inter-

181

national Conference on Artificial Neural Networks, Heidelberg:
Springer-Verlag.
Roberts, S. J., and Penny, W. D., 2000, Real-time brain-computer interfacing: A preliminary study using Bayesian learning, Med. Biol. Eng. Computing, 38:56–61.
Wessberg, J., Stambaugh, C. R., Kralik, J. D., Beck, P. D., Laubach, M.,
Chapin, J. K., Kim, J., Biggs, S. J., Srinivassan, M. A., and Nicolelis,
M. A. L., 2000. Real-time prediction of hand trajectory by ensembles of
cortical neurons in primates, Nature, 408:361–365.
Wolpaw, J. R., and McFarland, D. J., 1994, Multichannel EEG-based braincomputer communication, Electroenceph. Clin. Neurophysiol., 90:444–
449.
Wolpaw, J. R., Birbaumer, N., Heetderks, W. J., McFarland, D. J., Peckham, P. H., Schalk, G., Donehin, E., Quatrano, L. A., Robinson, C. J.,
and Vaughan, T. M., 2000, Brain-computer interface technology: A review of the first international meeting, IEEE Trans. on Rehab. Eng.,
8:164–173. Special Section on Brain-Computer Interfaces. ⽧

Canonical Neural Models
Frank Hoppensteadt and Eugene M. Izhikevich
Introduction
Mathematical modeling is a powerful tool for studying the fundamental principles of information processing in the brain. Unfortunately, mathematical analysis of a certain neural model could be
of limited value, because the results might depend on the particulars
of that model: various models of the same neural structure could
produce different results. For example, if an investigator obtains
results with a Hodgkin-Huxley-type model (see AXONAL MODELING) and then augments the model by adding more parameters and
variables to take into account more neurophysiological data, would
similar results emerge? A reasonable way to circumvent this problem is to derive results that are largely independent of the model
and that can be observed in a class or a family of models.
Having understood the importance of considering families of
neural models instead of a single model, we carry out this task by
reducing an entire family of Hodgkin-Huxley-type models to a canonical model (for precise definitions, see Section 4.1 in Hoppensteadt and Izhikevich, 1997). Briefly, a model is canonical for a
family if there is a continuous change of variables that transforms
any other model from the family into this one, as we illustrate in
Figure 1. For example, the entire family of weakly coupled oscillators of the form in Equation 1 can be converted into the canonical
phase model described by Equation 6, where Hij depend on the
particulars of the functions fi and gij. The change of variables does
not have to be invertible, so the canonical model is usually lower
dimensional, simple, and tractable. Yet it retains many important
features of the family. For example, if the canonical model has
multiple attractors, then each member of the family has multiple
attractors.
The major advantage to considering canonical models is that one
can study universal neurocomputational properties that are shared
by all members of the family, since all such members can be put
into the canonical form by a continuous change of variables. Moreover, an investigator need not actually present such a change of
variables explicitly, so that derivation of canonical models is possible even when the family is so broad that most of its members
are given implicitly, e.g., in the abstract form of Equation 1. For
example, the canonical phase model in Equation 6 reveals universal
computational abilities (e.g., oscillatory associative memory) that
are shared by all oscillatory systems, regardless of the nature of

each oscillator or the particulars of the equations that describe it.
Thus, the canonical model approach provides a rigorous way to
obtain results when only partial information about neuron dynamics is known. Many examples are given subsequently in this article.
The process of deriving canonical neural models is more an art
than a science, because a general algorithm for doing so is not
known. However, much success has been achieved when we consider weakly connected networks of neurons whose activity is near
a bifurcation, a situation that often occurs when the membrane
potential is near the threshold value (see DYNAMICS AND BIFURCATION IN NEURAL NETS and PHASE-PLANE ANALYSIS OF NEURAL
NETS). We review such bifurcations and corresponding canonical

Figure 1. Dynamical system ẏ ⳱ g( y) is a canonical model for the family
{ f1, f2, f3, f4} of neural models ẋ ⳱ f (x) because each such model can be
transformed into the form ẏ ⳱ g( y) by the continuous change of variables
hi .

182

Part III: Articles

models. Their rigorous derivation and detailed analysis can be
found in Hoppensteadt and Izhikevich (1997).

Weakly Connected Neural Networks
The assumption of weak neuronal connections is based on the observation that the typical size of a postsynaptic potential is less than
1 mV, which is small in comparison with the mean size necessary
to discharge a cell (around 20 mV) or the average size of the action
potential (around 100 mV) (for a detailed review of relevant electrophysiological data, see Hoppensteadt and Izhikevich, 1997,
chap. 1). From the mathematical point of view, this results in neural
models of “weakly connected” form
n

ẋi ⳱ f (xi, ki) Ⳮ e

兺

gij(xi, xj, e)

(1)

j⳱1

where each vector xi 僆 ⺢ m describes membrane potential, gating
variables, and other electrophysiological variables of the ith neuron
(see ION CHANNELS: KEYS TO NEURONAL SPECIALIZATION). Each
vector ki 僆 ⺢ l denotes various biophysical parameters of the neuron. The function f describes the neuron’s dynamics, and the functions gij describe connections between the neurons. The dimensionless parameter e K 1 is small, reflecting the strength of
connections between neurons.

Bistability and Hysteresis
Bistable and hysteretic dynamics are ubiquitous in neural models,
and they may play important roles in biological neurons. The cusp
bifurcation depicted in Figure 2 is one of the simplest bifurcations
leading to such dynamics. For example, the sigmoidal neuron
ẋ ⳱ ⳮx Ⳮ aS(x), S(x) ⳱ 1/(1 Ⳮ eⳮx)

parameters ki do not affect the form of the canonical model. They
only affect the parameters ri and sij. Thus, by studying the canonical
model in Equation 2 one can gain some insight into the neurocomputational behavior of any neural model near a cusp bifurcation,
whether it is a simple sigmoidal neuron or a biophysically detailed
conductance-based (Hodgkin-Huxley-type) neuron.
The canonical model in Equation 2 is quite simple: each equation
has only one nonlinear term, namely, y 3i , and two internal parameters, ri and sii. Still, the Cohen-Grossberg-Hopfield convergence
theorem applies, which means that the canonical model has the
same neurocomputational properties as the standard Hopfield network (see COMPUTING WITH ATTRACTORS).

Theorem 1 (Cohen-Grossberg-Hopfield
Convergence Theorem)
If the connection matrix S ⳱ (sij) is symmetric, then the canonical
neural network of Equation 2 is a gradient system.
One can easily check that
n

E( y) ⳱ ⳮ 兺 (ri yi ⳮ 1⁄4 y i4) ⳮ 1⁄2
i⳱1

n

兺 sij yi yj
i, j⳱1

is a potential function for Equation 2 in the sense that y i⬘ ⳱ ⳮE/
yi (see also ENERGY FUNCTIONALS FOR NEURAL NETWORKS).

Small-Amplitude Oscillations
Many biophysically detailed neural models can exhibit smallamplitude (damped) oscillations of the membrane potential, especially when the system is near transition from rest state to periodic
activity. In the simplest case this corresponds to the supercritical

is at a cusp bifurcation point x ⳱ 0.5 when a ⳱ 4. It is bistable
when a  4. If each neuron in the weakly connected network described by Equation 1 is near a supercritical cusp bifurcation, then
the entire network can be transformed into the canonical form
(Hoppensteadt and Izhikevich, 1997)
n

y ⬘i ⳱ ri ⳮ y 3i Ⳮ

兺 sij yj
j⳱1

(2)

where each scalar yi 僆 ⺢ describes rescaled dynamics of the ith
neuron. Particulars of the functions f and gij and the value of the

Figure 2. Cusp surface.

Figure 3. Supercritical Andronov-Hopf bifurcation in ẋ ⳱ f (x, k). On the
left, the rest state is stable. In the middle, the rest state is losing stability,
giving birth to a stable limit cycle corresponding to periodic activity. On
the right, the system exhibits periodic activity.

Canonical Neural Models
Andronov-Hopf bifurcation in Figure 3. Many weakly connected
networks (in the form of Equation 1) of such neurons can be transformed into the canonical model

The self-adjoint synaptic matrix arises naturally when one considers complex Hebbian learning rules (Hoppensteadt and Izhikevich, 1996):

n

z ⬘i ⳱ (ri Ⳮ ixi)zi ⳮ zi |zi |2 Ⳮ

兺

cij zj

(3)

183

cij ⳱

j⳱1

1
n

k

兺

n si n¯ kj

(5)

s⳱1

by a continuous change of variables (Aronson, Ermentrout, and
Kopell, 1990). Here i ⳱ 冪ⳮ1, and each complex variable zi 僆 ⺓
describes oscillatory activity of the ith neuron. Again, particulars
of the form of the functions f and gij in Equation 1 affect only the
values of the parameters ri and xi and the complex-valued synaptic
coefficients cij 僆 ⺓.
Even though the canonical model in Equation 3 exhibits oscillatory dynamics, one can still prove the following analogue of the
Cohen-Grossberg convergence theorem, which implies that the canonical model in Equation 3 has oscillatory associative memory;
that is, it can memorize and retrieve complex oscillatory patterns
(Hoppensteadt and Izhikevich, 1996) (Figure 4).

where each vector n s ⳱ (n s1, . . . , n ns ) 僆 ⺓n denotes a pattern of
phase relations between neurons to be memorized (see also HEBBIAN SYNAPTIC PLASTICITY). Notice that the problem of negative
(mirror) images does not arise in oscillatory neural networks, since
both n k and ⳮn k result in the same phase relations.
The key difference between the Hopfield-Grossberg network and
the oscillatory network (Equation 3) is that memorized images correspond to equilibrium (point) attractors in the former and to limit
cycle attractors in the latter. Pattern recognition by an oscillatory
neural network involves convergence to the corresponding limit
cycle attractor, which results in synchronization of the network
activity with an appropriate phase relation between neurons, as in
Figure 4 (see also COMPUTING WITH ATTRACTORS).

Theorem 2 (Synchronization Theorem for Oscillatory
Neural Networks)

Large Amplitude Oscillations

If in the canonical neural network in Equation 3 all neurons have
equal frequencies x1 ⳱ . . . ⳱ xn and the connection matrix C ⳱
(cij) is self-adjoint, i.e.,
cij ⳱ c̄ji

for all i and j

(4)

then the network always converges to an oscillatory phase-locked
pattern; that is, the neurons oscillate with equal frequencies and
constant, but not necessarily identical, phases. There could be many
such phase-locked patterns corresponding to many memorized
images.
The proof follows from the existence of an orbital energy
function
n

E(z) ⳱ ⳮ 兺 (ri |zi |2 ⳮ 1⁄2 |zi |4) ⳮ
i⳱1

n

兺

cij z̄i zj

i, j⳱1

for Equation 3 (see ENERGY FUNCTIONALS
NETWORKS).

FOR

NEURAL

Figure 4. Pattern recognition via phase locking by the oscillatory canonical
model (Equation 3). Complex Hebbian learning rule (5) was used to memorize patterns “1,” “2,” and “3.” When the distorted pattern “1” is presented

Suppose that neurons in the weakly connected network described
by Equation 1 exhibit periodic spiking (Figure 5; see also CHAINS
OF OSCILLATORS IN MOTOR AND SENSORY SYSTEMS, COLLECTIVE
BEHAVIOR OF COUPLED OSCILLATORS, and PHASE-PLANE ANALYSIS OF NEURAL NETS). If they have nearly equal frequencies, then
the network can be transformed into the phase canonical model
n

u⬘i ⳱ xi Ⳮ

兺

Hij (uj ⳮ ui)

(6)

j⳱1

where each ui 僆 ⺣1 is a one-dimensional (angle) variable that describes the phase of the ith oscillator along the limit cycle attractor
corresponding to its periodic spiking (see Figure 5), and each Hij
is a function that depends on f and gij that can be explicitly computed using Malkin’s theorem (Theorem 9.2 in Hoppensteadt and
Izhikevich, 1997).
The phase canonical model (Equation 6) describes frequency
locking, phase locking, and synchronization properties of the original system in Equation 1. Therefore, to understand these and other
nonlinear phenomena that might take place in oscillating neural

as an initial state, the neurons synchronize with the phase relations corresponding to the memorized pattern “1.”

184

Part III: Articles
Figure 5. Examples of large-amplitude
limit cycle attractors corresponding to periodic spiking in two biophysically detailed neural models, Morris and Lecar
(1981) and Hodgkin and Huxley (1952).

networks, it usually suffices to consider the phase model. In particular, one can glimpse the universal computational abilities that
are shared by all oscillatory systems, regardless of the nature of
each oscillator or the particulars of the equations that describe it.
Indeed, one can prove the following analogue of Theorem 2.

for all i and j. If we denote cij ⳱ sij e iwij, then these conditions have
the form shown by Equation 4. The energy function for Kuramoto’s
model is
n

E(u) ⳱ ⳮ1⁄2

兺

sij cos (uj Ⳮ wij ⳮ ui)

i, j⳱1

Theorem 3 (Synchronization Theorem for Oscillatory
Neural Networks)
If all oscillators in Equation 6 have equal frequencies, i.e., x1 ⳱
. . . ⳱ xn , and the connection functions Hij have pairwise odd
form, i.e.,
Hij (ⳮw) ⳱ ⳮHji (w)

(7)

for all i and j, then the canonical phase model in Equation 6 converges to a phase-locked pattern ui (t) r x1t Ⳮ ␾i for all i, so the
neurons oscillate with equal frequencies (x1) and constant phase
relations (ui (t) ⳮ uj (t) ⳱ ␾i ⳮ ␾j). In this sense the network
dynamic is synchronized. There could be many stable synchronized
patterns corresponding to many memorized images.
The proof is based on the observation that the phase canonical
model in Equation 6 has the energy function
n

E(u) ⳱ 1⁄2

兺

Rij (uj ⳮ ui)

i, j⳱1

where Rij is the antiderivative of Hij ; that is, R⬘ij ⳱ Hij (see Theorem
9.15 in Hoppensteadt and Izhikevich, 1997, and ENERGY FUNCTIONALS FOR NEURAL NETWORKS).
For example, Kuramoto’s (1984) model
n

u⬘i ⳱ xi Ⳮ

兺 sij sin (uj Ⳮ wij ⳮ ui)
j⳱1

(8)

has such an oscillatory associative memory when x1 ⳱ . . . ⳱ xn:
sij ⳱ sji and wij ⳱ ⳮwji

There are various estimates of the storage capacity of the network, as discussed by Vicente, Arenas, and Bonilla (1996). In particular, those authors found a time scale during which oscillatory
networks can have better performance than Cohen-GrossbergHopfield-type attractor neural networks.
Since neither the form of the functions f and gij nor the dimension
of each oscillator in Equation 1 were specified, one could take the
above result to the extreme and claim that anything that can oscillate can also be used for computing, as for associative pattern recognition, etc. The only problem is how to couple the oscillators so
that Equation 7 is satisfied.

Neural Excitability
An interesting intermediate case between rest and periodic spiking
behavior is when a neural system is excitable; that is, it is at rest,
but can generate a large-amplitude spike in response to a small
perturbation (Figure 6) (see PHASE-PLANE ANALYSIS OF NEURAL
NETS and OSCILLATORY AND BURSTING PROPERTIES OF NEURONS). A simple but useful criterion for classifying neural excitability was suggested by Hodgkin (1948), who stimulated cells by
applying currents of various strengths. When the current is weak
the cell is quiet. When the current is strong enough, the cell starts
to fire repeatedly. He suggested the following classification according to the emerging frequency of firing (Figure 7):
• Class 1 neural excitability: Action potentials can be generated
with arbitrarily low frequency. The frequency increases as the
applied current is increased.
• Class 2 neural excitability: Action potentials are generated in a
certain frequency band that is relatively insensitive to changes in
the strength of the applied current.

Canonical Neural Models

185

Figure 6. Neural excitability in Morris
and Lecar (1981) neuron having fast
Ca2Ⳮ and slow KⳭ voltage-gated currents. The rest state (solid circle) is stable,
but small perturbations can push the voltage beyond the threshold (open circle),
thereby causing a large-amplitude excursion, or action potential. The voltage variable changes slowly near the rest states
but fast during the generation of action
potentials.

Their class of excitability influences neurocomputational properties
of cells (see review in Izhikevich, 2000). For example, class 1
neural systems have a well-defined threshold manifold for their
state variables, beyond which they generate a large-amplitude
spike. They generate an all-or-none response, and they act as integrators, meaning that the higher the frequency of the incoming
pulses, the sooner they fire. In contrast, class 2 neural systems may
not have a threshold manifold. They could generate spikes of arbitrary intermediate amplitude, and they could act as resonators.
That is, they respond to certain resonant frequencies of the incoming pulses. Increasing the incoming frequency may delay or even
terminate their response.
A canonical model for class 1 excitable systems is described in
the next section. The canonical model for class 2 systems has yet
to be found.

Class 1 Excitable Systems
Class 1 excitable systems are understood relatively well (Rinzel
and Ermentrout, 1989; Ermentrout, 1996; Hoppensteadt and Izhi-

kevich, 1997; Izhikevich, 2000). The transition from rest to periodic spiking in such systems occurs via a saddle node on invariant
circle bifurcation, as shown in Figure 8 (see also DYNAMICS AND
BIFURCATION IN NEURAL NETS and OSCILLATORY AND BURSTING
PROPERTIES OF NEURONS). A weakly connected network of such
neurons can be transformed into a canonical model, which can be
approximated by

冢

␽ i⬘ ⳱ 1 ⳮ cos ␽i Ⳮ (1 Ⳮ cos ␽i) ri Ⳮ

n

兺 sij d(␽j ⳮ p)冣
j⳱1

(9)

where ␽i 僆 ⺣1 is the phase of the ith neuron along the limit cycle
corresponding to the spiking solution. Again, particulars of the
functions f and gij in Equation 1 do not affect the form of
the canonical model in Equation 9, but only affect the values of
the parameters ri and sij , which can be computed explicitly (Hoppensteadt and Izhikevich, 1997, chap. 8). Notice that the canonical
model in Equation 9 is pulse coupled, whereas the original weakly
coupled network in Equation 1 is not. The qualitative reason for
pulse coupling is that the voltage changes are extremely slow most

Figure 7. Transition from rest to repetitive spiking in two biophysical models when the strength of applied current, I, increases. The neural excitability is
classified according to the frequency of emerging spiking.

186

Part III: Articles
where wi describes slow synaptic processes. The term siiwi denotes
not a self-synapse but a slow spike frequency adaptation (sii  0)
or facilitation (sii  0) process.

Discussion

Figure 8. Class 1 neural excitability via saddle node on invariant circle
bifurcation: The threshold state (saddle) approaches the rest state (node),
they coalesce and annihilate each other leaving only limit cycle attractor.
The oscillation on the attractor has two time scales: slow transition through
the “ghost” of the saddle-node bifurcation and fast rotation along the rest
of the limit cycle.

of the time because of the proximity to the rest state, but they are
relatively instantaneous during the generation of an action potential. Hence the duration of coupling looks infinitesimal on the slow
time scale.
The neuron is quiescent when ri  0 (Figure 8, left) and fires
periodically when ri  0 (Figure 8, right). It fires a spike exactly
when ␽i crosses the value p, which results in a step-like increase
in the phases of other neurons. Hence, the canonical model in Equation 9 is a pulse coupled neural network (Izhikevich, 1999). It has
many important physiological features, including absolute and relative refractory periods (Figure 9). Indeed, the effect of every incoming pulse depends on the internal state of the neuron, since it
is multiplied by the function (1 Ⳮ cos␽i). The effect is maximal
when the neuron is near rest, since (1 Ⳮ cos␽i)  2 when ␽i  0.
It is minimal when the neuron is generating a spike, since (1 Ⳮ
cos␽i)  0 when ␽i  p.
A canonical model for slowly connected class 1 excitable neurons with spike frequency adaptation has the form (Izhikevich,
2000):

冢

␽ i⬘ ⳱ 1 ⳮ cos␽i Ⳮ (1 Ⳮ cos ␽i) ri Ⳮ
w i⬘ ⳱ d(␽i ⳮ p) ⳮ gwi

n

兺 sij wj冣
j⳱1

Figure 9. Diagram of the canonical model in Equation 9 for class 1 neural
excitability. (From Hoppensteadt and Izhikevich, 1997.)

The canonical model approach to computational neuroscience provides a rigorous way to derive simple yet accurate models that
describe single-cell or network dynamics (see SINGLE-CELL MODELS). Such a derivation is possible even when no assumptions are
made regarding the detailed form of equations describing neural
activity. Indeed, we specify neither f nor gij in Equation 1. The only
assumptions we make are those concerning the dynamics of each
neuron—whether it is quiescent, excitable, periodic spiking, and
so on. Nevertheless, any such neural system can be transformed
into a canonical model by a piecewise continuous change of
variables.
The derivation of canonical models can be a daunting mathematical task. However, once found, the canonical models provide
invaluable information about universal neurocomputational properties shared by a large family of neural systems. For example,
studying the canonical model in Equation 9 sheds light on the behavior of all class 1 excitable systems and their networks, regardless of the details of equations describing their dynamics.
Road Map: Dynamic Systems
Background: Dynamics and Bifurcation in Neural Nets; Phase-Plane Analysis of Neural Nets
Related Reading: Axonal Modeling; Chains of Oscillators in Motor and
Sensory Systems; Collective Behavior of Coupled Oscillators; Computing with Attractors; Cooperative Phenomena; Energy Functionals for
Neural Networks; Pattern Formation, Neural

References
Aronson, D. G., Ermentrout, G. B., and Kopell, N., 1990, Amplitude response of coupled oscillators, Physica D, 41:403–449.
Ermentrout, G. B., 1996, Type I membranes, phase resetting curves, and
synchrony, Neural Computat., 8:979–1001. ⽧
Hodgkin, A. L., 1948, The local electric changes associated with repetitive
action in a non-medulated axon, J. Physiol., 107:165–181.
Hodgkin, A. L., and Huxley, A. F., 1952, A quantitative description of
membrane current and application to conduction and excitation in nerve,
J. Physiol., 117:500–544.
Hoppensteadt, F. C., and Izhikevich, E. M., 1996, Synaptic organizations
and dynamical properties of weakly connected neural oscillators: II.
Learning of phase information, Biol. Cybern., 75:129–135. ⽧
Hoppensteadt, F. C., and Izhikevich, E. M., 1997, Weakly Connected Neural Networks, New York: Springer-Verlag.
Izhikevich, E. M., 1999, Class 1 neural excitability, conventional synapses,
weakly connected networks, and mathematical foundations of pulsecoupled models, IEEE Trans. Neural Netw., 10:499–507
Izhikevich, E. M., 2000, Neural excitability, spiking, and bursting, Int. J.
Bifurcat. Chaos, 10:1171–1266. ⽧
Kuramoto, Y., 1984, Chemical Oscillations, Waves, and Turbulence, New
York: Springer-Verlag.
Morris, C., and Lecar, H., 1981, Voltage oscillations in the barnacle giant
muscle fiber, Biophys. J., 35:193–213.
Rinzel, J., and Ermentrout, G. B., 1989, Analysis of neural excitability and
oscillations, in Methods in Neuronal Modeling (C. Koch and I. Segev,
Eds.), Cambridge, MA: MIT Press. ⽧
Vicente, C. J., Arenas, A., and Bonilla, L. L., 1996, On the short-term
dynamics of networks of Hebbian coupled oscillators, J. Phys. A, L9–
L16.

Cerebellum and Conditioning

187

Cerebellum and Conditioning
Jeffrey S. Grethe and Richard F. Thompson
Introduction
For many years, psychologists and neurobiologists have been
searching for the substrates underlying learning and memory. Aristotle hypothesized that learning involved the association of ideas
with one another. Pavlov (1927, cited in Gormezano, Kehoe, and
Marshall, 1983) combined the concepts of learning and association
with the production of reflexes (see CONDITIONING). Classical conditioning (CC) has been extremely useful in examining the substrates underlying learning and memory. The standard CC paradigm (Figure 1) consists of pairing a neutral stimulus, the
conditioned stimulus (CS), with an aversive stimulus, the unconditioned stimulus (US). At the beginning of training, the only response to the stimuli is the unconditioned response (UR) to the US.
Over repeated pairings of the stimuli, an association is formed between the CS and US, resulting in the performance of a conditioned
response (a response that resembles the UR).
By varying stimulus parameters, a large number of behavioralconditioning phenomena can be observed (Gormezano et al., 1983).
Eye-blink conditioning can occur with a CS-US interstimulus interval (ISI) ranging from 100 ms to well over 1 s. The rate of
learning and asymptotic response level are optimal at an ISI of
about 250 ms. After 100–200 training trials at the optimal ISI,
rabbits give CRs on more than 90% of trials. The CR onset initially
develops near the US onset, gradually begins earlier in the trial
over the course of training, and generally peaks near the US onset
(Figure 1). More complex conditioning phenomena can also be
observed (see CONDITIONING).

Cerebellar Substrates of Classical
Eye-Blink Conditioning
Current evidence from extensive anatomical, lesion, and physiological studies argues very strongly that the essential memory trace
for the classically conditioned nictitating membrane response is
formed and stored in the cerebellum (Thompson et al., 1997). This
research has identified much of the network subserving classical
conditioning (Figure 2). Information regarding the US is transmit-

Figure 1. Standard delay conditioning paradigm. The CS is a 350-ms tone
and the US is a 100-ms airpuff directed at the cornea. Early in training, the
animal responds to the airpuff (UR). Over time, the animal learns to as-

ted by the dorsal accessory olive to the cerebellum through the
climbing fibers. Stimulation of the dorsal accessory olive, in place
of a corneal airpuff, has been shown to be an effective US. If the
dorsal accessory olive is lesioned before paired presentations of the
CS and US, learning of the conditioned response is prevented. In
addition, the interpositus provides inhibitory feedback to the inferior olive, and over the course of learning, the inhibitory feedback
increases, thereby decreasing the output of the inferior olive. This
evidence points to the olivary climbing fiber system as being the
essential reinforcing pathway that transmits an error signal to the
cerebellum. Information regarding the CS is transmitted to the cerebellum from the pontine nuclei by the mossy fibers. Stimulation
of the pontine nuclei or mossy fibers has been shown to be an
effective CS. The CR pathway consists of the anterior interpositus
nucleus, magnocellular red nucleus, and finally the accessory abducens nucleus and the facial (seventh) nucleus. This circuit points
to the cerebellum as the site of convergence where associative
learning may take place. Experimental evidence supports this view,
since lesions of the cerebellum, including the critical regions of the
interpositus nucleus, completely abolish the conditioned response
without affecting the UR.
One of the first theories to detail how associative memories could
be formed in the cerebellum was proposed by Albus (1971). The
most striking aspect of this model was that the parallel fiber synapses on Purkinje cells were modifiable. This theory of parallel
fiber–Purkinje cell plasticity now has considerable experimental
support. In addition, Albus predicted the occurrence of long-term
depression at this synapse (see CEREBELLUM: NEURAL PLASTICITY). Furthermore, Albus predicted that climbing fiber spikes are
the US and that mossy fiber activity patterns are the CS, predictions
that are now supported by the conditioning literature. However,
this theory does not account for the temporal dynamics of the CR.
One of the more interesting features of classical conditioning is
that of the well-timed response. Over the course of training, the
timing of the response to the CS becomes more precise (i.e., the
CR predicts the onset of the US). Most models of the cerebellum
and its involvement in the classically conditioned eye-blink response focus on the production of this well-timed CR.

sociate the tone with the airpuff, and produces a defensive blink that coincides with the airpuff (CR).

188

Part III: Articles
Figure 2. Essential circuitry for the classically conditioned nictitating membrane response.

Modeling the Role of the Cerebellum
in Classical Conditioning
One of the first theories as to how the cerebellum could control
movement timing was proposed by Braitenberg (1961). He suggested that parallel fibers could act as tapped delay lines, with the
conduction time of the parallel fibers yielding movement timing.
However, this theory cannot account for movements with time
scales on the order of hundreds of milliseconds; it can only account
for delays on the order of a few milliseconds. Many current models,
however, still rely on this notion.
Moore, Desmond, and Berthier (1989) constructed a model of
the cerebellum based on an earlier adaptive network model. In order
to temporally associate the CS and US, the CS inputs are not discrete events but rather stimulus traces that persist for some time
after the CS is gone (see CONDITIONING). Learning is then allowed
to occur when the CS trace and the US coincide. In the cerebellar
implementation the adaptive unit is the Purkinje cell, whereas the
Golgi cell learns to gate an image of the CR from the brainstem to
the Purkinje cell. Purkinje and Golgi cell plasticity, combined with
the model’s tapped delay lines, yields anticipatory CRs for delay
and trace conditioning at all ISIs. The learning rules follow the
form of Rescorla and Wagner’s (1972) model and thus account for
the same stimulus context effect. One problem with this model is
that the tapped delay lines must be on the order of hundreds of
milliseconds, and no physiologically plausible mechanism has been
suggested for this. Another problem is the existence of the CR
image: the model does not include learning of this hypothesized
image. Moore et al. have hypothesized that this image is located
around the trigeminal nucleus. Learning-induced models of the CR
are present in the region bordering the trigeminal; however, evidence has shown that this model is relayed to the trigeminal from
the interpositus via the red nucleus. It is interesting to note that a
cerebellar implementation of the Sutton and Barto tapped delay
model (Moore and Choi, 1997) also requires feedback from extracerebellar structures. In order to properly determine the difference
between the predicted reinforcement and actual reinforcement, the
model uses an efference copy of the conditioned response from the
red nucleus and spinal trigeminal nucleus.
Jaffe (1990) proposed a model that moved the tapped delay lines
from the network and placed them at the level of the neuron. The
model proposed that single interpositus neurons can generate the
full range of delays for CRs by adjusting their input weights and
exploiting the phenomenon of delayed inhibitory rebound. The
most significant problem with this model is that the interpositus

cell must be quiescent at CS onset and cannot fire through the delay
period. However, most neurons in the interpositus are spontaneously active, and it seems unlikely that a few neurons silent during
the delay could account for the production of the CR.
Fiala, Grossberg, and Bullock (1996) also developed a neuroncentered tapped delay line model of the metabotropic glutamate
receptor (mGluR) second-messenger system (see CEREBELLUM:
NEURAL PLASTICITY), which is responsible for the well-timed CR.
Temporal correlation between the CS (parallel fiber–induced
mGluR response) and US causes a phosphorylation of AMPA receptors and calcium-dependent potassium channels. Phosphorylation of the calcium-dependent potassium channels results in a reduction in Purkinje cell firing during the CS-US interval. This
model is very interesting in that it explores possible biochemical
mechanisms for production of the well-timed response. However,
for this model to produce responses across the full range of timing,
the variety in the density of mGluR receptors on dendritic spines
must be highly variable, which may not be physiologically
plausible.
In addition to tapped delay line models, researchers began investigating how dynamical network processes within the cerebellum could yield precise timing. Buonomano and Mauk (1994) developed a semirealistic population model of the cerebellar cortex.
The CS activates a subset of mossy fibers, which in turn excites a
population of granule cells. Both the mossy fibers and the granule
cells excite a population of Golgi cells. The resulting Golgi inhibition of the granule cells produces a varying pattern of granule
cell activity over time in which different subsets of granule cells
are active at different times. Learning of the CR would occur
through weakening of the parallel fiber synapses that were active
around the occurrence of the US. An extension of this model (Mauk
and Donegan, 1997) includes two sites of plasticity within the cerebellum, the cerebellar cortex and interpositus nucleus. More important, the model showed that long-term depression of the parallel
fiber–Purkinje cell synapses, coupled with recurrent projections between the interpositus and inferior olive, produces a stable learning
system. The most pressing problem with these models is extreme
sensitivity to noise. If there is a substantial amount of noise in the
network or if the input pattern varies during the CS, the Purkinje
cell timing would be disrupted due to the changes in the granule
cell activity pattern.
Bartha (1992) developed a network simulation of the cerebellum
and associated circuitry that stressed the input and output representation of the cerebellum. The input representation was constrained by known properties of the mossy fibers and the output

Cerebellum and Conditioning
representation was modeled through detailed information on the
CR pathway and oculomotor plant. The cerebellar model consists
of two populations of granule cells, one responsive to the tone CS
and one unresponsive. The Golgi cell’s inhibitory influence on the
tone-unresponsive granule cells is to produce a variety of firing
patterns so that the granule cells display differing periods of depression. The Purkinje cell then selects the granule cells (through
long-term depression) that display the proper time interval of depression to produce a properly timed eye-blink. With realistic
single-neuron parameters, the model is able to reproduce many
aspects of CR timing and form. One concern with the model lies
with the Golgi cells. For the Golgi cell to be able to produce the
proper spectrum of delays for short ISIs, the time constant of the
Golgi cells’ influence on the granule cells must fall within the 100
ms to 250 ms range, which can be considered physiological. However, to produce properly timed blinks of longer latencies, the time
constant of this inhibitory effect must be considerably longer,
which does not seem physiologically plausible.
Grethe (2000) developed a model that focuses on the cerebellar
microcomplex as a Hebbian cell assembly (a highly interconnected
group of neurons that forms a reverberatory circuit that can sustain
activity). Each microcomplex contains a set of Purkinje cells and
their related nuclear neurons. The cerebellar cortex receives mossy
fiber input from both pontine nuclei and recurrent projections from
the interpositus nucleus. This basic architecture allows the recurrent
excitation in the excitatory interpositus cells to be modulated by
the Purkinje cells in the cerebellar cortex. The architecture of the
model tries to preserve the topographic projections between the
cerebellar cortex and the deep nuclei as well as the beam-like organization of Purkinje cells receiving input from the parallel fibers.
Local reverberations in the cerebellum, between the cerebellar cortex and deep nuclei, are responsible for the response topography
and timing. Long-term depression at the parallel fiber–Purkinje cell
synapse is responsible for the precise timing of the response. The
reverberations are controlled by the anatomical connectivity, the
bistability of Purkinje neurons, and the process of synaptic fatigue.
One interesting aspect of the model is the effect of stimulation
intensity on the response timing of the assembly. Since the timing
of the model is inherently dependent on the recurrent projections,
this timing process can be sped up by increasing the stimulation
intensity of the CS, which has been found experimentally.
Gluck and associates (2001) developed a connectionist-level
model of classical eye-blink conditioning incorporating basic features of the essential cerebellar circuitry, based on the RescorlaWagner model, a most successful trial-level behavioral formulation
of classical conditioning. The Rescorla-Wagner model assumes
that the change in association between a neutral CS and a responseevoking US is a function of the difference between the US and an
animal’s expectation of the US, given all CSs present in the trial.
Because the discrepancy, or “error,” between the animal’s expectations and what actually occurs drives learning in this theory, the
theory is referred to as an “error-correcting” learning procedure.
Thirty years after its publication, the Rescorla-Wagner model
stills stands as the most influential and powerful model in psychology for describing and predicting animal learning behavior in
conditioning studies. Moreover, its influence has extended far beyond animal conditioning. The model’s basic error correction principle has been rediscovered within cognitive psychology and cognitive neuroscience in the form of connectionist network models,
many of which rely on the same principle. In addition, the most
commonly used connectionist learning procedure, back propagation (see PERCEPTRONS, ADALINES, AND BACKPROPAGATION),
along with its simpler predecessor, the delta rule (see CONDITIONING), both are generalizations of the Rescorla-Wagner model.
Although the Gluck et al. model does not attempt to model neuronal processes, it includes two key features of cerebellar circuitry:

189

positive feedback via the pontine nuclei and negative feedback via
inhibition of the inferior olive. This rather simple connectionist
model successfully predicts a wide range of behavioral phenomena
of classical conditioning, including, in particular, adaptive timing
of the CR and blocking. In the model, adaptive timing depends on
positive feedback; if this feedback is blocked, adaptive timing no
longer occurs. The model circuitry predicts that inhibition of the
inferior olive-climbing fiber input system is necessary for the behavioral phenomenon of blocking. This prediction was empirically
verified by Kim, Krupa, and Thompson (1998), who showed that
blocking interpositus-evoked GABA inhibition of the inferior olive
during compound-stimulus training completely prevented behavioral blocking.

Discussion
Currently, no model accounts for all the data on cerebellar neurobiology and conditioning. However, models of conditioning that
take into account the neurobiology of the cerebellum can pose
questions that researchers may be able to examine. For example,
in order to experimentally test some of the network models presented, single-unit recordings from the granule would be necessary.
These recordings are difficult because granule cells are small and
closely spaced. Moreover, it would be difficult to obtain these recordings while the task is being performed. The predictions from
several of the models regarding granule cells are all different:
• Buonomano and Mauk predict that a subset of granule cells will
exhibit nonperiodic activity.
• Bartha predicts that the subset of granule cells will primarily
show depressions in activity at all conditionable intervals after
the presentation of a CS.
• Grethe predicts that activation of the granule cells in the cell
assembly should occur in a wave-like fashion as the cell assembly’s reverberating activity increases.
The future will bring new experiments that test these and future
models, which in turn will lead to further experiments and models.
Road Map: Mammalian Brain Regions
Related Reading: Cerebellum and Motor Control; Cerebellum: Neural
Plasticity; Conditioning

References
Albus, J. S., 1971, A theory of cerebellar function, Math. Biosci., 10:25–
61.
Bartha, G. T., 1992, A computer model of oculomotor and neural contributions to conditioned blink timing, Ph.D. diss., University of Southern
California.
Braitenberg, V., 1961, Functional interpretation of cerebellar histology, Nature, 190:539–540.
Buonomano, D. V., and Mauk, M. D., 1994, Neural network model of the
cerebellum: Temporal discrimination and the timing of motor responses,
Neural Computat., 6:38–55.
Fiala, J. C., Grossberg, S., and Bullock, D., 1996, Metabotropic glutamate
receptor activation in cerebellar Purkinje cells as substrate for adaptive
timing of the classically conditioned eye-blink response, J. Neurosci.,
16:3760–3774.
Gluck, M. A., Allen, M. T., Myers, C. E., and Thompson, R. F., 2001,
Cerebellar substrates for error-correction in motor conditioning, Neurobiol. Learn. Mem., 76:314–341.
Gormezano, I., Kehoe, E. J., and Marshall, B. S., 1983, Twenty years of
classical conditioning research with the rabbit, Prog. Psychobiol. Physiol. Psychol., 10:197–275. ⽧
Grethe, J. S., 2000, Neuroinformatics and the cerebellum: Towards an understanding of the cerebellar microzone and its contribution to the welltimed classically conditioned eyeblink response, Ph.D. diss., University
of Southern California.

190

Part III: Articles

Jaffe, S., 1990, A neuronal model for variable latency response, in Analysis
and Modeling of Neural Systems (F. H. Eeckman, Ed.), Boston: Kluwer,
pp. 405–410.
Kim, J., Krupa, D., and Thompson, R. F., 1998, Inhibitory cerebello-olivary
projections and blocking effect in classical conditioning, Science,
27:570–573.
Mauk, M. D., and Donegan, N. H., 1997, A model of pavlovian eyelid
conditioning based on the synaptic organization of the cerebellum,
Learn. Mem., 4:130–158.
Moore, J. W., and Choi, J-S., 1997, The TD model of classical conditioning:
Response topography and brain implementation, in Neural-Network
Models of Cognition (J. Donahoe and V. Dorsel, Eds.), North-Holland:
Elsevier, pp. 387–405.

Moore, J. W., Desmond, J. E., and Berthier, N. E., 1989, Adaptively timed
conditioned responses and the cerebellum: A neural network approach,
Biol. Cybern., 62:17–28.
Rescorla, R. A., and Wagner, A. R. A., 1972, A theory of pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement, in Classical Conditioning: II. Current Research and
Theory (A. H. Black and W. F. Prokasy, Eds.), New York: AppletonCentury-Crofts.
Thompson, R. F., Bao, S., Berg, M. S., Chen, L., Cipriano, B. D., Grethe,
J. S., Kim, J. J., Thompson, J. K., Tracy, J., and Krupa, D. J., 1997,
Associative learning, in The Cerebellum and Cognition (J. Schmahmann,
Ed.), Int. Rev. Neurobiol., 41:151–189 (special issue). ⽧

Cerebellum and Motor Control
Mitsuo Kawato
Introduction
Fast, smooth, and coordinated movements cannot be achieved by
basic feedback control alone because delays associated with feedback loops are long (about 200 ms for visual feedback and 100 ms
for somatosensory feedback) and feedback gains are low. Additionally, feedback controllers such as the commonly used PID (proportional, integral, and derivative) controllers do not incorporate
predictive dynamic or kinematic knowledge of controlled objects
or environments. Two major feedforward control schemes have
been proposed: the equilibrium point control hypothesis and the
inverse dynamics model hypothesis (see EQUILIBRIUM POINT HYPOTHESIS and MOTOR CONTROL, BIOLOGICAL AND THEORETICAL).
Some versions of the former scheme advocate that the central nervous system (CNS) can avoid inverse dynamics computation by
relying on the spring-like properties of muscles and reflex loops.
For this mechanism to work efficiently, the mechanical and neural
feedback gains, which can be measured as mechanical stiffness in
perturbation experiments, must be quite high. The low stiffness of
the arm, which was measured during visually guided point-to-point
multijoint movements (Gomi and Kawato, 1996), suggests the necessity of inverse dynamics models in these well-practiced and relaxed movements. The internal models in the brain must be acquired through motor learning in order to accommodate the
changes that occur with the growth of controlled objects such as
hands, legs, and torso, as well as the unpredictable variability of
the external world.
Where in the brain are internal models of the motor apparatus
likely to be stored? First, the locus should exhibit a remarkable
adaptive capability, which is essential for acquisition and continuous update of internal models of the motor apparatus. A number
of physiological studies have suggested important functional roles
of the cerebellum in motor learning and remarkable synaptic plasticity in the cerebellar cortex (Ito, 1984, 2001). Second, the biological objects of motor control by the brain, such as the arms,
speech articulators, and the torso, possess many degrees of freedom
and complicated nonlinear dynamics. Correspondingly, neural internal models should receive a broad range of sensory inputs and
possess a capacity high enough to approximate complex dynamics.
Extensive sensory signals carried by mossy fiber inputs and an
enormous number of granule cells in the cerebellar cortex seem to
fulfill these prerequisites for internal models. (See Figure 1 for
cerebellar circuitry and its connection to the cerebellar nucleus in
the case of the lateral cerebellum.) Finally, the cerebellar symptoms
usually classified as the “triad” of hypotonia, hypermetria, and intention tremor could be understood as degraded performance when

control is forced to rely solely on primitive feedback control after
internal models are destroyed or cannot be updated. This is because
precise, fast, coordinated movements can be executed if accurate
internal models of the motor apparatus can be utilized during trajectory planning, coordinate transformation, and motor control,
while primitive feedback controllers with long feedback delays and
small gains can attain only poor performance in these computations
and usually lead to oscillatory instability for forced fast movements.
Miall et al. (1993) grouped into classes many theories regarding
the role of the cerebellum (such as coordination, studied by Flourens; comparators, studied by Holmes; gain controllers; associative
learning). The most complete class of theories, these authors suggested, comprises theories in which the cerebellum forms an internal model of the motor system; such theories can encompass all
the alternative theories, while fitting many of the known facts of
cerebellar organization. These theories require that the cerebellum
be an adaptive system capable of learning and of updating a model
as the behavior of the motor system changes. They also require that
the cerebellum store relevant parameters of the motor system, as
these parameters form part of the description of the motor system
behavior. Another requirement is timing capabilities: the motor
system is dynamic, so a useful model will also need dynamic (i.e.,
time-dependent) behavior. How might internal models be acquired
in the cerebellum through motor learning?

Marr-Albus Model and Synaptic Plasticity
Purkinje cells are the only output neurons from the cerebellar cortex. They receive two major synaptic inputs, from parallel fibers
and from climbing fibers (Figure 1). Waveforms of neuronal spikes
generated by the two kinds of synaptic inputs are different and can
be discriminated even on extracellular recordings: simple spikes
are triggered by parallel fibers and complex spikes are triggered by
climbing fibers. Modularity is the basic design principle in the cerebellum. In the spinocerebellum, somototopic fractured maps have
been identified for parallel fiber inputs to the cerebellar cortex.
Furthermore, microzones as long as several centimeters along the
longitudinal axis of the cerebellar folia and as wide as 0.2 mm have
been identified for climbing fiber inputs (Ito, 1984).
Marr (1969) and Albus (1971) proposed a detailed model of the
cerebellum, according to which the cerebellum can form associative memories between particular patterns on parallel fiber inputs
and Purkinje cell outputs. The basic idea is that the parallel fiber–
Purkinje cell synapses can be modified by input from the climbing

Cerebellum and Motor Control

191

Figure 1. Schematic diagram of a neural
circuit for voluntary movement learning
control by a cerebrocerebellar communication loop (see Ito, 1984, for details). Although the lateral part of the cerebellum
hemisphere is shown and the input and
output of other cerebellar regions are
vastly different, the neural circuit of the
cerebellar cortex is rather the same and
uniform. It must be emphasized that only
one climbing fiber makes contact with a
single Purkinje cell, whereas parallel fibers make 200,000 synapses on a single
Purkinje cell. The number of granule
cells, the origin of parallel fibers, is about
1011. CF, climbing fiber; BC, basket cell;
GO, Golgi cell; GR, granule cell; MF,
Mossy fiber; PC, Purkinje cell; PF, parallel fiber; ST, stellate cell; DE, dentate
nucleus; IO, inferior olivary nucleus; PN,
pontine nuclei; RNp, parvocellular red
nucleus; VL, ventrolateral nucleus of the
thalamus.

fibers. In the perceptron models, the efficacy of a parallel fiber–
Purkinje cell synapse is assumed to change when there exists a
parallel fiber and climbing fiber input conjunction. The presence of
the putative heterosynaptic plasticity of Purkinje cells was demonstrated as a long-term depression (LTD) (Ito, 2001).
The original Marr-Albus model did not take into account the
dynamic and temporal characteristics of sensorimotor integration
and was inappropriate in proposing simple associative memories
for motor control problems. More satisfactory dynamic modeling
in cerebellar learning was started by Fujita (1982). An associative
LTD found in Purkinje cells can be modeled as the following heterosynaptic plasticity rule (Fujita, 1982): the rate of change of the
synaptic efficacy of a single parallel fiber synapse is proportional
to the negative product of the firing rate of that synapse’s input and

the increment of the climbing fiber firing rate from its spontaneous
level:
sdwi/dt ⳱ ⳮxi(F ⳮ Fspont)

(1)

where s is the time constant, wi is the synaptic weight of the ith
parallel fiber–Purkinje cell synapse, xi is the firing frequency of the
ith parallel fiber–Purkinje cell synapse, F is the firing frequency of
the climbing fiber input, and Fspont is its spontaneous level. This
single rule reproduces both the LTD and the long-term potentiation
(LTP) found in Purkinje cells. When the climbing fiber and the
parallel fiber are stimulated simultaneously, the parallel fiber synaptic efficacy decreases. In contrast, the parallel fiber synaptic efficacy increases when only the parallel fiber is stimulated (that is,
when the climbing fiber firing frequency is lower than its sponta-

192

Part III: Articles

neous level). From a computational viewpoint, if the Purkinje cell
output is the linear weighted summation of the parallel fiber inputs,
Equation 1 can be regarded as the steepest descent of the error
function defined as the square of the second factor. That is, this
equation could provide a supervised learning rule if and only if the
climbing fiber firing rate encodes an error signal.
Although early-day cerebellar models (Marr, 1969; Albus, 1971)
were epoch-making in proposing Purkinje cell plasticity as a basis
of cerebellar learning at the hardware level, they were not satisfactory at the representational and computational theory levels. What
is actually learned and stored in the cerebellum? What neural representations are used in inputs and outputs of the Purkinje cells?
Recent models and experimental efforts point to answers entirely
different from those suggested by the early-day models.

Models of Limb Motor Control in the Cerebellum
Boylls (1975) proposed that the spatiotemporal neural firing patterns formed by the excitatory loop due to the cerebellar reverberating circuit and the inhibitory loop via the Purkinje cells are computationally beneficial for the generation of rhythmic interlimb
coordination patterns in locomotion. In Boylls’s theory, the purpose of cerebellar computation is to create synergically meaningful
excitation profiles on a cerebellar nucleus, whose profiles are subsequently transmitted via an “output nucleus” to spinal levels.
Boylls’s model was later extended by Houk and Barto (1992) to
accommodate motor learning in the cerebellum as an adjustable
pattern generator (APG) model of the cerebellum. Temporal patterns of movement are acquired through motor learning, based on
the LTD of Purkinje cells in combination with the reverberating
circuit. Artificial neural network models with recurrent connections
that can learn and generate arm trajectories were the computational
bases of their model. The learning scheme proposed is mathematically based on associative reward-penalty learning (see REINFORCEMENT LEARNING IN MOTOR CONTROL). One of its attractive
features is that a temporal movement pattern is selected and generated, which is impossible with a simple internal forward or inverse model. Correspondingly, however, learning is more difficult.

Acquiring an inverse dynamics model through motor learning is
computationally difficult because the necessary teaching signal for
the desired motor command, which is the output of the inverse
dynamics model, is not available. Several computational learning
schemes to resolve this difficulty have been proposed (see SENSORIMOTOR LEARNING). Kawato and colleagues (Kawato, 1999)
proposed a cerebellar feedback-error-learning model (CBFELM;
Figure 2), which turned out to be the most biologically plausible
of the various proposals as a model of the cerebellum. In this
model, simple spikes (SS) represent feedforward motor commands
and the parallel fiber inputs represent the desired trajectory as well
as the sensory feedback of the current status of the controlled object. A microzone of the cerebellar cortex constitutes (part of) an
inverse model of a specific controlled object such as an eye or an
arm. Most important climbing fiber inputs are assumed to carry a
copy of the feedback motor commands generated by a crude feedback control circuit. Thus, the complex spikes (CS) of Purkinje
cells activated by climbing fiber inputs are predicted to be sensory
error signals already expressed in motor command coordinates. The
supervised learning equation (Equation 1) allows an interpretation
that a crude feedback controller could generate approximation to
the necessary error signal in motor commands. Stability and convergence of the CBFELM have been proved mathematically in the
recent control theory literature.

Experimental Supports for the Cerebellar
Feedback-Error-Learning Model
The CBFELM model was directly supported by neurophysiological
studies in the ventral paraflocculus (VPFL) of monkey cerebellum

Cerebellar Feedback-Error-Learning Model
Internal models can be largely classified into forward models and
inverse models (see SENSORIMOTOR LEARNING). Forward models
predict the sensory consequences of movements from an efference
copy of issued motor commands. Inverse models compute necessary feedforward motor commands from desired movement information. Both kinds of internal models are assumed to be located
in the cerebellum (Kawato, 1999). However, the evidence for the
forward models is much more circumstantial than is the evidence
for the inverse models (see MOTOR CONTROL, BIOLOGICAL AND
THEORETICAL). Learning forward models are generally much easier
than inverse models because actual sensory feedback can be utilized as a teaching signal in the supervised learning equation (Equation 1), except for the difficulty associated with the delay in sensory
feedback. Miall et al. (1993) proposed that the cerebellum forms
two types of forward internal models. One model is a forward
model of the motor apparatus. The second is a forward model of
the transport time delays in the control loop (due to receptor and
effector delays, axonal conductance, and cognitive processing delays). The second model delays the copy of prediction made by the
first model so that it can be compared in temporal registration with
actual sensory feedback from the movement. The second model
resolves the difficulty of a temporal mismatch between the sensory
signal delayed by the feedback loop and the output calculated by
forward internal models.

Figure 2. A, The general feedback-error-learning model. B, The cerebellar
feedback-error-learning model (CBFELM) (Kawato, 1999). The “controlled object” is a physical entity that needs to be controlled by the CNS,
such as the eyes, hands, legs, or torso.

Cerebellum and Motor Control
during ocular following responses (OFRs) (Shidara et al., 1993;
Kawato, 1999; Yamamoto et al., 2002). OFRs are tracking movements of the eyes evoked by movements in a visual scene and are
thought to be important for the visual stabilization of gaze. The
phylogenetically older, crude feedback circuit of the CBFELM is
comprised of the retina, the accessory optic system (AOS), and the
brainstem. The phylogenetically newer, more sophisticated feedforward pathway and the inverse dynamics model of the CBFELM
correspond to the cerebral and cerebellar cortical pathway and the
cerebellum, respectively.
During OFRs, the temporal waveforms of SS firing frequency
of VPFL Purkinje cells show complicated patterns. However,
they (Figure 3, thin curve) were quite accurately reconstructed
by using an inverse dynamics representation of the eye movement (Figure 3, thick curve; Shidara et al., 1993). The model fit
was good for the majority of the neurons studied under a wide
range of visual stimulus conditions. The same inverse dynamics
analysis of firing frequency was applied to neurons in the area
MST and dorsolateral pontine nucleus (DLPN), which provide
visual mossy fiber inputs to the VPFL. In this area neural firing
patterns were not well reconstructed. Taken together, these data
suggest that the VPFL is the major site of the inverse dynamics
model of the eye for OFRs.
The CBFELM model assumes that motor commands, which
are conveyed by SS, are directly modified and acquired through
synaptic plasticity by motor command errors, which are conveyed by climbing fiber inputs. For this to work, the motor commands and climbing fiber inputs must have comparable temporal
and spatial characteristics, but the ultra-low discharge rates of
the latter (1–2 spikes/s) would appear to rule this out. This apparently discrete and intermittent nature of climbing fiber inputs
once suggested a reinforcement learning type of theory of cerebellar learning. However, if thousands of trials were averaged to
compute firing frequencies of the climbing fiber inputs, the firing
rates actually conveyed very accurate and reliable temporal
waveforms of motor command error (Figure 3Cb). Because the
LTD is a rather slow process of several tens of minutes of time
constants, the averaging over many trials can actually be conducted by the LTD dynamics itself. Consequently, the firing
probability of climbing fiber inputs aligned with the stimulus
motion onset had high-frequency temporal dynamics matching
those of the dynamic command signals. Thus, the most critical
assumption of the CBFELM model was satisfied.
The preferred directions of MST and DLPN neurons were
evenly distributed over 360 degrees. Thus, the visual coordinates
for OFRs are uniformly distributed over all possible directions.
On the other hand, the spatial coordinates of the extraocular muscles lie in either the horizontal or vertical directions, and are
entirely different from the visual coordinates. The preferred directions of Purkinje cell SS were either downward or ipsilateral,
and at the site of each recording, electrical stimulation of a Purkinje cell elicited eye movement toward the preferred direction
of the SS of that Purkinje cell. This observation indicates that
the SS coordinate framework is already that of the motor commands. Thus, at the parallel fiber–Purkinje cell synapse, a drastic
visuomotor coordinate transformation occurs. Hence, the neural
representation dramatically changes from population coding in
MST and DLPN to firing rate coding of Purkinje cells at the
parallel fiber–Purkinje cell synapse. What is the origin of this
drastic transformation? According to the CBFELM model, the
CS and eventually the AOS are the source of this motor command spatial framework. The preferred directions of pretectum
neurons are upward, and those of nucleus of optic tract neurons
are contralateral, and they are propagated to the inferior olive
neurons and the CS of Purkinje cells. Yamamoto et al. (2002)
reproduced all these experimental findings of Purkinje cell firing

193

characteristics during OFRs based on CBFELM, thus providing
quite strong evidence for the theory.
Although direct and rigorous support of the CBFELM model
was limited to a small portion of the cerebellum and to only
several types of eye movements (OFR, VOR, OKR, smooth pursuit), because the neural circuit of different parts of the cerebellum is uniform and LTD is ubiquitous, we believe that the computational principle and the neural architecture demonstrated are
common to all parts of the cerebellum. Recent physiological and
brain imaging experiments provided further support of the
CBFELM model in visually guided arm reaching movements
(Kitazawa, Kimura, and Yin, 1998) and in the learning of a new
tool (Imamizu et al., 2000).

Discussion
Humans can manipulate a vast number of tools, and exhibit an
almost infinite number of behaviors in different environments.
Given this multitude of contexts for sensorimotor control, there
are two qualitatively distinct strategies to motor control and
learning. The first is to use a single controller that uses all the
contextual information in an attempt to produce an appropriate
control signal. However, such a controller would have to be enormously complex to allow for all possible scenarios. If this controller were unable to encapsulate all the contexts, it would need
to adapt every time the context of the movement changed before
it could produce appropriate motor commands. This would produce transient but possibly large performance errors. Alternatively, a modular approach could be used in which multiple controllers coexist, with each controller suitable for one or a small
set of contexts. Depending on the current context, only those
appropriate controllers should be active to generate the motor
command.
The modular approach has several computational advantages
over the nonmodular approach. First, by using multiple inverse
models, each of which might capture the motor commands necessary when interacting with a particular object or within a particular environment, we could achieve an efficient coding of the
world. In other words, the large set of environmental conditions
in which we are required to generate movement requires multiple
behaviors or sets of motor commands, each embodied within a
module. Second, the use of a modular system allows individual
modules to adapt through motor learning, without affecting the
motor behaviors already learned by other modules. Third, many
situations that we encounter are derived from combinations of
previously experienced contexts, such as novel conjoints of manipulated objects and environments. By modulating the contribution to the final motor command of the outputs of the inverse
modules, an enormous repertoire of behaviors can be generated.
With as few as 32 inverse models, in which the output of each
model either contributes or does not contribute to the final motor
command, we have 210 behaviors—sufficient for a new behavior
for every second of one’s life. Therefore, multiple internal models can be regarded conceptually as motor primitives, which are
the building blocks used to construct intricate motor behaviors
with enormous vocabulary (see MOTOR PRIMITIVES).
Based on the benefits of a modular approach and the experimental evidence for modularity in observed behaviors, Wolpert
and Kawato (1988) have proposed that the problem of motor
learning and control is best solved using multiple controllers—
that is, inverse models. At any given time, one or a subset of
these inverse models will contribute to the final motor command
(Figure 4 gives the details of the model). However, if there are
multiple controllers, then there must also be some scheme to
select the appropriate controller or controllers at each moment in
time. The basic idea is that multiple inverse models exist to

194

Part III: Articles

Figure 3. Change of neural codes and learning of inverse dynamics model in the cerebellum for ocular following responses (OFRs) (see Yamamoto et al., 2002, for a model reproduction of this experimental
data). A, The firing characteristics of MST, DLPN, and VPFL neurons. B, A schematic neural circuit for OFR. C, The temporal firing patterns of VPFL Purkinje cells in upward eye movements induced
by upward visual motion. In A, the left, middle, and right columns are for MST, DLPN, and VPFL neurons. In a, post-stimulus-time histograms of the firing rates of a typical neuron in each of the three
areas are shown. The origin of time is the onset of visual stimulus motion. In b, histograms of a number of cells within a given range of the optimum stimulus speeds are shown. In c, polar plots of
optimum stimulus directions are shown. U, C, D, and I indicate upward, contralateral, downward, and ipsilateral, respectively. VPFL Purkinje cells were classified into two groups, vertical cells and
horizontal cells, based on simple spike (dotted line) and complex spike (solid line) optimum directions. B, The circuit can be divided into two main pathways. The upper part shows the corticocortical
(the cerebral cortex to the cerebellar cortex) pathway, which corresponds to the feedforward arc of the feedback-error-learning model. The lower part shows the phylogenetically older feedback pathway
containing the accessory optic system, which corresponds to a crude feedback controller in the feedback-error-learning scheme. C, Temporal firing patterns of nine Purkinje cells accumulated (thin curves)
and their reconstruction based on an inverse-dynamics model (bold curves). The model predicts that the temporal firing patterns of simple spikes (a) and complex spikes (b) should be mirror images of
each other, and this was confirmed experimentally. MST, medial superior temporal area; DLPN, dorsolateral pontine nucleus; VPFL, ventral paraflocculus; AOS, accessory optic system; PT, pretectum;
NOT, nucleus of optic tract; MT, middle temporal area; STS, superior temporal sulcus; LGN, lateral geniculate nucleus; EOMN, extraocular motor neurons.

Cerebellum and Motor Control

195

Recent human brain imaging studies have started to accumulate evidence supporting multiple internal models of tools in the
cerebellum (Imamizu et al., 2000, and successive studies). Other
imaging studies suggest forward models in the cerebellum as
well as inverse models. Each modular internal model in MOSAIC could have good anatomical correspondence with microzones. MOSAIC is capable of learning to produce appropriate
motor commands in a variety of contexts and can switch rapidly
between controllers as the context changes. These features are
important for a full model of motor control and motor learning,
as it is clear that the human motor system is capable of very
flexible, modular adaptation. Furthermore, MOSAIC has the potential to explain many human cognitive capabilities such as
thinking, communication, and language. This point is intriguing,
since the cerebellum was shown to be involved in these uniquely
human cognitive activities.
Road Maps: Mammalian Brain Regions; Mammalian Motor Control;
Neural Plasticity
Background: Motor Control, Biological and Theoretical
Related Reading: Cerebellum and Conditioning; Imaging the Grammatical Brain; Sensorimotor Learning

References

Figure 4. A schematic of the MOSAIC model (Wolpert and Kawato, 1998).
N-paired modules are shown as stacked sheets (the dotted lines represent
training signals and three-signal multiplication). The details of the first
module are shown. Interactions between modules take place through the
responsibility estimator. Each module consists of three interacting parts.
The first two, the forward model and the responsibility predictor, are used
to determine the responsibility of the module. This responsibility signal
reflects the degree to which the module captures the current context and
should, therefore, participate in control.

control the system, and each is augmented with a forward model
that determines the responsibility each controller should assume
during movement. This responsibility signal reflects, at any given
time, the degree to which each pair of forward and inverse models should be responsible for controlling the current behavior.
Within each module, the inverse and forward internal models are
tightly coupled during their acquisition, through motor learning.
This ensures that the forward models learn to divide up experience so that at least one forward model can predict the consequence of actions performed in any given context. By coupling
the learning of the forward and inverse models, the inverse models learn to provide appropriate control commands in contexts in
which their paired forward model produces accurate predictions.
The model was once called the multiple paired forward and inverse models, but it was later renamed MOSAIC (MOdular Selection And Identification Control). MOSAIC is a version of the
mixture-of-experts architecture (see MODULAR AND HIERARCHICAL LEARNING SYSTEMS).

Albus, J. S., 1971, A theory of cerebellar functions, Math. Biosci., 10:25–
61.
Boylls, C. C., 1975, A Theory of Cerebellar Function with Applications
to Locomotion: I. The Physiological Role of Climbing Fiber Inputs in
Anterior Lobe Operation, COINS Technical Report, Amherst: University of Massachusetts, Computer and Information Science.
Fujita, M., 1982, Adaptive filter model of the cerebellum, Biol. Cybern.,
45:195–206.
Gomi, H., and Kawato, M., 1996, Equilibrium-point control hypothesis
examined by measured arm stiffness during multi-joint movement, Science, 272:117–120.
Houk, J. C., and Barto, A. G., 1992, Distributed sensorimotor learning,
in Tutorial in Motor Behavior II (G. E. Stelmach and J. Requin, Eds.),
Amsterdam: Elsevier, pp. 71–100.
Imamizu, H., Miyauchi, S., Tamada, T. Sasaki, Y., Takino, R., Puetz,
B., Yoshioka, T., and Kawato, M., 2000, Human cerebellar activity
reflecting an acquired internal model of a new tool, Nature, 403:192–
195. ⽧
Ito, M., 1984, The Cerebellum and Neural Control, New York: Raven
Press.
Ito, M., 2001, Long-term depression: Characterization, signal transduction, and functional roles, Physiol. Rev., 81:1143–1195. ⽧
Kawato, M., 1999, Internal models for motor control and trajectory planning, Curr. Opin. Neurobiol., 9:718–727. ⽧
Kitazawa, S., Kimura, T., and Yin, P., 1998, Cerebellar complex spikes
encode both destinations and errors in arm movements, Nature,
392:494–497.
Marr, D., 1969, A theory of cerebellar cortex, J. Physiol., 202:437–470.
Miall, R. C., Weir, D. J., Wolpert, D. M., and Stein, J. F., 1993, Is the
cerebellum a Smith predictor? J. Motor Behav., 25:203–216.
Shidara, M., Kawano, K., Gomi, H., and Kawato, M., 1993, Inverse
dynamics model eye movement control by Purkinje cells in the cerebellum, Nature, 365:50–52.
Wolpert, D., and Kawato, M., 1998, Multiple paired forward and inverse
models for motor control, Neural Netw., 11:1317–1329. ⽧
Yamamoto, K., Kobayashi, Y., Takemura, A., Kawano, K., and Kawato,
M., 2002, Computational studies on acquisition and adaptation of ocular following responses based on cerebellar synaptic plasticity, J.
Neurophysiol., 87:1554–1571.